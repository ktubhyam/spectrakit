{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SpectraKit","text":"<p>Python toolkit for spectral data processing.</p> <p>SpectraKit provides a comprehensive, pip-installable library for preprocessing and analyzing spectral data from IR, Raman, and NIR spectroscopy.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Format Parsers \u2014 Read JCAMP-DX, SPC, CSV, Bruker OPUS, HDF5 files</li> <li>Baseline Correction \u2014 ALS, SNIP, polynomial, rubberband</li> <li>Normalization \u2014 SNV, min-max, area, vector (L2)</li> <li>Smoothing \u2014 Savitzky-Golay, Whittaker</li> <li>Derivatives \u2014 Savitzky-Golay, gap-segment (Norris-Williams)</li> <li>Scatter Correction \u2014 MSC, Extended MSC</li> <li>Spectral Transforms \u2014 Kubelka-Munk, ATR correction</li> <li>Spectral Operations \u2014 Subtract, average, interpolate</li> <li>Peak Analysis \u2014 Peak finding, integration</li> <li>Similarity Metrics \u2014 Cosine, Pearson, spectral angle, Euclidean</li> <li>Pipeline \u2014 Chain processing steps as pure functions</li> <li>Plotting \u2014 Spectrum, comparison, and baseline visualizations</li> <li>scikit-learn Bridge \u2014 Use any function in <code>sklearn.pipeline.Pipeline</code></li> <li>Optional Backends \u2014 pybaselines (200+ methods), lmfit (peak fitting)</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import numpy as np\nfrom spectrakit import (\n    baseline_als,\n    normalize_snv,\n    smooth_savgol,\n)\nfrom spectrakit.pipeline import Pipeline\n\n# Load or create spectral data\nspectra = np.random.default_rng(42).random((10, 1000))\n\n# Process with individual functions\nsmoothed = smooth_savgol(spectra, window_length=11)\ncorrected = baseline_als(smoothed, lam=1e6)\nnormalized = normalize_snv(corrected)\n\n# Or use a Pipeline\npipe = Pipeline()\npipe.add(smooth_savgol, window_length=11)\npipe.add(baseline_als, lam=1e6)\npipe.add(normalize_snv)\nresult = pipe.transform(spectra)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyspectrakit\n</code></pre> <p>See the installation guide for optional dependencies.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>See CHANGELOG.md on GitHub for the full release history.</p>"},{"location":"api/","title":"API Reference","text":"<p>SpectraKit's API is organized into subpackages by function category. All core processing functions are also re-exported from the top-level <code>spectrakit</code> namespace.</p>"},{"location":"api/#core-containers","title":"Core Containers","text":"Module Description Spectrum Spectral data container Pipeline Processing pipeline Exceptions Custom exception types"},{"location":"api/#processing-functions","title":"Processing Functions","text":"Module Functions Description Baseline <code>baseline_als</code>, <code>baseline_snip</code>, <code>baseline_polynomial</code>, <code>baseline_rubberband</code> Baseline correction Normalize <code>normalize_snv</code>, <code>normalize_minmax</code>, <code>normalize_area</code>, <code>normalize_vector</code> Normalization Smooth <code>smooth_savgol</code>, <code>smooth_whittaker</code> Smoothing / noise reduction Derivative <code>derivative_savgol</code>, <code>derivative_gap_segment</code> Spectral derivatives Scatter <code>scatter_msc</code>, <code>scatter_emsc</code> Scatter correction Transform <code>transform_kubelka_munk</code>, <code>transform_atr_correction</code> Spectral transforms Operations <code>spectral_subtract</code>, <code>spectral_average</code>, <code>spectral_interpolate</code> Spectral arithmetic Peaks <code>peaks_find</code>, <code>peaks_integrate</code> Peak analysis Similarity <code>similarity_cosine</code>, <code>similarity_pearson</code>, <code>similarity_spectral_angle</code>, <code>similarity_euclidean</code> Similarity metrics"},{"location":"api/#io-and-visualization","title":"I/O and Visualization","text":"Module Description IO File format parsers Plot Plotting utilities (requires matplotlib)"},{"location":"api/#optional-integrations","title":"Optional Integrations","text":"Module Description Contrib pybaselines and lmfit backends sklearn scikit-learn transformer bridge"},{"location":"api/baseline/","title":"Baseline Correction","text":""},{"location":"api/baseline/#spectrakit.baseline.baseline_als","title":"spectrakit.baseline.baseline_als","text":"<pre><code>baseline_als(\n    intensities: ndarray,\n    lam: float = DEFAULT_LAMBDA,\n    p: float = DEFAULT_P,\n    max_iter: int = DEFAULT_MAX_ITER,\n    tol: float = DEFAULT_TOL,\n    return_info: bool = False,\n) -&gt; np.ndarray | ConvergenceInfo\n</code></pre> <p>Estimate baseline using Asymmetric Least Squares smoothing.</p> <p>Iteratively fits a smooth baseline by penalizing deviations asymmetrically: points above the baseline are penalized less (weight p) than points below (weight 1-p).</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <code>lam</code> <code>float</code> <p>Smoothness parameter (lambda). Larger = smoother. Typical range: 1e4 to 1e9.</p> <code>DEFAULT_LAMBDA</code> <code>p</code> <code>float</code> <p>Asymmetry parameter. Smaller values push baseline lower. Typical range: 0.001 to 0.05.</p> <code>DEFAULT_P</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>DEFAULT_MAX_ITER</code> <code>tol</code> <code>float</code> <p>Convergence tolerance on weight change.</p> <code>DEFAULT_TOL</code> <code>return_info</code> <code>bool</code> <p>If <code>True</code>, return a :class:<code>ConvergenceInfo</code> object with iteration count, convergence status, and baseline. Only supported for 1-D input.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray | ConvergenceInfo</code> <p>Estimated baseline (same shape as input), or</p> <code>ndarray | ConvergenceInfo</code> <p>class:<code>ConvergenceInfo</code> if <code>return_info=True</code>.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; corrected = intensities - baseline_als(intensities)\n&gt;&gt;&gt; info = baseline_als(intensities, return_info=True)\n&gt;&gt;&gt; print(info.iterations, info.converged)\n</code></pre> Source code in <code>src/spectrakit/baseline/als.py</code> <pre><code>def baseline_als(\n    intensities: np.ndarray,\n    lam: float = DEFAULT_LAMBDA,\n    p: float = DEFAULT_P,\n    max_iter: int = DEFAULT_MAX_ITER,\n    tol: float = DEFAULT_TOL,\n    return_info: bool = False,\n) -&gt; np.ndarray | ConvergenceInfo:\n    \"\"\"Estimate baseline using Asymmetric Least Squares smoothing.\n\n    Iteratively fits a smooth baseline by penalizing deviations\n    asymmetrically: points above the baseline are penalized less\n    (weight p) than points below (weight 1-p).\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n        lam: Smoothness parameter (lambda). Larger = smoother.\n            Typical range: 1e4 to 1e9.\n        p: Asymmetry parameter. Smaller values push baseline lower.\n            Typical range: 0.001 to 0.05.\n        max_iter: Maximum number of iterations.\n        tol: Convergence tolerance on weight change.\n        return_info: If ``True``, return a :class:`ConvergenceInfo`\n            object with iteration count, convergence status, and\n            baseline. Only supported for 1-D input.\n\n    Returns:\n        Estimated baseline (same shape as input), or\n        :class:`ConvergenceInfo` if ``return_info=True``.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n\n    Examples:\n        &gt;&gt;&gt; corrected = intensities - baseline_als(intensities)\n        &gt;&gt;&gt; info = baseline_als(intensities, return_info=True)\n        &gt;&gt;&gt; print(info.iterations, info.converged)\n    \"\"\"\n    if not 0 &lt; p &lt; 1:\n        raise ValueError(f\"p (asymmetry) must be in (0, 1), got {p}\")\n    if lam &lt;= 0:\n        raise ValueError(f\"lam (smoothness) must be positive, got {lam}\")\n    if max_iter &lt; 1:\n        raise ValueError(f\"max_iter must be &gt;= 1, got {max_iter}\")\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    # Pre-compute the penalty matrix H = lam * D'D once.\n    # This is the same for every spectrum in a batch since it depends\n    # only on spectrum length and lam.\n    n = intensities.shape[-1]\n    D = sparse.diags([1, -2, 1], [0, 1, 2], shape=(n - 2, n))\n    H = lam * D.T @ D\n\n    if return_info:\n        if intensities.ndim != 1:\n            raise ValueError(\"return_info=True is only supported for 1-D input\")\n        return _baseline_als_1d_info(intensities, penalty=H, p=p, max_iter=max_iter, tol=tol)\n\n    return apply_along_spectra(\n        _baseline_als_1d, intensities, penalty=H, p=p, max_iter=max_iter, tol=tol\n    )\n</code></pre>"},{"location":"api/baseline/#spectrakit.baseline.baseline_snip","title":"spectrakit.baseline.baseline_snip","text":"<pre><code>baseline_snip(\n    intensities: ndarray,\n    max_half_window: int = DEFAULT_MAX_HALF_WINDOW,\n    decreasing: bool = True,\n) -&gt; np.ndarray\n</code></pre> <p>Estimate baseline using the SNIP algorithm.</p> <p>Iteratively clips peaks by comparing each point to the average of its neighbors at increasing window sizes.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <code>max_half_window</code> <code>int</code> <p>Maximum half-window size. Controls how broad the features that get clipped can be.</p> <code>DEFAULT_MAX_HALF_WINDOW</code> <code>decreasing</code> <code>bool</code> <p>If True, iterate from max_half_window down to 1.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Estimated baseline, same shape as intensities.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/baseline/snip.py</code> <pre><code>def baseline_snip(\n    intensities: np.ndarray,\n    max_half_window: int = DEFAULT_MAX_HALF_WINDOW,\n    decreasing: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Estimate baseline using the SNIP algorithm.\n\n    Iteratively clips peaks by comparing each point to the average of\n    its neighbors at increasing window sizes.\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n        max_half_window: Maximum half-window size. Controls how broad\n            the features that get clipped can be.\n        decreasing: If True, iterate from max_half_window down to 1.\n\n    Returns:\n        Estimated baseline, same shape as intensities.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    if max_half_window &lt; 1:\n        raise ValueError(f\"max_half_window must be &gt;= 1, got {max_half_window}\")\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    return apply_along_spectra(\n        _baseline_snip_1d,\n        intensities,\n        max_half_window=max_half_window,\n        decreasing=decreasing,\n    )\n</code></pre>"},{"location":"api/baseline/#spectrakit.baseline.baseline_polynomial","title":"spectrakit.baseline.baseline_polynomial","text":"<pre><code>baseline_polynomial(\n    intensities: ndarray,\n    degree: int = DEFAULT_DEGREE,\n    max_iter: int = DEFAULT_MAX_ITER,\n    tol: float = DEFAULT_TOL,\n    return_info: bool = False,\n) -&gt; np.ndarray | ConvergenceInfo\n</code></pre> <p>Estimate baseline using iterative polynomial fitting.</p> <p>Fits a polynomial, then iteratively removes points above it (peaks) and refits until convergence.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <code>degree</code> <code>int</code> <p>Polynomial degree. Higher = more complex baselines.</p> <code>DEFAULT_DEGREE</code> <code>max_iter</code> <code>int</code> <p>Maximum iterations for peak-removal loop.</p> <code>DEFAULT_MAX_ITER</code> <code>tol</code> <code>float</code> <p>Convergence tolerance (fraction of points changed).</p> <code>DEFAULT_TOL</code> <code>return_info</code> <code>bool</code> <p>If <code>True</code>, return a :class:<code>ConvergenceInfo</code> object. Only supported for 1-D input.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray | ConvergenceInfo</code> <p>Estimated baseline (same shape as input), or</p> <code>ndarray | ConvergenceInfo</code> <p>class:<code>ConvergenceInfo</code> if <code>return_info=True</code>.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/baseline/polynomial.py</code> <pre><code>def baseline_polynomial(\n    intensities: np.ndarray,\n    degree: int = DEFAULT_DEGREE,\n    max_iter: int = DEFAULT_MAX_ITER,\n    tol: float = DEFAULT_TOL,\n    return_info: bool = False,\n) -&gt; np.ndarray | ConvergenceInfo:\n    \"\"\"Estimate baseline using iterative polynomial fitting.\n\n    Fits a polynomial, then iteratively removes points above it\n    (peaks) and refits until convergence.\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n        degree: Polynomial degree. Higher = more complex baselines.\n        max_iter: Maximum iterations for peak-removal loop.\n        tol: Convergence tolerance (fraction of points changed).\n        return_info: If ``True``, return a :class:`ConvergenceInfo`\n            object. Only supported for 1-D input.\n\n    Returns:\n        Estimated baseline (same shape as input), or\n        :class:`ConvergenceInfo` if ``return_info=True``.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    if degree &lt; 0:\n        raise ValueError(f\"degree must be non-negative, got {degree}\")\n    if max_iter &lt; 1:\n        raise ValueError(f\"max_iter must be &gt;= 1, got {max_iter}\")\n    if tol &lt;= 0:\n        raise ValueError(f\"tol must be positive, got {tol}\")\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if return_info:\n        if intensities.ndim != 1:\n            raise ValueError(\"return_info=True is only supported for 1-D input\")\n        return _baseline_polynomial_1d_info(intensities, degree=degree, max_iter=max_iter, tol=tol)\n\n    return apply_along_spectra(\n        _baseline_polynomial_1d, intensities, degree=degree, max_iter=max_iter, tol=tol\n    )\n</code></pre>"},{"location":"api/baseline/#spectrakit.baseline.baseline_rubberband","title":"spectrakit.baseline.baseline_rubberband","text":"<pre><code>baseline_rubberband(intensities: ndarray) -&gt; np.ndarray\n</code></pre> <p>Estimate baseline using the rubberband (convex hull) method.</p> <p>Computes the lower convex hull of the spectrum and interpolates between the hull vertices to form the baseline.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Estimated baseline, same shape as intensities.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/baseline/rubberband.py</code> <pre><code>def baseline_rubberband(intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate baseline using the rubberband (convex hull) method.\n\n    Computes the lower convex hull of the spectrum and interpolates\n    between the hull vertices to form the baseline.\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n\n    Returns:\n        Estimated baseline, same shape as intensities.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    return apply_along_spectra(_baseline_rubberband_1d, intensities)\n</code></pre>"},{"location":"api/contrib/","title":"Contrib \u2014 Optional Backends","text":"<p>Optional backends that wrap third-party libraries for advanced functionality.</p>"},{"location":"api/contrib/#pybaselines-backend","title":"pybaselines Backend","text":"<p>Note</p> <p>Requires pybaselines: <code>pip install pyspectrakit[baselines]</code></p>"},{"location":"api/contrib/#spectrakit.contrib._pybaselines.pybaselines_method","title":"spectrakit.contrib._pybaselines.pybaselines_method","text":"<pre><code>pybaselines_method(\n    intensities: ndarray, method: str, **kwargs: Any\n) -&gt; np.ndarray\n</code></pre> <p>Apply any pybaselines method through a unified interface.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>method</code> <code>str</code> <p>Name of the pybaselines method (e.g., <code>\"asls\"</code>, <code>\"airpls\"</code>, <code>\"mor\"</code>).</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to the pybaselines method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Estimated baseline, same shape as intensities.</p> <p>Raises:</p> Type Description <code>DependencyError</code> <p>If pybaselines is not installed.</p> <code>ValueError</code> <p>If the method name is not recognized.</p> Source code in <code>src/spectrakit/contrib/_pybaselines.py</code> <pre><code>def pybaselines_method(\n    intensities: np.ndarray,\n    method: str,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    \"\"\"Apply any pybaselines method through a unified interface.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)`` or ``(N, W)``.\n        method: Name of the pybaselines method (e.g., ``\"asls\"``,\n            ``\"airpls\"``, ``\"mor\"``).\n        **kwargs: Keyword arguments forwarded to the pybaselines method.\n\n    Returns:\n        Estimated baseline, same shape as intensities.\n\n    Raises:\n        DependencyError: If pybaselines is not installed.\n        ValueError: If the method name is not recognized.\n    \"\"\"\n    baseline_cls = _get_pybaselines()\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n\n    return apply_along_spectra(\n        _pybaselines_1d,\n        intensities,\n        method=method,\n        baseline_cls=baseline_cls,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/contrib/#spectrakit.contrib._pybaselines.list_pybaselines_methods","title":"spectrakit.contrib._pybaselines.list_pybaselines_methods","text":"<pre><code>list_pybaselines_methods() -&gt; dict[str, list[str]]\n</code></pre> <p>Return a dictionary of available pybaselines methods by category.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dict mapping category names to lists of method names.</p> Source code in <code>src/spectrakit/contrib/_pybaselines.py</code> <pre><code>def list_pybaselines_methods() -&gt; dict[str, list[str]]:\n    \"\"\"Return a dictionary of available pybaselines methods by category.\n\n    Returns:\n        Dict mapping category names to lists of method names.\n    \"\"\"\n    return dict(_PYBASELINES_CATEGORIES)\n</code></pre>"},{"location":"api/contrib/#lmfit-backend","title":"lmfit Backend","text":"<p>Note</p> <p>Requires lmfit: <code>pip install pyspectrakit[fitting]</code></p>"},{"location":"api/contrib/#spectrakit.contrib._lmfit.fit_peaks","title":"spectrakit.contrib._lmfit.fit_peaks","text":"<pre><code>fit_peaks(\n    intensities: ndarray,\n    wavenumbers: ndarray,\n    peak_positions: list[float],\n    model: str = \"gaussian\",\n    **kwargs: Any,\n) -&gt; FitResult\n</code></pre> <p>Fit spectral peaks using lmfit models.</p> <p>Creates a composite model of multiple peaks and fits them to the data. Each peak is initialized near the specified position.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code>.</p> required <code>wavenumbers</code> <code>ndarray</code> <p>Wavenumber axis, shape <code>(W,)</code>.</p> required <code>peak_positions</code> <code>list[float]</code> <p>Approximate peak center positions in wavenumber units.</p> required <code>model</code> <code>str</code> <p>Peak shape model. One of <code>\"gaussian\"</code>, <code>\"lorentzian\"</code>, <code>\"voigt\"</code>, <code>\"pseudo_voigt\"</code>.</p> <code>'gaussian'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>lmfit.Model.fit</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FitResult</code> <p><code>FitResult</code> with the best fit, individual components,</p> <code>FitResult</code> <p>parameters, and residual.</p> <p>Raises:</p> Type Description <code>DependencyError</code> <p>If lmfit is not installed.</p> <code>ValueError</code> <p>If the model name is not supported.</p> Source code in <code>src/spectrakit/contrib/_lmfit.py</code> <pre><code>def fit_peaks(\n    intensities: np.ndarray,\n    wavenumbers: np.ndarray,\n    peak_positions: list[float],\n    model: str = \"gaussian\",\n    **kwargs: Any,\n) -&gt; FitResult:\n    \"\"\"Fit spectral peaks using lmfit models.\n\n    Creates a composite model of multiple peaks and fits them to the\n    data. Each peak is initialized near the specified position.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)``.\n        wavenumbers: Wavenumber axis, shape ``(W,)``.\n        peak_positions: Approximate peak center positions in\n            wavenumber units.\n        model: Peak shape model. One of ``\"gaussian\"``,\n            ``\"lorentzian\"``, ``\"voigt\"``, ``\"pseudo_voigt\"``.\n        **kwargs: Additional keyword arguments passed to ``lmfit.Model.fit``.\n\n    Returns:\n        ``FitResult`` with the best fit, individual components,\n        parameters, and residual.\n\n    Raises:\n        DependencyError: If lmfit is not installed.\n        ValueError: If the model name is not supported.\n    \"\"\"\n    lmfit = _get_lmfit()\n    intensities = ensure_float64(intensities)\n    wavenumbers = ensure_float64(wavenumbers)\n\n    if model not in SUPPORTED_MODELS:\n        raise ValueError(f\"Unknown model '{model}'. Supported: {list(SUPPORTED_MODELS.keys())}\")\n\n    model_class = getattr(lmfit.models, SUPPORTED_MODELS[model])\n\n    # Build composite model\n    composite = None\n    params = lmfit.Parameters()\n\n    for i, center in enumerate(peak_positions):\n        prefix = f\"p{i}_\"\n        peak_model = model_class(prefix=prefix)\n\n        if composite is None:\n            composite = peak_model\n        else:\n            composite = composite + peak_model\n\n        # Initialize parameters near the peak position\n        peak_height = float(intensities[np.argmin(np.abs(wavenumbers - center))])\n        params.update(peak_model.make_params())\n        params[f\"{prefix}center\"].set(value=center)\n        params[f\"{prefix}amplitude\"].set(value=peak_height, min=0)\n\n    if composite is None:\n        raise ValueError(\"At least one peak position is required\")\n\n    result = composite.fit(intensities, params, x=wavenumbers, **kwargs)\n\n    # Extract components\n    components = []\n    param_list = []\n    for i in range(len(peak_positions)):\n        prefix = f\"p{i}_\"\n        component_params = {\n            k.replace(prefix, \"\"): v.value for k, v in result.params.items() if k.startswith(prefix)\n        }\n        param_list.append(component_params)\n\n        # Evaluate individual component\n        comp_model = model_class(prefix=prefix)\n        comp_vals = comp_model.eval(\n            params=result.params,\n            x=wavenumbers,\n        )\n        components.append(comp_vals)\n\n    return FitResult(\n        best_fit=result.best_fit,\n        components=components,\n        parameters=param_list,\n        residual=intensities - result.best_fit,\n        success=result.success,\n        info={\"redchi\": result.redchi, \"aic\": result.aic, \"bic\": result.bic},\n    )\n</code></pre>"},{"location":"api/contrib/#spectrakit.contrib._lmfit.FitResult","title":"spectrakit.contrib._lmfit.FitResult  <code>dataclass</code>","text":"<p>Container for peak fitting results.</p> <p>Attributes:</p> Name Type Description <code>best_fit</code> <code>ndarray</code> <p>Fitted curve, shape <code>(W,)</code>.</p> <code>components</code> <code>list[ndarray]</code> <p>Individual peak components, list of arrays.</p> <code>parameters</code> <code>list[dict[str, float]]</code> <p>Fitted parameter values per peak.</p> <code>residual</code> <code>ndarray</code> <p>Residual (data - fit), shape <code>(W,)</code>.</p> <code>success</code> <code>bool</code> <p>Whether the fit converged.</p> Source code in <code>src/spectrakit/contrib/_lmfit.py</code> <pre><code>@dataclass\nclass FitResult:\n    \"\"\"Container for peak fitting results.\n\n    Attributes:\n        best_fit: Fitted curve, shape ``(W,)``.\n        components: Individual peak components, list of arrays.\n        parameters: Fitted parameter values per peak.\n        residual: Residual (data - fit), shape ``(W,)``.\n        success: Whether the fit converged.\n    \"\"\"\n\n    best_fit: np.ndarray\n    components: list[np.ndarray]\n    parameters: list[dict[str, float]]\n    residual: np.ndarray\n    success: bool\n    info: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api/contrib/#spectrakit.contrib._lmfit.SUPPORTED_MODELS","title":"spectrakit.contrib._lmfit.SUPPORTED_MODELS  <code>module-attribute</code>","text":"<pre><code>SUPPORTED_MODELS = {\n    \"gaussian\": \"GaussianModel\",\n    \"lorentzian\": \"LorentzianModel\",\n    \"voigt\": \"VoigtModel\",\n    \"pseudo_voigt\": \"PseudoVoigtModel\",\n}\n</code></pre>"},{"location":"api/derivative/","title":"Derivatives","text":""},{"location":"api/derivative/#spectrakit.derivative.derivative_savgol","title":"spectrakit.derivative.derivative_savgol","text":"<pre><code>derivative_savgol(\n    intensities: ndarray,\n    window_length: int = DEFAULT_WINDOW_LENGTH,\n    polyorder: int = DEFAULT_POLYORDER,\n    deriv: int = DEFAULT_DERIV,\n    delta: float = 1.0,\n) -&gt; np.ndarray\n</code></pre> <p>Compute spectral derivative using Savitzky-Golay filter.</p> <p>Simultaneously smooths and differentiates spectral data. First and second derivatives are the most commonly used in chemometrics.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>window_length</code> <code>int</code> <p>Length of the filter window (must be odd and greater than <code>polyorder</code>).</p> <code>DEFAULT_WINDOW_LENGTH</code> <code>polyorder</code> <code>int</code> <p>Order of the polynomial used to fit the samples.</p> <code>DEFAULT_POLYORDER</code> <code>deriv</code> <code>int</code> <p>Order of the derivative to compute. Common values: 1 (first derivative) or 2 (second derivative).</p> <code>DEFAULT_DERIV</code> <code>delta</code> <code>float</code> <p>Spacing of the samples to which the filter is applied. Used to scale the derivative by <code>1 / delta**deriv</code>.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Derivative spectrum, same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <code>ValueError</code> <p>If parameters are invalid (window_length not odd, polyorder &gt;= window_length, deriv &lt; 0, delta &lt;= 0).</p> Source code in <code>src/spectrakit/derivative/savgol.py</code> <pre><code>def derivative_savgol(\n    intensities: np.ndarray,\n    window_length: int = DEFAULT_WINDOW_LENGTH,\n    polyorder: int = DEFAULT_POLYORDER,\n    deriv: int = DEFAULT_DERIV,\n    delta: float = 1.0,\n) -&gt; np.ndarray:\n    \"\"\"Compute spectral derivative using Savitzky-Golay filter.\n\n    Simultaneously smooths and differentiates spectral data. First and\n    second derivatives are the most commonly used in chemometrics.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)`` or ``(N, W)``.\n        window_length: Length of the filter window (must be odd and\n            greater than ``polyorder``).\n        polyorder: Order of the polynomial used to fit the samples.\n        deriv: Order of the derivative to compute. Common values:\n            1 (first derivative) or 2 (second derivative).\n        delta: Spacing of the samples to which the filter is applied.\n            Used to scale the derivative by ``1 / delta**deriv``.\n\n    Returns:\n        Derivative spectrum, same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n        ValueError: If parameters are invalid (window_length not odd,\n            polyorder &gt;= window_length, deriv &lt; 0, delta &lt;= 0).\n    \"\"\"\n    if window_length &lt; 1 or window_length % 2 == 0:\n        raise ValueError(f\"window_length must be a positive odd integer, got {window_length}\")\n    if polyorder &lt; 0:\n        raise ValueError(f\"polyorder must be non-negative, got {polyorder}\")\n    if polyorder &gt;= window_length:\n        raise ValueError(\n            f\"polyorder ({polyorder}) must be less than window_length ({window_length})\"\n        )\n    if deriv &lt; 0:\n        raise ValueError(f\"deriv must be non-negative, got {deriv}\")\n    if delta &lt;= 0:\n        raise ValueError(f\"delta must be positive, got {delta}\")\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    return apply_along_spectra(\n        _derivative_savgol_1d,\n        intensities,\n        window_length=window_length,\n        polyorder=polyorder,\n        deriv=deriv,\n        delta=delta,\n    )\n</code></pre>"},{"location":"api/derivative/#spectrakit.derivative.derivative_gap_segment","title":"spectrakit.derivative.derivative_gap_segment","text":"<pre><code>derivative_gap_segment(\n    intensities: ndarray,\n    gap: int = DEFAULT_GAP,\n    segment: int = DEFAULT_SEGMENT,\n    deriv: int = DEFAULT_DERIV,\n) -&gt; np.ndarray\n</code></pre> <p>Compute gap-segment derivative (Norris-Williams method).</p> <p>Averages over <code>segment</code> points then takes differences separated by <code>gap</code> points. Commonly used for NIR pre-treatment.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>gap</code> <code>int</code> <p>Gap size (number of points between segments).</p> <code>DEFAULT_GAP</code> <code>segment</code> <code>int</code> <p>Segment size (number of points to average).</p> <code>DEFAULT_SEGMENT</code> <code>deriv</code> <code>int</code> <p>Derivative order. 1 = first derivative, 2 = second.</p> <code>DEFAULT_DERIV</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Derivative spectrum, same shape as input (padded with zeros</p> <code>ndarray</code> <p>at edges where the computation is undefined).</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <code>ValueError</code> <p>If <code>deriv</code> is not 1 or 2.</p> Source code in <code>src/spectrakit/derivative/gap_segment.py</code> <pre><code>def derivative_gap_segment(\n    intensities: np.ndarray,\n    gap: int = DEFAULT_GAP,\n    segment: int = DEFAULT_SEGMENT,\n    deriv: int = DEFAULT_DERIV,\n) -&gt; np.ndarray:\n    \"\"\"Compute gap-segment derivative (Norris-Williams method).\n\n    Averages over ``segment`` points then takes differences separated\n    by ``gap`` points. Commonly used for NIR pre-treatment.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)`` or ``(N, W)``.\n        gap: Gap size (number of points between segments).\n        segment: Segment size (number of points to average).\n        deriv: Derivative order. 1 = first derivative, 2 = second.\n\n    Returns:\n        Derivative spectrum, same shape as input (padded with zeros\n        at edges where the computation is undefined).\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n        ValueError: If ``deriv`` is not 1 or 2.\n    \"\"\"\n    if deriv not in (1, 2):\n        raise ValueError(f\"deriv must be 1 or 2, got {deriv}\")\n    if gap &lt; 1:\n        raise ValueError(f\"gap must be &gt;= 1, got {gap}\")\n    if segment &lt; 1:\n        raise ValueError(f\"segment must be &gt;= 1, got {segment}\")\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    return apply_along_spectra(\n        _derivative_gap_segment_1d,\n        intensities,\n        gap=gap,\n        segment=segment,\n        deriv=deriv,\n    )\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#spectrakit.exceptions.SpectraKitError","title":"spectrakit.exceptions.SpectraKitError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all SpectraKit errors.</p> Source code in <code>src/spectrakit/exceptions.py</code> <pre><code>class SpectraKitError(Exception):\n    \"\"\"Base exception for all SpectraKit errors.\"\"\"\n</code></pre>"},{"location":"api/exceptions/#spectrakit.exceptions.SpectrumShapeError","title":"spectrakit.exceptions.SpectrumShapeError","text":"<p>               Bases: <code>SpectraKitError</code>, <code>ValueError</code></p> <p>Raised when array shapes are incompatible.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; raise SpectrumShapeError(\n...     \"Expected 1D or 2D array, got shape (3, 4, 5)\"\n... )\n</code></pre> Source code in <code>src/spectrakit/exceptions.py</code> <pre><code>class SpectrumShapeError(SpectraKitError, ValueError):\n    \"\"\"Raised when array shapes are incompatible.\n\n    Examples:\n        &gt;&gt;&gt; raise SpectrumShapeError(\n        ...     \"Expected 1D or 2D array, got shape (3, 4, 5)\"\n        ... )\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#spectrakit.exceptions.FileFormatError","title":"spectrakit.exceptions.FileFormatError","text":"<p>               Bases: <code>SpectraKitError</code>, <code>ValueError</code></p> <p>Raised when a spectral file cannot be parsed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; raise FileFormatError(\"No XYDATA block found in sample.jdx\")\n</code></pre> Source code in <code>src/spectrakit/exceptions.py</code> <pre><code>class FileFormatError(SpectraKitError, ValueError):\n    \"\"\"Raised when a spectral file cannot be parsed.\n\n    Examples:\n        &gt;&gt;&gt; raise FileFormatError(\"No XYDATA block found in sample.jdx\")\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#spectrakit.exceptions.DependencyError","title":"spectrakit.exceptions.DependencyError","text":"<p>               Bases: <code>SpectraKitError</code>, <code>ImportError</code></p> <p>Raised when an optional dependency is missing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; raise DependencyError(\n...     \"h5py is required for HDF5 files. \"\n...     \"Install with: pip install spectrakit[io]\"\n... )\n</code></pre> Source code in <code>src/spectrakit/exceptions.py</code> <pre><code>class DependencyError(SpectraKitError, ImportError):\n    \"\"\"Raised when an optional dependency is missing.\n\n    Examples:\n        &gt;&gt;&gt; raise DependencyError(\n        ...     \"h5py is required for HDF5 files. \"\n        ...     \"Install with: pip install spectrakit[io]\"\n        ... )\n    \"\"\"\n</code></pre>"},{"location":"api/exceptions/#spectrakit.exceptions.EmptySpectrumError","title":"spectrakit.exceptions.EmptySpectrumError","text":"<p>               Bases: <code>SpectraKitError</code>, <code>ValueError</code></p> <p>Raised when input spectrum is empty or has insufficient points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; raise EmptySpectrumError(\"intensities array is empty\")\n</code></pre> Source code in <code>src/spectrakit/exceptions.py</code> <pre><code>class EmptySpectrumError(SpectraKitError, ValueError):\n    \"\"\"Raised when input spectrum is empty or has insufficient points.\n\n    Examples:\n        &gt;&gt;&gt; raise EmptySpectrumError(\"intensities array is empty\")\n    \"\"\"\n</code></pre>"},{"location":"api/io/","title":"I/O \u2014 File Format Parsers","text":""},{"location":"api/io/#spectrakit.io.read_jcamp","title":"spectrakit.io.read_jcamp","text":"<pre><code>read_jcamp(path: str | Path) -&gt; Spectrum\n</code></pre> <p>Read a JCAMP-DX file and return a Spectrum.</p> <p>Parses ##XYDATA=(X++(Y..Y)) format. Supports AFFN (ASCII free-format numeric) encoding.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the .dx / .jdx / .jcamp file.</p> required <p>Returns:</p> Type Description <code>Spectrum</code> <p>Spectrum with intensities shape (W,) and wavenumbers shape (W,).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path does not exist.</p> <code>ValueError</code> <p>If the file cannot be parsed.</p> Source code in <code>src/spectrakit/io/jcamp.py</code> <pre><code>def read_jcamp(path: str | Path) -&gt; Spectrum:\n    \"\"\"Read a JCAMP-DX file and return a Spectrum.\n\n    Parses ##XYDATA=(X++(Y..Y)) format. Supports AFFN (ASCII free-format\n    numeric) encoding.\n\n    Args:\n        path: Path to the .dx / .jdx / .jcamp file.\n\n    Returns:\n        Spectrum with intensities shape (W,) and wavenumbers shape (W,).\n\n    Raises:\n        FileNotFoundError: If path does not exist.\n        ValueError: If the file cannot be parsed.\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"JCAMP file not found: {path}\")\n\n    validate_file_size(path.stat().st_size, path_name=str(path))\n\n    metadata: dict[str, str] = {}\n    x_values: list[float] = []\n    y_values: list[float] = []\n    in_xydata = False\n\n    with open(path, encoding=\"utf-8\", errors=\"replace\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n\n            ldr_match = _LDR_PATTERN.match(line)\n            if ldr_match:\n                key = ldr_match.group(1).strip().upper()\n                value = ldr_match.group(2).strip()\n\n                if key == \"XYDATA\":\n                    in_xydata = True\n                    continue\n                elif key == \"END\":\n                    in_xydata = False\n                    continue\n                else:\n                    metadata[key] = value\n                    in_xydata = False\n                    continue\n\n            if in_xydata:\n                numbers = _AFFN_NUMBER.findall(line)\n                if len(numbers) &gt;= 2:\n                    x_values.append(float(numbers[0]))\n                    for y_str in numbers[1:]:\n                        y_values.append(float(y_str))\n\n    if not y_values:\n        raise FileFormatError(f\"No XYDATA found in {path}\")\n\n    first_x = float(metadata.get(\"FIRSTX\", str(x_values[0])))\n    last_x = float(metadata.get(\"LASTX\", str(x_values[-1])))\n    n_points = len(y_values)\n\n    wavenumbers = np.linspace(first_x, last_x, n_points)\n    intensities = np.array(y_values, dtype=np.float64)\n\n    logger.debug(\"Read JCAMP: %d points from %s\", n_points, path.name)\n\n    return Spectrum(\n        intensities=intensities,\n        wavenumbers=wavenumbers,\n        metadata=metadata,\n        source_format=\"jcamp\",\n        label=path.stem,\n    )\n</code></pre>"},{"location":"api/io/#spectrakit.io.read_spc","title":"spectrakit.io.read_spc","text":"<pre><code>read_spc(path: str | Path) -&gt; Spectrum\n</code></pre> <p>Read a Galactic SPC file and return a Spectrum.</p> <p>Requires the <code>spc-spectra</code> package (install via <code>pip install spectrakit[io]</code>).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the .spc file.</p> required <p>Returns:</p> Type Description <code>Spectrum</code> <p>Spectrum with intensities shape (W,) or (N, W) for multi-trace files.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If spc-spectra is not installed.</p> <code>FileNotFoundError</code> <p>If path does not exist.</p> Source code in <code>src/spectrakit/io/spc.py</code> <pre><code>def read_spc(path: str | Path) -&gt; Spectrum:\n    \"\"\"Read a Galactic SPC file and return a Spectrum.\n\n    Requires the ``spc-spectra`` package (install via\n    ``pip install spectrakit[io]``).\n\n    Args:\n        path: Path to the .spc file.\n\n    Returns:\n        Spectrum with intensities shape (W,) or (N, W) for multi-trace files.\n\n    Raises:\n        ImportError: If spc-spectra is not installed.\n        FileNotFoundError: If path does not exist.\n    \"\"\"\n    try:\n        import spc\n    except ImportError as e:\n        raise DependencyError(\n            \"spc-spectra is required for SPC files. Install with: pip install spectrakit[io]\"\n        ) from e\n\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"SPC file not found: {path}\")\n\n    validate_file_size(path.stat().st_size, path_name=str(path))\n\n    f = spc.File(str(path))\n\n    if f.fnsub == 1:\n        wavenumbers = np.array(f.x, dtype=np.float64)\n        intensities = np.array(f.sub[0].y, dtype=np.float64)\n    else:\n        wavenumbers = np.array(f.x, dtype=np.float64)\n        intensities = np.array([sub.y for sub in f.sub], dtype=np.float64)\n\n    metadata = {\n        \"fnsub\": f.fnsub,\n        \"fexper\": getattr(f, \"fexper\", \"\"),\n    }\n\n    logger.debug(\"Read SPC: %s sub-spectra from %s\", f.fnsub, path.name)\n\n    return Spectrum(\n        intensities=intensities,\n        wavenumbers=wavenumbers,\n        metadata=metadata,\n        source_format=\"spc\",\n        label=path.stem,\n    )\n</code></pre>"},{"location":"api/io/#spectrakit.io.read_csv","title":"spectrakit.io.read_csv","text":"<pre><code>read_csv(\n    path: str | Path,\n    delimiter: str = \",\",\n    x_column: int = 0,\n    y_column: int = 1,\n    skip_header: int = 0,\n    orientation: Literal[\"columns\", \"rows\"] = \"columns\",\n) -&gt; Spectrum\n</code></pre> <p>Read spectral data from a CSV or TSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the CSV file.</p> required <code>delimiter</code> <code>str</code> <p>Column separator. Use \"\\t\" for TSV.</p> <code>','</code> <code>x_column</code> <code>int</code> <p>Index of the wavenumber/wavelength column. Set to -1 to indicate no x-axis column (y data only).</p> <code>0</code> <code>y_column</code> <code>int</code> <p>Index of the intensity column (for single-spectrum files). Ignored when orientation=\"rows\".</p> <code>1</code> <code>skip_header</code> <code>int</code> <p>Number of header lines to skip.</p> <code>0</code> <code>orientation</code> <code>Literal['columns', 'rows']</code> <p>\"columns\" means each column is a variable (x, y1, y2...); \"rows\" means each row is a full spectrum.</p> <code>'columns'</code> <p>Returns:</p> Type Description <code>Spectrum</code> <p>Spectrum with intensities and optional wavenumbers.</p> Source code in <code>src/spectrakit/io/csv.py</code> <pre><code>def read_csv(\n    path: str | Path,\n    delimiter: str = \",\",\n    x_column: int = 0,\n    y_column: int = 1,\n    skip_header: int = 0,\n    orientation: Literal[\"columns\", \"rows\"] = \"columns\",\n) -&gt; Spectrum:\n    \"\"\"Read spectral data from a CSV or TSV file.\n\n    Args:\n        path: Path to the CSV file.\n        delimiter: Column separator. Use \"\\\\t\" for TSV.\n        x_column: Index of the wavenumber/wavelength column. Set to -1\n            to indicate no x-axis column (y data only).\n        y_column: Index of the intensity column (for single-spectrum files).\n            Ignored when orientation=\"rows\".\n        skip_header: Number of header lines to skip.\n        orientation: \"columns\" means each column is a variable (x, y1, y2...);\n            \"rows\" means each row is a full spectrum.\n\n    Returns:\n        Spectrum with intensities and optional wavenumbers.\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"CSV file not found: {path}\")\n\n    validate_file_size(path.stat().st_size, path_name=str(path))\n\n    data = np.genfromtxt(\n        path,\n        delimiter=delimiter,\n        skip_header=skip_header,\n        dtype=np.float64,\n    )\n\n    if data.ndim == 1:\n        return Spectrum(\n            intensities=data,\n            source_format=\"csv\",\n            label=path.stem,\n        )\n\n    wavenumbers: np.ndarray | None = None\n\n    if orientation == \"columns\":\n        if x_column &gt;= 0:\n            wavenumbers = data[:, x_column]\n            y_cols = [i for i in range(data.shape[1]) if i != x_column]\n            if len(y_cols) == 1:\n                intensities = data[:, y_cols[0]]\n            else:\n                intensities = data[:, y_cols].T  # (N, W)\n        else:\n            if y_column &gt;= 0 and data.shape[1] &gt; 1:\n                intensities = data[:, y_column]\n            else:\n                intensities = data\n    else:\n        if x_column &gt;= 0:\n            wavenumbers = data[x_column, :]\n            intensities = np.delete(data, x_column, axis=0)\n        else:\n            intensities = data\n\n    logger.debug(\"Read CSV: shape %s from %s\", intensities.shape, path.name)\n\n    return Spectrum(\n        intensities=intensities,\n        wavenumbers=wavenumbers,\n        source_format=\"csv\",\n        label=path.stem,\n    )\n</code></pre>"},{"location":"api/io/#spectrakit.io.read_opus","title":"spectrakit.io.read_opus","text":"<pre><code>read_opus(path: str | Path) -&gt; Spectrum\n</code></pre> <p>Read a Bruker OPUS binary file and return a Spectrum.</p> <p>Parses the OPUS binary format natively without external dependencies. Extracts the absorbance/transmittance spectrum, wavenumber axis (computed from FXV/LXV parameters), and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the OPUS file (.0, .1, .2, etc.).</p> required <p>Returns:</p> Type Description <code>Spectrum</code> <p>Spectrum with intensities shape <code>(W,)</code> and wavenumbers <code>(W,)</code>.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path does not exist.</p> <code>FileFormatError</code> <p>If the file cannot be parsed as valid OPUS.</p> Source code in <code>src/spectrakit/io/opus.py</code> <pre><code>def read_opus(path: str | Path) -&gt; Spectrum:\n    \"\"\"Read a Bruker OPUS binary file and return a Spectrum.\n\n    Parses the OPUS binary format natively without external dependencies.\n    Extracts the absorbance/transmittance spectrum, wavenumber axis\n    (computed from FXV/LXV parameters), and metadata.\n\n    Args:\n        path: Path to the OPUS file (.0, .1, .2, etc.).\n\n    Returns:\n        Spectrum with intensities shape ``(W,)`` and wavenumbers ``(W,)``.\n\n    Raises:\n        FileNotFoundError: If *path* does not exist.\n        FileFormatError: If the file cannot be parsed as valid OPUS.\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"OPUS file not found: {path}\")\n\n    try:\n        raw = path.read_bytes()\n    except OSError as exc:\n        raise FileFormatError(f\"Cannot read OPUS file: {exc}\") from exc\n\n    validate_file_size(len(raw), path_name=str(path))\n\n    if len(raw) &lt; _MIN_FILE_SIZE:\n        raise FileFormatError(f\"File too small to be OPUS format ({len(raw)} bytes).\")\n\n    # \u2500\u2500 Parse block directory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    try:\n        entries = _parse_directory(raw)\n    except FileFormatError:\n        raise\n    except (struct.error, IndexError, TypeError, ValueError) as exc:  # pragma: no cover\n        raise FileFormatError(f\"Failed to parse OPUS directory: {exc}\") from exc\n\n    # \u2500\u2500 Locate data and parameter blocks \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Strategy: look for AB (absorbance) data first, then fall back to\n    # single-channel sample, then single-channel reference.\n    data_type_priority = [\n        _DATA_TYPE_AB,\n        _DATA_TYPE_SC_SAMPLE,\n        _DATA_TYPE_SC_REF,\n    ]\n    param_type_priority = [\n        _PARAM_DATA_STATUS,\n        _PARAM_SC_STATUS,\n        _PARAM_RF_STATUS,\n    ]\n\n    data_block: tuple[int, int, int] | None = None\n    param_block: tuple[int, int, int] | None = None\n\n    for data_type, param_type in zip(data_type_priority, param_type_priority, strict=True):\n        data_candidates = _find_blocks_by_type(entries, data_type)\n        param_candidates = _find_blocks_by_type(entries, param_type)\n        if data_candidates and param_candidates:\n            data_block = data_candidates[0]\n            param_block = param_candidates[0]\n            logger.debug(\n                \"Using data block type=0x%02X, param block type=0x%02X\",\n                data_type,\n                param_type,\n            )\n            break\n\n    # If we found data but no paired parameter block, try any parameter block\n    if data_block is None:\n        for data_type in data_type_priority:\n            data_candidates = _find_blocks_by_type(entries, data_type)\n            if data_candidates:\n                data_block = data_candidates[0]\n                break\n\n    if data_block is None:\n        raise FileFormatError(\n            \"No spectral data block found in OPUS file. \"\n            \"Searched for AB, single-channel sample, and reference blocks.\"\n        )\n\n    # Try all parameter block types if we don't have one yet\n    if param_block is None:\n        for param_type in param_type_priority:\n            param_candidates = _find_blocks_by_type(entries, param_type)\n            if param_candidates:\n                param_block = param_candidates[0]\n                break\n\n    if param_block is None:\n        raise FileFormatError(\n            \"No data parameter block found in OPUS file. Cannot determine NPT, FXV, LXV.\"\n        )\n\n    # \u2500\u2500 Extract spectral parameters (NPT, FXV, LXV) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    _, param_len, param_off = param_block\n    try:\n        params = _parse_parameter_block(raw, param_off, param_len)\n    except (struct.error, IndexError, UnicodeDecodeError, ValueError) as exc:  # pragma: no cover\n        raise FileFormatError(f\"Failed to parse OPUS parameter block: {exc}\") from exc\n\n    n_points = params.get(\"NPT\")\n    first_x = params.get(\"FXV\")\n    last_x = params.get(\"LXV\")\n\n    if n_points is None:\n        raise FileFormatError(\"NPT (number of points) not found in OPUS parameter block.\")\n    n_points = int(n_points)\n\n    if n_points &lt;= 0:\n        raise FileFormatError(f\"Invalid NPT value in OPUS file: {n_points}\")\n\n    # \u2500\u2500 Read spectral data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    _, _data_len, data_off = data_block\n    try:\n        intensities = _read_float32_block(raw, data_off, n_points)\n    except FileFormatError:\n        raise\n    except (struct.error, ValueError, TypeError, IndexError) as exc:  # pragma: no cover\n        raise FileFormatError(f\"Failed to read OPUS data block: {exc}\") from exc\n\n    # \u2500\u2500 Build wavenumber axis \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    wavenumbers: np.ndarray | None = None\n    if first_x is not None and last_x is not None:\n        wavenumbers = np.linspace(float(first_x), float(last_x), n_points, dtype=np.float64)\n\n    # \u2500\u2500 Collect metadata from instrument/sample parameter blocks \u2500\u2500\u2500\u2500\n    metadata: dict[str, Any] = {}\n    metadata_block_types = [_PARAM_INSTRUMENT, _PARAM_SAMPLE]\n    for mtype in metadata_block_types:\n        for _, mlen, moff in _find_blocks_by_type(entries, mtype):\n            try:\n                block_params = _parse_parameter_block(raw, moff, mlen)\n                metadata.update(block_params)\n            except (struct.error, ValueError, IndexError) as exc:  # pragma: no cover\n                logger.debug(\"Could not parse metadata block at offset %d: %s\", moff, exc)\n\n    # Also include the data parameters in metadata\n    metadata.update(params)\n\n    logger.debug(\"Read OPUS: %d points from %s\", n_points, path.name)\n\n    return Spectrum(\n        intensities=intensities,\n        wavenumbers=wavenumbers,\n        metadata=metadata,\n        source_format=\"opus\",\n        label=path.stem,\n    )\n</code></pre>"},{"location":"api/io/#spectrakit.io.read_hdf5","title":"spectrakit.io.read_hdf5","text":"<pre><code>read_hdf5(\n    path: str | Path,\n    intensities_key: str = \"intensities\",\n    wavenumbers_key: str = \"wavenumbers\",\n) -&gt; Spectrum\n</code></pre> <p>Read spectral data from an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the .h5 / .hdf5 file.</p> required <code>intensities_key</code> <code>str</code> <p>Dataset key for intensity values.</p> <code>'intensities'</code> <code>wavenumbers_key</code> <code>str</code> <p>Dataset key for wavenumber values.</p> <code>'wavenumbers'</code> <p>Returns:</p> Type Description <code>Spectrum</code> <p>Spectrum loaded from the HDF5 datasets.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If h5py is not installed.</p> <code>FileNotFoundError</code> <p>If path does not exist.</p> Source code in <code>src/spectrakit/io/hdf5.py</code> <pre><code>def read_hdf5(\n    path: str | Path,\n    intensities_key: str = \"intensities\",\n    wavenumbers_key: str = \"wavenumbers\",\n) -&gt; Spectrum:\n    \"\"\"Read spectral data from an HDF5 file.\n\n    Args:\n        path: Path to the .h5 / .hdf5 file.\n        intensities_key: Dataset key for intensity values.\n        wavenumbers_key: Dataset key for wavenumber values.\n\n    Returns:\n        Spectrum loaded from the HDF5 datasets.\n\n    Raises:\n        ImportError: If h5py is not installed.\n        FileNotFoundError: If path does not exist.\n    \"\"\"\n    try:\n        import h5py\n    except ImportError as e:\n        raise DependencyError(\n            \"h5py is required for HDF5 files. Install with: pip install spectrakit[io]\"\n        ) from e\n\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"HDF5 file not found: {path}\")\n\n    validate_file_size(path.stat().st_size, path_name=str(path))\n\n    with h5py.File(path, \"r\") as f:\n        intensities = np.array(f[intensities_key], dtype=np.float64)\n        wavenumbers = None\n        if wavenumbers_key in f:\n            wavenumbers = np.array(f[wavenumbers_key], dtype=np.float64)\n\n        metadata: dict[str, Any] = {}\n        for key, value in f.attrs.items():\n            metadata[key] = value\n\n    logger.debug(\"Read HDF5: shape %s from %s\", intensities.shape, path.name)\n\n    return Spectrum(\n        intensities=intensities,\n        wavenumbers=wavenumbers,\n        metadata=metadata,\n        source_format=\"hdf5\",\n        label=path.stem,\n    )\n</code></pre>"},{"location":"api/io/#spectrakit.io.write_hdf5","title":"spectrakit.io.write_hdf5","text":"<pre><code>write_hdf5(\n    spectrum: Spectrum,\n    path: str | Path,\n    intensities_key: str = \"intensities\",\n    wavenumbers_key: str = \"wavenumbers\",\n) -&gt; None\n</code></pre> <p>Write a Spectrum to an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>spectrum</code> <code>Spectrum</code> <p>Spectrum to save.</p> required <code>path</code> <code>str | Path</code> <p>Output file path.</p> required <code>intensities_key</code> <code>str</code> <p>Dataset key for intensity values.</p> <code>'intensities'</code> <code>wavenumbers_key</code> <code>str</code> <p>Dataset key for wavenumber values.</p> <code>'wavenumbers'</code> Source code in <code>src/spectrakit/io/hdf5.py</code> <pre><code>def write_hdf5(\n    spectrum: Spectrum,\n    path: str | Path,\n    intensities_key: str = \"intensities\",\n    wavenumbers_key: str = \"wavenumbers\",\n) -&gt; None:\n    \"\"\"Write a Spectrum to an HDF5 file.\n\n    Args:\n        spectrum: Spectrum to save.\n        path: Output file path.\n        intensities_key: Dataset key for intensity values.\n        wavenumbers_key: Dataset key for wavenumber values.\n    \"\"\"\n    try:\n        import h5py\n    except ImportError as e:\n        raise DependencyError(\n            \"h5py is required for HDF5 files. Install with: pip install spectrakit[io]\"\n        ) from e\n\n    path = Path(path)\n    with h5py.File(path, \"w\") as f:\n        f.create_dataset(intensities_key, data=spectrum.intensities)\n        if spectrum.wavenumbers is not None:\n            f.create_dataset(wavenumbers_key, data=spectrum.wavenumbers)\n        for key, value in spectrum.metadata.items():\n            try:\n                f.attrs[key] = value\n            except TypeError:\n                f.attrs[key] = str(value)\n\n    logger.debug(\"Wrote HDF5: %s\", path)\n</code></pre>"},{"location":"api/normalize/","title":"Normalization","text":""},{"location":"api/normalize/#spectrakit.normalize.normalize_snv","title":"spectrakit.normalize.normalize_snv","text":"<pre><code>normalize_snv(intensities: ndarray) -&gt; np.ndarray\n</code></pre> <p>Apply Standard Normal Variate normalization.</p> <p>Centers each spectrum to zero mean and unit variance. Removes multiplicative scatter effects common in diffuse reflectance data.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>SNV-normalized intensities, same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/normalize/snv.py</code> <pre><code>def normalize_snv(intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Apply Standard Normal Variate normalization.\n\n    Centers each spectrum to zero mean and unit variance. Removes\n    multiplicative scatter effects common in diffuse reflectance data.\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n\n    Returns:\n        SNV-normalized intensities, same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if intensities.ndim == 1:\n        mean = np.mean(intensities)\n        std = np.std(intensities)\n        if std &lt; EPSILON:\n            logger.warning(\"SNV: near-zero std (%.2e), returning zero-centered\", std)\n            return intensities - mean  # type: ignore[no-any-return]\n        return (intensities - mean) / std  # type: ignore[no-any-return]\n\n    means = np.mean(intensities, axis=1, keepdims=True)\n    stds = np.std(intensities, axis=1, keepdims=True)\n    degenerate = stds &lt; EPSILON\n    stds = np.where(degenerate, 1.0, stds)\n    n_constant = int(np.sum(degenerate))\n    if n_constant &gt; 0:\n        warnings.warn(\n            f\"SNV: {n_constant} spectrum/spectra have near-zero std and \"\n            \"will be zero-centered only (not variance-scaled).\",\n            stacklevel=2,\n        )\n\n    return (intensities - means) / stds  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/normalize/#spectrakit.normalize.normalize_minmax","title":"spectrakit.normalize.normalize_minmax","text":"<pre><code>normalize_minmax(intensities: ndarray) -&gt; np.ndarray\n</code></pre> <p>Scale intensities to the [0, 1] range per spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Min-max normalized intensities, same shape.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/normalize/minmax.py</code> <pre><code>def normalize_minmax(intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Scale intensities to the [0, 1] range per spectrum.\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n\n    Returns:\n        Min-max normalized intensities, same shape.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if intensities.ndim == 1:\n        mn = intensities.min()\n        mx = intensities.max()\n        rng = mx - mn\n        if rng &lt; EPSILON:\n            warnings.warn(\n                \"Min-max: constant spectrum (range &lt; epsilon), returning zeros.\",\n                stacklevel=2,\n            )\n            return np.zeros_like(intensities)\n        return (intensities - mn) / rng  # type: ignore[no-any-return]\n\n    mins = intensities.min(axis=1, keepdims=True)\n    maxs = intensities.max(axis=1, keepdims=True)\n    rngs = maxs - mins\n    degenerate = rngs &lt; EPSILON\n    rngs = np.where(degenerate, 1.0, rngs)\n    n_constant = int(np.sum(degenerate))\n    if n_constant &gt; 0:\n        warnings.warn(\n            f\"Min-max: {n_constant} spectrum/spectra have near-zero range and \"\n            \"will be returned as zeros.\",\n            stacklevel=2,\n        )\n\n    return (intensities - mins) / rngs  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/normalize/#spectrakit.normalize.normalize_area","title":"spectrakit.normalize.normalize_area","text":"<pre><code>normalize_area(\n    intensities: ndarray, wavenumbers: ndarray | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Normalize spectra so that the area under each curve equals 1.</p> <p>Uses the trapezoidal rule for integration. If wavenumbers are not provided, assumes unit spacing.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <code>wavenumbers</code> <code>ndarray | None</code> <p>X-axis values, shape (W,). Used for proper integration spacing. None assumes unit spacing.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Area-normalized intensities, same shape.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/normalize/area.py</code> <pre><code>def normalize_area(\n    intensities: np.ndarray,\n    wavenumbers: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Normalize spectra so that the area under each curve equals 1.\n\n    Uses the trapezoidal rule for integration. If wavenumbers are not\n    provided, assumes unit spacing.\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n        wavenumbers: X-axis values, shape (W,). Used for proper\n            integration spacing. None assumes unit spacing.\n\n    Returns:\n        Area-normalized intensities, same shape.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if intensities.ndim == 1:\n        area = np.trapezoid(np.abs(intensities), x=wavenumbers)\n        if area &lt; EPSILON:\n            warnings.warn(\n                \"Area normalization: near-zero area, returning spectrum unchanged.\",\n                stacklevel=2,\n            )\n            return intensities\n        return intensities / area  # type: ignore[no-any-return]\n\n    areas = np.trapezoid(np.abs(intensities), x=wavenumbers, axis=1).reshape(-1, 1)\n    degenerate = areas &lt; EPSILON\n    areas = np.where(degenerate, 1.0, areas)\n    n_zero = int(np.sum(degenerate))\n    if n_zero &gt; 0:\n        warnings.warn(\n            f\"Area normalization: {n_zero} spectrum/spectra have near-zero area \"\n            \"and will be returned unchanged.\",\n            stacklevel=2,\n        )\n\n    return intensities / areas  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/normalize/#spectrakit.normalize.normalize_vector","title":"spectrakit.normalize.normalize_vector","text":"<pre><code>normalize_vector(intensities: ndarray) -&gt; np.ndarray\n</code></pre> <p>Normalize each spectrum to unit L2 norm.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape (W,) or (N, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>L2-normalized intensities, same shape.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/normalize/vector.py</code> <pre><code>def normalize_vector(intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize each spectrum to unit L2 norm.\n\n    Args:\n        intensities: Spectral intensities, shape (W,) or (N, W).\n\n    Returns:\n        L2-normalized intensities, same shape.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if intensities.ndim == 1:\n        norm = np.linalg.norm(intensities)\n        if norm &lt; EPSILON:\n            warnings.warn(\n                \"Vector normalization: near-zero L2 norm, returning spectrum unchanged.\",\n                stacklevel=2,\n            )\n            return intensities\n        return intensities / norm  # type: ignore[no-any-return]\n\n    norms = np.linalg.norm(intensities, axis=1, keepdims=True)\n    degenerate = norms &lt; EPSILON\n    norms = np.where(degenerate, 1.0, norms)\n    n_zero = int(np.sum(degenerate))\n    if n_zero &gt; 0:\n        warnings.warn(\n            f\"Vector normalization: {n_zero} spectrum/spectra have near-zero norm \"\n            \"and will be returned unchanged.\",\n            stacklevel=2,\n        )\n\n    return intensities / norms  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/ops/","title":"Spectral Operations","text":""},{"location":"api/ops/#spectrakit.ops.spectral_subtract","title":"spectrakit.ops.spectral_subtract","text":"<pre><code>spectral_subtract(\n    spectrum: ndarray,\n    background: ndarray,\n    factor: float = 1.0,\n) -&gt; np.ndarray\n</code></pre> <p>Subtract background from spectrum.</p> <p>Computes <code>spectrum - factor * background</code>. Useful for background subtraction, solvent subtraction, or difference spectroscopy.</p> <p>Parameters:</p> Name Type Description Default <code>spectrum</code> <code>ndarray</code> <p>Spectrum or batch, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>background</code> <code>ndarray</code> <p>Spectrum to subtract, shape <code>(W,)</code>. If spectrum is 2-D, background is subtracted from every row.</p> required <code>factor</code> <code>float</code> <p>Scaling factor for background before subtraction.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Difference spectrum, same shape as spectrum.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If shapes are incompatible (different number of wavelength points).</p> <code>EmptySpectrumError</code> <p>If spectrum has zero elements.</p> Source code in <code>src/spectrakit/ops/subtract.py</code> <pre><code>def spectral_subtract(\n    spectrum: np.ndarray,\n    background: np.ndarray,\n    factor: float = 1.0,\n) -&gt; np.ndarray:\n    \"\"\"Subtract *background* from *spectrum*.\n\n    Computes ``spectrum - factor * background``. Useful for background\n    subtraction, solvent subtraction, or difference spectroscopy.\n\n    Args:\n        spectrum: Spectrum or batch, shape ``(W,)`` or ``(N, W)``.\n        background: Spectrum to subtract, shape ``(W,)``. If *spectrum*\n            is 2-D, *background* is subtracted from every row.\n        factor: Scaling factor for *background* before subtraction.\n\n    Returns:\n        Difference spectrum, same shape as *spectrum*.\n\n    Raises:\n        SpectrumShapeError: If shapes are incompatible (different number\n            of wavelength points).\n        EmptySpectrumError: If *spectrum* has zero elements.\n    \"\"\"\n    spectrum = ensure_float64(spectrum)\n    background = ensure_float64(background)\n    validate_1d_or_2d(spectrum, name=\"spectrum\")\n    warn_if_not_finite(spectrum, name=\"spectrum\")\n    warn_if_not_finite(background, name=\"background\")\n\n    # Validate compatible shapes\n    spec_w = spectrum.shape[-1]\n    bg_w = background.shape[-1] if background.ndim &gt;= 1 else 0\n    if spec_w != bg_w:\n        raise SpectrumShapeError(f\"spectrum has {spec_w} points but background has {bg_w} points\")\n\n    if spectrum.ndim == 1:\n        return spectrum - factor * background\n\n    return spectrum - factor * background[np.newaxis, :]\n</code></pre>"},{"location":"api/ops/#spectrakit.ops.spectral_average","title":"spectrakit.ops.spectral_average","text":"<pre><code>spectral_average(intensities: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the mean spectrum from a batch.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral batch, shape <code>(N, W)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Mean spectrum, shape <code>(W,)</code>.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 2-D.</p> Source code in <code>src/spectrakit/ops/average.py</code> <pre><code>def spectral_average(intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the mean spectrum from a batch.\n\n    Args:\n        intensities: Spectral batch, shape ``(N, W)``.\n\n    Returns:\n        Mean spectrum, shape ``(W,)``.\n\n    Raises:\n        SpectrumShapeError: If input is not 2-D.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if intensities.ndim == 1:\n        return intensities.copy()\n\n    return np.mean(intensities, axis=0)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/ops/#spectrakit.ops.spectral_interpolate","title":"spectrakit.ops.spectral_interpolate","text":"<pre><code>spectral_interpolate(\n    intensities: ndarray,\n    wavenumbers: ndarray,\n    new_wavenumbers: ndarray,\n    kind: str = \"linear\",\n) -&gt; np.ndarray\n</code></pre> <p>Interpolate spectra onto a new wavenumber axis.</p> <p>Useful for aligning spectra measured on different instruments or resampling to a uniform grid.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>wavenumbers</code> <code>ndarray</code> <p>Original wavenumber axis, shape <code>(W,)</code>.</p> required <code>new_wavenumbers</code> <code>ndarray</code> <p>Target wavenumber axis, shape <code>(M,)</code>.</p> required <code>kind</code> <code>str</code> <p>Interpolation method (<code>\"linear\"</code>, <code>\"cubic\"</code>, etc.). Passed to <code>scipy.interpolate.interp1d</code>.</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Interpolated intensities, shape <code>(M,)</code> or <code>(N, M)</code>.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If intensities is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If intensities has zero elements.</p> Source code in <code>src/spectrakit/ops/interpolate.py</code> <pre><code>def spectral_interpolate(\n    intensities: np.ndarray,\n    wavenumbers: np.ndarray,\n    new_wavenumbers: np.ndarray,\n    kind: str = \"linear\",\n) -&gt; np.ndarray:\n    \"\"\"Interpolate spectra onto a new wavenumber axis.\n\n    Useful for aligning spectra measured on different instruments or\n    resampling to a uniform grid.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)`` or ``(N, W)``.\n        wavenumbers: Original wavenumber axis, shape ``(W,)``.\n        new_wavenumbers: Target wavenumber axis, shape ``(M,)``.\n        kind: Interpolation method (``\"linear\"``, ``\"cubic\"``, etc.).\n            Passed to ``scipy.interpolate.interp1d``.\n\n    Returns:\n        Interpolated intensities, shape ``(M,)`` or ``(N, M)``.\n\n    Raises:\n        SpectrumShapeError: If *intensities* is not 1-D or 2-D.\n        EmptySpectrumError: If *intensities* has zero elements.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    wavenumbers = ensure_float64(wavenumbers)\n    new_wavenumbers = ensure_float64(new_wavenumbers)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    expected_w = intensities.shape[-1]\n    if wavenumbers.shape[0] != expected_w:\n        raise ValueError(\n            f\"wavenumbers length {wavenumbers.shape[0]} does not match \"\n            f\"intensities spectral width {expected_w}\"\n        )\n\n    if new_wavenumbers.min() &lt; wavenumbers.min() or new_wavenumbers.max() &gt; wavenumbers.max():\n        warnings.warn(\n            \"new_wavenumbers extends beyond the original range \"\n            f\"[{wavenumbers.min():.2f}, {wavenumbers.max():.2f}]. \"\n            \"Extrapolated values may be unreliable.\",\n            stacklevel=2,\n        )\n\n    return apply_along_spectra(\n        _interpolate_1d,\n        intensities,\n        wavenumbers=wavenumbers,\n        new_wavenumbers=new_wavenumbers,\n        kind=kind,\n    )\n</code></pre>"},{"location":"api/peaks/","title":"Peak Analysis","text":""},{"location":"api/peaks/#spectrakit.peaks.peaks_find","title":"spectrakit.peaks.peaks_find","text":"<pre><code>peaks_find(\n    intensities: ndarray,\n    wavenumbers: ndarray | None = None,\n    height: float | None = None,\n    distance: int = DEFAULT_DISTANCE,\n    prominence: float | None = None,\n) -&gt; PeakResult\n</code></pre> <p>Find peaks in a 1-D spectrum.</p> <p>Wraps <code>scipy.signal.find_peaks</code> with spectroscopy-friendly defaults and returns a structured result.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code>.</p> required <code>wavenumbers</code> <code>ndarray | None</code> <p>Wavenumber axis, shape <code>(W,)</code>. Used to report peak positions in wavenumber units.</p> <code>None</code> <code>height</code> <code>float | None</code> <p>Minimum peak height. If <code>None</code>, uses the 10th percentile of the spectrum as a threshold.</p> <code>None</code> <code>distance</code> <code>int</code> <p>Minimum number of points between peaks.</p> <code>DEFAULT_DISTANCE</code> <code>prominence</code> <code>float | None</code> <p>Minimum peak prominence. If <code>None</code>, no prominence filter is applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>PeakResult</code> <p><code>PeakResult</code> with indices, heights, and optional wavenumbers.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D.</p> Source code in <code>src/spectrakit/peaks/find.py</code> <pre><code>def peaks_find(\n    intensities: np.ndarray,\n    wavenumbers: np.ndarray | None = None,\n    height: float | None = None,\n    distance: int = DEFAULT_DISTANCE,\n    prominence: float | None = None,\n) -&gt; PeakResult:\n    \"\"\"Find peaks in a 1-D spectrum.\n\n    Wraps ``scipy.signal.find_peaks`` with spectroscopy-friendly\n    defaults and returns a structured result.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)``.\n        wavenumbers: Wavenumber axis, shape ``(W,)``. Used to report\n            peak positions in wavenumber units.\n        height: Minimum peak height. If ``None``, uses the 10th\n            percentile of the spectrum as a threshold.\n        distance: Minimum number of points between peaks.\n        prominence: Minimum peak prominence. If ``None``, no\n            prominence filter is applied.\n\n    Returns:\n        ``PeakResult`` with indices, heights, and optional wavenumbers.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n\n    if intensities.ndim != 1:\n        raise SpectrumShapeError(\"peaks_find requires a 1-D spectrum. For batches, call per-row.\")\n\n    if height is None:\n        height = float(np.percentile(intensities, DEFAULT_HEIGHT_PERCENTILE))\n\n    kwargs: dict[str, float | int] = {\"height\": height, \"distance\": distance}\n    if prominence is not None:\n        kwargs[\"prominence\"] = prominence\n\n    indices, properties = scipy_find_peaks(intensities, **kwargs)\n\n    peak_wavenumbers = None\n    if wavenumbers is not None:\n        wavenumbers = ensure_float64(wavenumbers)\n        peak_wavenumbers = wavenumbers[indices]\n\n    return PeakResult(\n        indices=indices,\n        heights=intensities[indices],\n        wavenumbers=peak_wavenumbers,\n        properties=properties,\n    )\n</code></pre>"},{"location":"api/peaks/#spectrakit.peaks.PeakResult","title":"spectrakit.peaks.PeakResult  <code>dataclass</code>","text":"<p>Container for peak detection results.</p> <p>Attributes:</p> Name Type Description <code>indices</code> <code>ndarray</code> <p>Array of peak indices, shape <code>(P,)</code>.</p> <code>heights</code> <code>ndarray</code> <p>Peak heights at the detected positions, shape <code>(P,)</code>.</p> <code>wavenumbers</code> <code>ndarray | None</code> <p>Peak wavenumber positions if wavenumbers were provided, shape <code>(P,)</code>. <code>None</code> otherwise.</p> Source code in <code>src/spectrakit/peaks/find.py</code> <pre><code>@dataclass\nclass PeakResult:\n    \"\"\"Container for peak detection results.\n\n    Attributes:\n        indices: Array of peak indices, shape ``(P,)``.\n        heights: Peak heights at the detected positions, shape ``(P,)``.\n        wavenumbers: Peak wavenumber positions if wavenumbers were\n            provided, shape ``(P,)``. ``None`` otherwise.\n    \"\"\"\n\n    indices: np.ndarray\n    heights: np.ndarray\n    wavenumbers: np.ndarray | None = None\n    properties: dict[str, np.ndarray] = field(default_factory=dict)\n</code></pre>"},{"location":"api/peaks/#spectrakit.peaks.peaks_integrate","title":"spectrakit.peaks.peaks_integrate","text":"<pre><code>peaks_integrate(\n    intensities: ndarray,\n    wavenumbers: ndarray | None = None,\n    ranges: list[tuple[float, float]] | None = None,\n) -&gt; np.ndarray | float\n</code></pre> <p>Integrate peak areas over specified wavenumber ranges.</p> <p>If <code>ranges</code> is provided, computes the trapezoidal integral for each range. Otherwise, integrates the entire spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code>.</p> required <code>wavenumbers</code> <code>ndarray | None</code> <p>Wavenumber axis, shape <code>(W,)</code>. Required when <code>ranges</code> is specified.</p> <code>None</code> <code>ranges</code> <code>list[tuple[float, float]] | None</code> <p>List of <code>(start, end)</code> wavenumber ranges to integrate. Each range defines a spectral region. If <code>None</code>, integrates the full spectrum.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | float</code> <p>If <code>ranges</code> is <code>None</code>, a scalar (total area). If <code>ranges</code></p> <code>ndarray | float</code> <p>is provided, an array of shape <code>(len(ranges),)</code> with the area</p> <code>ndarray | float</code> <p>for each range.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If ranges is specified but wavenumbers is <code>None</code>.</p> Source code in <code>src/spectrakit/peaks/integrate.py</code> <pre><code>def peaks_integrate(\n    intensities: np.ndarray,\n    wavenumbers: np.ndarray | None = None,\n    ranges: list[tuple[float, float]] | None = None,\n) -&gt; np.ndarray | float:\n    \"\"\"Integrate peak areas over specified wavenumber ranges.\n\n    If ``ranges`` is provided, computes the trapezoidal integral for\n    each range. Otherwise, integrates the entire spectrum.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)``.\n        wavenumbers: Wavenumber axis, shape ``(W,)``. Required when\n            ``ranges`` is specified.\n        ranges: List of ``(start, end)`` wavenumber ranges to integrate.\n            Each range defines a spectral region. If ``None``, integrates\n            the full spectrum.\n\n    Returns:\n        If ``ranges`` is ``None``, a scalar (total area). If ``ranges``\n        is provided, an array of shape ``(len(ranges),)`` with the area\n        for each range.\n\n    Raises:\n        ValueError: If *ranges* is specified but *wavenumbers* is ``None``.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    if intensities.ndim != 1:\n        raise SpectrumShapeError(\n            f\"peaks_integrate requires 1-D input, got shape {intensities.shape}. \"\n            \"Apply row-by-row for 2-D batches.\"\n        )\n    warn_if_not_finite(intensities)\n\n    if ranges is None:\n        return float(np.trapezoid(intensities, x=wavenumbers))\n\n    if wavenumbers is None:\n        raise ValueError(\"wavenumbers are required when ranges is specified\")\n\n    wavenumbers = ensure_float64(wavenumbers)\n    areas = []\n\n    for start, end in ranges:\n        low, high = min(start, end), max(start, end)\n        mask = (wavenumbers &gt;= low) &amp; (wavenumbers &lt;= high)\n\n        if not np.any(mask):\n            areas.append(0.0)\n            continue\n\n        region_wn = wavenumbers[mask]\n        region_y = intensities[mask]\n        areas.append(float(np.trapezoid(region_y, x=region_wn)))\n\n    return np.array(areas, dtype=np.float64)\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline","text":""},{"location":"api/pipeline/#spectrakit.pipeline.Pipeline","title":"spectrakit.pipeline.Pipeline","text":"<p>Chain spectral processing steps into a reusable pipeline.</p> <p>Each step is a callable that takes a numpy array (W,) or (N, W) and returns the same shape. Steps are executed in order.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spectrakit import Pipeline, baseline_als, normalize_snv\n&gt;&gt;&gt; pipe = Pipeline()\n&gt;&gt;&gt; pipe.add(\"baseline\", baseline_als, lam=1e6)\n&gt;&gt;&gt; pipe.add(\"normalize\", normalize_snv)\n&gt;&gt;&gt; corrected = pipe.transform(raw_intensities)\n</code></pre> Source code in <code>src/spectrakit/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Chain spectral processing steps into a reusable pipeline.\n\n    Each step is a callable that takes a numpy array (W,) or (N, W)\n    and returns the same shape. Steps are executed in order.\n\n    Examples:\n        &gt;&gt;&gt; from spectrakit import Pipeline, baseline_als, normalize_snv\n        &gt;&gt;&gt; pipe = Pipeline()\n        &gt;&gt;&gt; pipe.add(\"baseline\", baseline_als, lam=1e6)\n        &gt;&gt;&gt; pipe.add(\"normalize\", normalize_snv)\n        &gt;&gt;&gt; corrected = pipe.transform(raw_intensities)\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: list[tuple[str, Callable[..., np.ndarray], dict[str, Any]]] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize pipeline with optional named steps.\n\n        Args:\n            steps: List of (name, callable, kwargs) tuples.\n        \"\"\"\n        self.steps: list[tuple[str, Callable[..., np.ndarray], dict[str, Any]]] = steps or []\n\n    def add(\n        self,\n        name: str,\n        fn: Callable[..., np.ndarray],\n        **kwargs: Any,\n    ) -&gt; Pipeline:\n        \"\"\"Add a processing step to the pipeline.\n\n        Args:\n            name: Human-readable step name for logging.\n            fn: Processing function (e.g., baseline_als, normalize_snv).\n            **kwargs: Keyword arguments passed to fn.\n\n        Returns:\n            Self, for method chaining.\n        \"\"\"\n        self.steps.append((name, fn, kwargs))\n        return self\n\n    def transform(self, intensities: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Apply all pipeline steps to the input.\n\n        Args:\n            intensities: Input spectral data, shape (W,) or (N, W).\n\n        Returns:\n            Processed spectral data, same shape.\n        \"\"\"\n        result = intensities.copy()\n        for name, fn, kwargs in self.steps:\n            logger.debug(\"Pipeline step: %s\", name)\n            result = fn(result, **kwargs)\n        return result\n\n    def transform_spectrum(self, spectrum: Spectrum) -&gt; Spectrum:\n        \"\"\"Apply pipeline to a Spectrum, returning a new Spectrum.\n\n        Args:\n            spectrum: Input Spectrum.\n\n        Returns:\n            New Spectrum with processed intensities.\n        \"\"\"\n        new_intensities = self.transform(spectrum.intensities)\n        return Spectrum(\n            intensities=new_intensities,\n            wavenumbers=spectrum.wavenumbers.copy() if spectrum.wavenumbers is not None else None,\n            metadata={**spectrum.metadata, \"pipeline_steps\": [s[0] for s in self.steps]},\n            source_format=spectrum.source_format,\n            label=spectrum.label,\n        )\n\n    def __repr__(self) -&gt; str:\n        step_names = [name for name, _, _ in self.steps]\n        return f\"Pipeline(steps={step_names})\"\n</code></pre>"},{"location":"api/pipeline/#spectrakit.pipeline.Pipeline.__init__","title":"__init__","text":"<pre><code>__init__(\n    steps: list[\n        tuple[str, Callable[..., ndarray], dict[str, Any]]\n    ]\n    | None = None,\n) -&gt; None\n</code></pre> <p>Initialize pipeline with optional named steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>list[tuple[str, Callable[..., ndarray], dict[str, Any]]] | None</code> <p>List of (name, callable, kwargs) tuples.</p> <code>None</code> Source code in <code>src/spectrakit/pipeline.py</code> <pre><code>def __init__(\n    self,\n    steps: list[tuple[str, Callable[..., np.ndarray], dict[str, Any]]] | None = None,\n) -&gt; None:\n    \"\"\"Initialize pipeline with optional named steps.\n\n    Args:\n        steps: List of (name, callable, kwargs) tuples.\n    \"\"\"\n    self.steps: list[tuple[str, Callable[..., np.ndarray], dict[str, Any]]] = steps or []\n</code></pre>"},{"location":"api/pipeline/#spectrakit.pipeline.Pipeline.add","title":"add","text":"<pre><code>add(\n    name: str, fn: Callable[..., ndarray], **kwargs: Any\n) -&gt; Pipeline\n</code></pre> <p>Add a processing step to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable step name for logging.</p> required <code>fn</code> <code>Callable[..., ndarray]</code> <p>Processing function (e.g., baseline_als, normalize_snv).</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to fn.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self, for method chaining.</p> Source code in <code>src/spectrakit/pipeline.py</code> <pre><code>def add(\n    self,\n    name: str,\n    fn: Callable[..., np.ndarray],\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"Add a processing step to the pipeline.\n\n    Args:\n        name: Human-readable step name for logging.\n        fn: Processing function (e.g., baseline_als, normalize_snv).\n        **kwargs: Keyword arguments passed to fn.\n\n    Returns:\n        Self, for method chaining.\n    \"\"\"\n    self.steps.append((name, fn, kwargs))\n    return self\n</code></pre>"},{"location":"api/pipeline/#spectrakit.pipeline.Pipeline.transform","title":"transform","text":"<pre><code>transform(intensities: ndarray) -&gt; np.ndarray\n</code></pre> <p>Apply all pipeline steps to the input.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Input spectral data, shape (W,) or (N, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Processed spectral data, same shape.</p> Source code in <code>src/spectrakit/pipeline.py</code> <pre><code>def transform(self, intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Apply all pipeline steps to the input.\n\n    Args:\n        intensities: Input spectral data, shape (W,) or (N, W).\n\n    Returns:\n        Processed spectral data, same shape.\n    \"\"\"\n    result = intensities.copy()\n    for name, fn, kwargs in self.steps:\n        logger.debug(\"Pipeline step: %s\", name)\n        result = fn(result, **kwargs)\n    return result\n</code></pre>"},{"location":"api/pipeline/#spectrakit.pipeline.Pipeline.transform_spectrum","title":"transform_spectrum","text":"<pre><code>transform_spectrum(spectrum: Spectrum) -&gt; Spectrum\n</code></pre> <p>Apply pipeline to a Spectrum, returning a new Spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>spectrum</code> <code>Spectrum</code> <p>Input Spectrum.</p> required <p>Returns:</p> Type Description <code>Spectrum</code> <p>New Spectrum with processed intensities.</p> Source code in <code>src/spectrakit/pipeline.py</code> <pre><code>def transform_spectrum(self, spectrum: Spectrum) -&gt; Spectrum:\n    \"\"\"Apply pipeline to a Spectrum, returning a new Spectrum.\n\n    Args:\n        spectrum: Input Spectrum.\n\n    Returns:\n        New Spectrum with processed intensities.\n    \"\"\"\n    new_intensities = self.transform(spectrum.intensities)\n    return Spectrum(\n        intensities=new_intensities,\n        wavenumbers=spectrum.wavenumbers.copy() if spectrum.wavenumbers is not None else None,\n        metadata={**spectrum.metadata, \"pipeline_steps\": [s[0] for s in self.steps]},\n        source_format=spectrum.source_format,\n        label=spectrum.label,\n    )\n</code></pre>"},{"location":"api/plot/","title":"Plotting","text":"<p>Note</p> <p>Requires matplotlib: <code>pip install pyspectrakit[plot]</code></p>"},{"location":"api/plot/#spectrakit.plot.plot_spectrum","title":"spectrakit.plot.plot_spectrum","text":"<pre><code>plot_spectrum(\n    intensities: ndarray,\n    wavenumbers: ndarray | None = None,\n    *,\n    ax: Axes | None = None,\n    title: str | None = None,\n    xlabel: str = \"Wavenumber\",\n    ylabel: str = \"Intensity\",\n    invert_x: bool = True,\n    labels: list[str] | None = None,\n    **kwargs: Any,\n) -&gt; Axes\n</code></pre> <p>Plot one or more spectra.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral data, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>wavenumbers</code> <code>ndarray | None</code> <p>Wavenumber axis, shape <code>(W,)</code>. If None, uses integer indices.</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>Matplotlib Axes to plot on. If None, creates a new figure.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Plot title.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label.</p> <code>'Wavenumber'</code> <code>ylabel</code> <code>str</code> <p>Y-axis label.</p> <code>'Intensity'</code> <code>invert_x</code> <code>bool</code> <p>Whether to invert the x-axis (standard for IR spectra).</p> <code>True</code> <code>labels</code> <code>list[str] | None</code> <p>Legend labels for each spectrum. Only used for 2D input.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>ax.plot()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Matplotlib Axes object.</p> Source code in <code>src/spectrakit/plot.py</code> <pre><code>def plot_spectrum(\n    intensities: np.ndarray,\n    wavenumbers: np.ndarray | None = None,\n    *,\n    ax: Axes | None = None,\n    title: str | None = None,\n    xlabel: str = \"Wavenumber\",\n    ylabel: str = \"Intensity\",\n    invert_x: bool = True,\n    labels: list[str] | None = None,\n    **kwargs: Any,\n) -&gt; Axes:\n    \"\"\"Plot one or more spectra.\n\n    Args:\n        intensities: Spectral data, shape ``(W,)`` or ``(N, W)``.\n        wavenumbers: Wavenumber axis, shape ``(W,)``. If None, uses\n            integer indices.\n        ax: Matplotlib Axes to plot on. If None, creates a new figure.\n        title: Plot title.\n        xlabel: X-axis label.\n        ylabel: Y-axis label.\n        invert_x: Whether to invert the x-axis (standard for IR spectra).\n        labels: Legend labels for each spectrum. Only used for 2D input.\n        **kwargs: Additional keyword arguments passed to ``ax.plot()``.\n\n    Returns:\n        Matplotlib Axes object.\n    \"\"\"\n    plt = _get_matplotlib()\n    intensities = ensure_float64(intensities)\n    intensities = validate_1d_or_2d(intensities)\n\n    if ax is None:\n        _, ax = plt.subplots()\n\n    x = wavenumbers if wavenumbers is not None else np.arange(intensities.shape[-1])\n\n    if intensities.ndim == 1:\n        label = labels[0] if labels else None\n        ax.plot(x, intensities, label=label, **kwargs)\n    else:\n        for i, spectrum in enumerate(intensities):\n            label = labels[i] if labels and i &lt; len(labels) else None\n            ax.plot(x, spectrum, label=label, **kwargs)\n\n    if title:\n        ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    if invert_x and wavenumbers is not None:\n        ax.invert_xaxis()\n\n    if labels:\n        ax.legend()\n\n    return ax\n</code></pre>"},{"location":"api/plot/#spectrakit.plot.plot_comparison","title":"spectrakit.plot.plot_comparison","text":"<pre><code>plot_comparison(\n    original: ndarray,\n    processed: ndarray,\n    wavenumbers: ndarray | None = None,\n    *,\n    ax: Axes | None = None,\n    labels: tuple[str, str] = (\"Original\", \"Processed\"),\n    title: str | None = None,\n    xlabel: str = \"Wavenumber\",\n    ylabel: str = \"Intensity\",\n    invert_x: bool = True,\n    **kwargs: Any,\n) -&gt; Axes\n</code></pre> <p>Plot before/after comparison of spectral processing.</p> <p>Parameters:</p> Name Type Description Default <code>original</code> <code>ndarray</code> <p>Original spectrum, shape <code>(W,)</code>.</p> required <code>processed</code> <code>ndarray</code> <p>Processed spectrum, shape <code>(W,)</code>.</p> required <code>wavenumbers</code> <code>ndarray | None</code> <p>Wavenumber axis, shape <code>(W,)</code>. If None, uses integer indices.</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>Matplotlib Axes to plot on. If None, creates a new figure.</p> <code>None</code> <code>labels</code> <code>tuple[str, str]</code> <p>Legend labels for original and processed spectra.</p> <code>('Original', 'Processed')</code> <code>title</code> <code>str | None</code> <p>Plot title.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label.</p> <code>'Wavenumber'</code> <code>ylabel</code> <code>str</code> <p>Y-axis label.</p> <code>'Intensity'</code> <code>invert_x</code> <code>bool</code> <p>Whether to invert the x-axis.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>ax.plot()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Matplotlib Axes object.</p> Source code in <code>src/spectrakit/plot.py</code> <pre><code>def plot_comparison(\n    original: np.ndarray,\n    processed: np.ndarray,\n    wavenumbers: np.ndarray | None = None,\n    *,\n    ax: Axes | None = None,\n    labels: tuple[str, str] = (\"Original\", \"Processed\"),\n    title: str | None = None,\n    xlabel: str = \"Wavenumber\",\n    ylabel: str = \"Intensity\",\n    invert_x: bool = True,\n    **kwargs: Any,\n) -&gt; Axes:\n    \"\"\"Plot before/after comparison of spectral processing.\n\n    Args:\n        original: Original spectrum, shape ``(W,)``.\n        processed: Processed spectrum, shape ``(W,)``.\n        wavenumbers: Wavenumber axis, shape ``(W,)``. If None, uses\n            integer indices.\n        ax: Matplotlib Axes to plot on. If None, creates a new figure.\n        labels: Legend labels for original and processed spectra.\n        title: Plot title.\n        xlabel: X-axis label.\n        ylabel: Y-axis label.\n        invert_x: Whether to invert the x-axis.\n        **kwargs: Additional keyword arguments passed to ``ax.plot()``.\n\n    Returns:\n        Matplotlib Axes object.\n    \"\"\"\n    plt = _get_matplotlib()\n    original = ensure_float64(original)\n    processed = ensure_float64(processed)\n\n    if ax is None:\n        _, ax = plt.subplots()\n\n    x = wavenumbers if wavenumbers is not None else np.arange(len(original))\n\n    ax.plot(x, original, label=labels[0], alpha=0.7, **kwargs)\n    ax.plot(x, processed, label=labels[1], **kwargs)\n\n    if title:\n        ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    if invert_x and wavenumbers is not None:\n        ax.invert_xaxis()\n\n    ax.legend()\n\n    return ax\n</code></pre>"},{"location":"api/plot/#spectrakit.plot.plot_baseline","title":"spectrakit.plot.plot_baseline","text":"<pre><code>plot_baseline(\n    intensities: ndarray,\n    baseline: ndarray,\n    wavenumbers: ndarray | None = None,\n    *,\n    ax: Axes | None = None,\n    show_corrected: bool = True,\n    title: str | None = None,\n    xlabel: str = \"Wavenumber\",\n    ylabel: str = \"Intensity\",\n    invert_x: bool = True,\n    **kwargs: Any,\n) -&gt; Axes\n</code></pre> <p>Plot a spectrum with its estimated baseline.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Original spectrum, shape <code>(W,)</code>.</p> required <code>baseline</code> <code>ndarray</code> <p>Estimated baseline, shape <code>(W,)</code>.</p> required <code>wavenumbers</code> <code>ndarray | None</code> <p>Wavenumber axis, shape <code>(W,)</code>. If None, uses integer indices.</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>Matplotlib Axes to plot on. If None, creates a new figure.</p> <code>None</code> <code>show_corrected</code> <code>bool</code> <p>If True, also plots the corrected spectrum.</p> <code>True</code> <code>title</code> <code>str | None</code> <p>Plot title.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label.</p> <code>'Wavenumber'</code> <code>ylabel</code> <code>str</code> <p>Y-axis label.</p> <code>'Intensity'</code> <code>invert_x</code> <code>bool</code> <p>Whether to invert the x-axis.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>ax.plot()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Matplotlib Axes object.</p> Source code in <code>src/spectrakit/plot.py</code> <pre><code>def plot_baseline(\n    intensities: np.ndarray,\n    baseline: np.ndarray,\n    wavenumbers: np.ndarray | None = None,\n    *,\n    ax: Axes | None = None,\n    show_corrected: bool = True,\n    title: str | None = None,\n    xlabel: str = \"Wavenumber\",\n    ylabel: str = \"Intensity\",\n    invert_x: bool = True,\n    **kwargs: Any,\n) -&gt; Axes:\n    \"\"\"Plot a spectrum with its estimated baseline.\n\n    Args:\n        intensities: Original spectrum, shape ``(W,)``.\n        baseline: Estimated baseline, shape ``(W,)``.\n        wavenumbers: Wavenumber axis, shape ``(W,)``. If None, uses\n            integer indices.\n        ax: Matplotlib Axes to plot on. If None, creates a new figure.\n        show_corrected: If True, also plots the corrected spectrum.\n        title: Plot title.\n        xlabel: X-axis label.\n        ylabel: Y-axis label.\n        invert_x: Whether to invert the x-axis.\n        **kwargs: Additional keyword arguments passed to ``ax.plot()``.\n\n    Returns:\n        Matplotlib Axes object.\n    \"\"\"\n    plt = _get_matplotlib()\n    intensities = ensure_float64(intensities)\n    baseline = ensure_float64(baseline)\n\n    if ax is None:\n        _, ax = plt.subplots()\n\n    x = wavenumbers if wavenumbers is not None else np.arange(len(intensities))\n\n    ax.plot(x, intensities, label=\"Original\", alpha=0.7, **kwargs)\n    ax.plot(x, baseline, label=\"Baseline\", linestyle=\"--\", color=\"red\", **kwargs)\n\n    if show_corrected:\n        corrected = intensities - baseline\n        ax.plot(x, corrected, label=\"Corrected\", alpha=0.8, **kwargs)\n\n    if title:\n        ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    if invert_x and wavenumbers is not None:\n        ax.invert_xaxis()\n\n    ax.legend()\n\n    return ax\n</code></pre>"},{"location":"api/scatter/","title":"Scatter Correction","text":""},{"location":"api/scatter/#spectrakit.scatter.scatter_msc","title":"spectrakit.scatter.scatter_msc","text":"<pre><code>scatter_msc(\n    intensities: ndarray, reference: ndarray | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Apply Multiplicative Scatter Correction.</p> <p>Each spectrum is corrected by fitting a linear regression against a reference spectrum (default: mean spectrum), then subtracting the intercept and dividing by the slope.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(N, W)</code> for a batch or <code>(W,)</code> for a single spectrum (requires <code>reference</code>).</p> required <code>reference</code> <code>ndarray | None</code> <p>Reference spectrum, shape <code>(W,)</code>. If <code>None</code>, uses the mean of the batch.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>MSC-corrected intensities, same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <code>ValueError</code> <p>If a single spectrum is provided without a reference.</p> Source code in <code>src/spectrakit/scatter/msc.py</code> <pre><code>def scatter_msc(\n    intensities: np.ndarray,\n    reference: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Apply Multiplicative Scatter Correction.\n\n    Each spectrum is corrected by fitting a linear regression against a\n    reference spectrum (default: mean spectrum), then subtracting the\n    intercept and dividing by the slope.\n\n    Args:\n        intensities: Spectral intensities, shape ``(N, W)`` for a batch\n            or ``(W,)`` for a single spectrum (requires ``reference``).\n        reference: Reference spectrum, shape ``(W,)``. If ``None``,\n            uses the mean of the batch.\n\n    Returns:\n        MSC-corrected intensities, same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n        ValueError: If a single spectrum is provided without a reference.\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if intensities.ndim == 1:\n        if reference is None:\n            raise ValueError(\n                \"reference is required for single-spectrum MSC. \"\n                \"Pass a batch (N, W) array or provide a reference spectrum.\"\n            )\n        reference = ensure_float64(reference)\n        return _msc_single(intensities, reference)\n\n    if reference is None:\n        reference = np.mean(intensities, axis=0)\n    else:\n        reference = ensure_float64(reference)\n\n    return apply_along_spectra(_msc_single, intensities, reference=reference)\n</code></pre>"},{"location":"api/scatter/#spectrakit.scatter.scatter_emsc","title":"spectrakit.scatter.scatter_emsc","text":"<pre><code>scatter_emsc(\n    intensities: ndarray,\n    reference: ndarray | None = None,\n    poly_order: int = DEFAULT_POLY_ORDER,\n) -&gt; np.ndarray\n</code></pre> <p>Apply Extended Multiplicative Signal Correction.</p> <p>Extends MSC by also modeling polynomial baseline variations. Fits each spectrum as a linear combination of the reference spectrum plus orthogonal polynomial (Legendre) terms, then corrects by removing the polynomial and scatter contributions.</p> <p>Uses Legendre polynomials instead of monomials for improved numerical conditioning of the design matrix.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(N, W)</code> for a batch or <code>(W,)</code> for a single spectrum (requires <code>reference</code>).</p> required <code>reference</code> <code>ndarray | None</code> <p>Reference spectrum, shape <code>(W,)</code>. If <code>None</code>, uses the mean of the batch.</p> <code>None</code> <code>poly_order</code> <code>int</code> <p>Maximum polynomial order for baseline modeling. Set to 0 to disable polynomial correction (equivalent to MSC).</p> <code>DEFAULT_POLY_ORDER</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>EMSC-corrected intensities, same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <code>ValueError</code> <p>If a single spectrum is provided without a reference, or if <code>poly_order</code> is negative.</p> Source code in <code>src/spectrakit/scatter/emsc.py</code> <pre><code>def scatter_emsc(\n    intensities: np.ndarray,\n    reference: np.ndarray | None = None,\n    poly_order: int = DEFAULT_POLY_ORDER,\n) -&gt; np.ndarray:\n    \"\"\"Apply Extended Multiplicative Signal Correction.\n\n    Extends MSC by also modeling polynomial baseline variations.\n    Fits each spectrum as a linear combination of the reference\n    spectrum plus orthogonal polynomial (Legendre) terms, then\n    corrects by removing the polynomial and scatter contributions.\n\n    Uses Legendre polynomials instead of monomials for improved\n    numerical conditioning of the design matrix.\n\n    Args:\n        intensities: Spectral intensities, shape ``(N, W)`` for a batch\n            or ``(W,)`` for a single spectrum (requires ``reference``).\n        reference: Reference spectrum, shape ``(W,)``. If ``None``,\n            uses the mean of the batch.\n        poly_order: Maximum polynomial order for baseline modeling.\n            Set to 0 to disable polynomial correction (equivalent to MSC).\n\n    Returns:\n        EMSC-corrected intensities, same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n        ValueError: If a single spectrum is provided without a reference,\n            or if ``poly_order`` is negative.\n    \"\"\"\n    if poly_order &lt; 0:\n        raise ValueError(f\"poly_order must be non-negative, got {poly_order}\")\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if intensities.ndim == 1:\n        if reference is None:\n            raise ValueError(\n                \"reference is required for single-spectrum EMSC. \"\n                \"Pass a batch (N, W) array or provide a reference spectrum.\"\n            )\n        reference = ensure_float64(reference)\n        return _emsc_single(intensities, reference, poly_order)\n\n    if reference is None:\n        reference = np.mean(intensities, axis=0)\n    else:\n        reference = ensure_float64(reference)\n\n    return apply_along_spectra(\n        _emsc_single, intensities, reference=reference, poly_order=poly_order\n    )\n</code></pre>"},{"location":"api/similarity/","title":"Similarity Metrics","text":""},{"location":"api/similarity/#spectrakit.similarity.similarity_cosine","title":"spectrakit.similarity.similarity_cosine","text":"<pre><code>similarity_cosine(\n    query: ndarray, reference: ndarray\n) -&gt; float | np.ndarray\n</code></pre> <p>Compute cosine similarity between spectra.</p> <p>For single spectra (1-D), returns a scalar. For a query (1-D) against a library (2-D), returns an array of similarities. For a batch of queries (2-D) against a library (2-D), returns a similarity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>ndarray</code> <p>Query spectrum, shape <code>(W,)</code> or <code>(M, W)</code>.</p> required <code>reference</code> <code>ndarray</code> <p>Reference spectrum shape <code>(W,)</code>, or library shape <code>(N, W)</code>.</p> required <p>Returns:</p> Type Description <code>float | ndarray</code> <p>Cosine similarity in [-1, 1].</p> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(W,)</code> \u2192 scalar</li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(N, W)</code> \u2192 array <code>(N,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(W,)</code> \u2192 array <code>(M,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(N, W)</code> \u2192 matrix <code>(M, N)</code></li> </ul> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If query or reference is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If inputs have zero elements.</p> Source code in <code>src/spectrakit/similarity/cosine.py</code> <pre><code>def similarity_cosine(query: np.ndarray, reference: np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Compute cosine similarity between spectra.\n\n    For single spectra (1-D), returns a scalar. For a query (1-D) against\n    a library (2-D), returns an array of similarities. For a batch of\n    queries (2-D) against a library (2-D), returns a similarity matrix.\n\n    Args:\n        query: Query spectrum, shape ``(W,)`` or ``(M, W)``.\n        reference: Reference spectrum shape ``(W,)``, or library shape ``(N, W)``.\n\n    Returns:\n        Cosine similarity in [-1, 1].\n\n        - query ``(W,)`` + reference ``(W,)`` \u2192 scalar\n        - query ``(W,)`` + reference ``(N, W)`` \u2192 array ``(N,)``\n        - query ``(M, W)`` + reference ``(W,)`` \u2192 array ``(M,)``\n        - query ``(M, W)`` + reference ``(N, W)`` \u2192 matrix ``(M, N)``\n\n    Raises:\n        SpectrumShapeError: If *query* or *reference* is not 1-D or 2-D.\n        EmptySpectrumError: If inputs have zero elements.\n    \"\"\"\n    query = ensure_float64(query)\n    reference = ensure_float64(reference)\n    validate_1d_or_2d(query, name=\"query\")\n    validate_1d_or_2d(reference, name=\"reference\")\n    warn_if_not_finite(query, name=\"query\")\n    warn_if_not_finite(reference, name=\"reference\")\n    validate_matching_width(query, reference)\n\n    # 1D query vs 1D reference \u2192 scalar\n    if query.ndim == 1 and reference.ndim == 1:\n        dot = np.dot(query, reference)\n        denom = np.linalg.norm(query) * np.linalg.norm(reference)\n        if denom &lt; EPSILON:\n            return 0.0\n        return float(dot / denom)\n\n    # 1D query vs 2D reference \u2192 (N,)\n    if query.ndim == 1 and reference.ndim == 2:\n        dots = reference @ query\n        norms_ref = np.linalg.norm(reference, axis=1)\n        norm_query = np.linalg.norm(query)\n        denoms = norms_ref * norm_query\n        denoms = np.where(denoms &lt; EPSILON, 1.0, denoms)\n        return dots / denoms  # type: ignore[no-any-return]\n\n    # 2D query vs 1D reference \u2192 (M,)\n    if query.ndim == 2 and reference.ndim == 1:\n        dots = query @ reference\n        norms_query = np.linalg.norm(query, axis=1)\n        norm_ref = np.linalg.norm(reference)\n        denoms = norms_query * norm_ref\n        denoms = np.where(denoms &lt; EPSILON, 1.0, denoms)\n        return dots / denoms  # type: ignore[no-any-return]\n\n    # 2D query vs 2D reference \u2192 (M, N)\n    dots = query @ reference.T\n    norms_query = np.linalg.norm(query, axis=1, keepdims=True)\n    norms_ref = np.linalg.norm(reference, axis=1, keepdims=True)\n    denoms = norms_query @ norms_ref.T\n    denoms = np.where(denoms &lt; EPSILON, 1.0, denoms)\n    return dots / denoms  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/similarity/#spectrakit.similarity.similarity_pearson","title":"spectrakit.similarity.similarity_pearson","text":"<pre><code>similarity_pearson(\n    query: ndarray, reference: ndarray\n) -&gt; float | np.ndarray\n</code></pre> <p>Compute Pearson correlation between spectra.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>ndarray</code> <p>Query spectrum, shape <code>(W,)</code> or <code>(M, W)</code>.</p> required <code>reference</code> <code>ndarray</code> <p>Reference spectrum shape <code>(W,)</code>, or library shape <code>(N, W)</code>.</p> required <p>Returns:</p> Type Description <code>float | ndarray</code> <p>Pearson r in [-1, 1]. Returns <code>0.0</code> for constant spectra.</p> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(W,)</code> \u2192 scalar</li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(N, W)</code> \u2192 array <code>(N,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(W,)</code> \u2192 array <code>(M,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(N, W)</code> \u2192 matrix <code>(M, N)</code></li> </ul> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If query or reference is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If inputs have zero elements.</p> Note <p>The 2-D \u00d7 2-D case uses matrix multiplication (<code>O(M*N*W)</code>). For large M or N, consider batching queries to manage memory.</p> Source code in <code>src/spectrakit/similarity/pearson.py</code> <pre><code>def similarity_pearson(query: np.ndarray, reference: np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Compute Pearson correlation between spectra.\n\n    Args:\n        query: Query spectrum, shape ``(W,)`` or ``(M, W)``.\n        reference: Reference spectrum shape ``(W,)``, or library shape ``(N, W)``.\n\n    Returns:\n        Pearson *r* in [-1, 1]. Returns ``0.0`` for constant spectra.\n\n        - query ``(W,)`` + reference ``(W,)`` \u2192 scalar\n        - query ``(W,)`` + reference ``(N, W)`` \u2192 array ``(N,)``\n        - query ``(M, W)`` + reference ``(W,)`` \u2192 array ``(M,)``\n        - query ``(M, W)`` + reference ``(N, W)`` \u2192 matrix ``(M, N)``\n\n    Raises:\n        SpectrumShapeError: If *query* or *reference* is not 1-D or 2-D.\n        EmptySpectrumError: If inputs have zero elements.\n\n    Note:\n        The 2-D \u00d7 2-D case uses matrix multiplication (``O(M*N*W)``).\n        For large *M* or *N*, consider batching queries to manage memory.\n    \"\"\"\n    query = ensure_float64(query)\n    reference = ensure_float64(reference)\n    validate_1d_or_2d(query, name=\"query\")\n    validate_1d_or_2d(reference, name=\"reference\")\n    warn_if_not_finite(query, name=\"query\")\n    warn_if_not_finite(reference, name=\"reference\")\n    validate_matching_width(query, reference)\n\n    # 1D query vs 1D reference \u2192 scalar\n    if query.ndim == 1 and reference.ndim == 1:\n        qc = query - np.mean(query)\n        rc = reference - np.mean(reference)\n        denom = np.linalg.norm(qc) * np.linalg.norm(rc)\n        if denom &lt; EPSILON:\n            return 0.0\n        return float(np.dot(qc, rc) / denom)\n\n    # 1D query vs 2D reference \u2192 (N,)\n    if query.ndim == 1 and reference.ndim == 2:\n        qc = query - np.mean(query)\n        rc = reference - np.mean(reference, axis=1, keepdims=True)\n        numerator = rc @ qc\n        denom_q = np.linalg.norm(qc)\n        denom_r = np.linalg.norm(rc, axis=1)\n        denoms = np.where(denom_q * denom_r &lt; EPSILON, 1.0, denom_q * denom_r)\n        return numerator / denoms  # type: ignore[no-any-return]\n\n    # 2D query vs 1D reference \u2192 (M,)\n    if query.ndim == 2 and reference.ndim == 1:\n        qc = query - np.mean(query, axis=1, keepdims=True)\n        rc = reference - np.mean(reference)\n        numerator = qc @ rc\n        denom_q = np.linalg.norm(qc, axis=1)\n        denom_r = np.linalg.norm(rc)\n        denoms = np.where(denom_q * denom_r &lt; EPSILON, 1.0, denom_q * denom_r)\n        return numerator / denoms  # type: ignore[no-any-return]\n\n    # 2D query vs 2D reference \u2192 (M, N)\n    qc = query - np.mean(query, axis=1, keepdims=True)\n    rc = reference - np.mean(reference, axis=1, keepdims=True)\n    numerator = qc @ rc.T\n    norms_q = np.linalg.norm(qc, axis=1, keepdims=True)\n    norms_r = np.linalg.norm(rc, axis=1, keepdims=True)\n    denom_matrix = norms_q @ norms_r.T\n    denoms = np.where(denom_matrix &lt; EPSILON, 1.0, denom_matrix)\n    return numerator / denoms  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/similarity/#spectrakit.similarity.similarity_spectral_angle","title":"spectrakit.similarity.similarity_spectral_angle","text":"<pre><code>similarity_spectral_angle(\n    query: ndarray, reference: ndarray\n) -&gt; float | np.ndarray\n</code></pre> <p>Compute Spectral Angle Mapper (SAM) between spectra.</p> <p>Returns the angle in radians between spectral vectors. Smaller angle means more similar. Range: [0, pi].</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>ndarray</code> <p>Query spectrum, shape <code>(W,)</code> or <code>(M, W)</code>.</p> required <code>reference</code> <code>ndarray</code> <p>Reference spectrum shape <code>(W,)</code>, or library shape <code>(N, W)</code>.</p> required <p>Returns:</p> Type Description <code>float | ndarray</code> <p>Angle in radians in [0, pi].</p> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(W,)</code> \u2192 scalar</li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(N, W)</code> \u2192 array <code>(N,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(W,)</code> \u2192 array <code>(M,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(N, W)</code> \u2192 matrix <code>(M, N)</code></li> </ul> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If query or reference is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If inputs have zero elements.</p> Source code in <code>src/spectrakit/similarity/spectral_angle.py</code> <pre><code>def similarity_spectral_angle(query: np.ndarray, reference: np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Compute Spectral Angle Mapper (SAM) between spectra.\n\n    Returns the angle in radians between spectral vectors. Smaller\n    angle means more similar. Range: [0, pi].\n\n    Args:\n        query: Query spectrum, shape ``(W,)`` or ``(M, W)``.\n        reference: Reference spectrum shape ``(W,)``, or library shape ``(N, W)``.\n\n    Returns:\n        Angle in radians in [0, pi].\n\n        - query ``(W,)`` + reference ``(W,)`` \u2192 scalar\n        - query ``(W,)`` + reference ``(N, W)`` \u2192 array ``(N,)``\n        - query ``(M, W)`` + reference ``(W,)`` \u2192 array ``(M,)``\n        - query ``(M, W)`` + reference ``(N, W)`` \u2192 matrix ``(M, N)``\n\n    Raises:\n        SpectrumShapeError: If *query* or *reference* is not 1-D or 2-D.\n        EmptySpectrumError: If inputs have zero elements.\n    \"\"\"\n    query = ensure_float64(query)\n    reference = ensure_float64(reference)\n    validate_1d_or_2d(query, name=\"query\")\n    validate_1d_or_2d(reference, name=\"reference\")\n    warn_if_not_finite(query, name=\"query\")\n    warn_if_not_finite(reference, name=\"reference\")\n    validate_matching_width(query, reference)\n\n    # 1D query vs 1D reference \u2192 scalar\n    if query.ndim == 1 and reference.ndim == 1:\n        denom = np.linalg.norm(query) * np.linalg.norm(reference)\n        if denom &lt; EPSILON:\n            return 0.0\n        cos_angle = np.clip(np.dot(query, reference) / denom, -1.0, 1.0)\n        return float(np.arccos(cos_angle))\n\n    # 1D query vs 2D reference \u2192 (N,)\n    if query.ndim == 1 and reference.ndim == 2:\n        dots = reference @ query\n        norm_query = np.linalg.norm(query)\n        norms_ref = np.linalg.norm(reference, axis=1)\n        denoms = norm_query * norms_ref\n        denoms = np.where(denoms &lt; EPSILON, 1.0, denoms)\n        cos_angles = np.clip(dots / denoms, -1.0, 1.0)\n        return np.arccos(cos_angles)  # type: ignore[no-any-return]\n\n    # 2D query vs 1D reference \u2192 (M,)\n    if query.ndim == 2 and reference.ndim == 1:\n        dots = query @ reference\n        norms_query = np.linalg.norm(query, axis=1)\n        norm_ref = np.linalg.norm(reference)\n        denoms = norms_query * norm_ref\n        denoms = np.where(denoms &lt; EPSILON, 1.0, denoms)\n        cos_angles = np.clip(dots / denoms, -1.0, 1.0)\n        return np.arccos(cos_angles)  # type: ignore[no-any-return]\n\n    # 2D query vs 2D reference \u2192 (M, N)\n    dots = query @ reference.T\n    norms_query = np.linalg.norm(query, axis=1, keepdims=True)\n    norms_ref = np.linalg.norm(reference, axis=1, keepdims=True)\n    denoms = norms_query @ norms_ref.T\n    denoms = np.where(denoms &lt; EPSILON, 1.0, denoms)\n    cos_angles = np.clip(dots / denoms, -1.0, 1.0)\n    return np.arccos(cos_angles)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/similarity/#spectrakit.similarity.similarity_euclidean","title":"spectrakit.similarity.similarity_euclidean","text":"<pre><code>similarity_euclidean(\n    query: ndarray, reference: ndarray\n) -&gt; float | np.ndarray\n</code></pre> <p>Compute Euclidean distance between spectra.</p> <p>Lower values indicate greater similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>ndarray</code> <p>Query spectrum, shape <code>(W,)</code> or <code>(M, W)</code>.</p> required <code>reference</code> <code>ndarray</code> <p>Reference spectrum shape <code>(W,)</code>, or library shape <code>(N, W)</code>.</p> required <p>Returns:</p> Type Description <code>float | ndarray</code> <p>Euclidean distance in [0, inf).</p> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(W,)</code> \u2192 scalar</li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(W,)</code> + reference <code>(N, W)</code> \u2192 array <code>(N,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(W,)</code> \u2192 array <code>(M,)</code></li> </ul> <code>float | ndarray</code> <ul> <li>query <code>(M, W)</code> + reference <code>(N, W)</code> \u2192 matrix <code>(M, N)</code></li> </ul> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If query or reference is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If inputs have zero elements.</p> Note <p>The 2-D \u00d7 2-D case allocates an <code>(M, N, W)</code> intermediate array. For large M, N, or W, consider batching queries to manage memory.</p> Source code in <code>src/spectrakit/similarity/euclidean.py</code> <pre><code>def similarity_euclidean(query: np.ndarray, reference: np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Compute Euclidean distance between spectra.\n\n    Lower values indicate greater similarity.\n\n    Args:\n        query: Query spectrum, shape ``(W,)`` or ``(M, W)``.\n        reference: Reference spectrum shape ``(W,)``, or library shape ``(N, W)``.\n\n    Returns:\n        Euclidean distance in [0, inf).\n\n        - query ``(W,)`` + reference ``(W,)`` \u2192 scalar\n        - query ``(W,)`` + reference ``(N, W)`` \u2192 array ``(N,)``\n        - query ``(M, W)`` + reference ``(W,)`` \u2192 array ``(M,)``\n        - query ``(M, W)`` + reference ``(N, W)`` \u2192 matrix ``(M, N)``\n\n    Raises:\n        SpectrumShapeError: If *query* or *reference* is not 1-D or 2-D.\n        EmptySpectrumError: If inputs have zero elements.\n\n    Note:\n        The 2-D \u00d7 2-D case allocates an ``(M, N, W)`` intermediate array.\n        For large *M*, *N*, or *W*, consider batching queries to manage memory.\n    \"\"\"\n    query = ensure_float64(query)\n    reference = ensure_float64(reference)\n    validate_1d_or_2d(query, name=\"query\")\n    validate_1d_or_2d(reference, name=\"reference\")\n    warn_if_not_finite(query, name=\"query\")\n    warn_if_not_finite(reference, name=\"reference\")\n    validate_matching_width(query, reference)\n\n    # 1D query vs 1D reference \u2192 scalar\n    if query.ndim == 1 and reference.ndim == 1:\n        return float(np.linalg.norm(query - reference))\n\n    # 1D query vs 2D reference \u2192 (N,)\n    if query.ndim == 1 and reference.ndim == 2:\n        return np.linalg.norm(reference - query, axis=1)  # type: ignore[no-any-return]\n\n    # 2D query vs 1D reference \u2192 (M,)\n    if query.ndim == 2 and reference.ndim == 1:\n        return np.linalg.norm(query - reference, axis=1)  # type: ignore[no-any-return]\n\n    # 2D query vs 2D reference \u2192 (M, N)\n    m, w = query.shape\n    n = reference.shape[0]\n    max_elements = m * n * w\n    if max_elements &gt; _MAX_BROADCAST_ELEMENTS:\n        # Chunked computation to avoid allocating (M, N, W) intermediate\n        result = np.empty((m, n), dtype=np.float64)\n        for i in range(m):\n            result[i] = np.linalg.norm(reference - query[i], axis=1)\n        return result\n    diff = query[:, np.newaxis, :] - reference[np.newaxis, :, :]\n    return np.linalg.norm(diff, axis=2)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/sklearn/","title":"scikit-learn Integration","text":"<p>Note</p> <p>Requires scikit-learn: <code>pip install pyspectrakit[sklearn]</code></p>"},{"location":"api/sklearn/#spectrakit.sklearn.SpectralTransformer","title":"spectrakit.sklearn.SpectralTransformer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>scikit-learn transformer wrapping any SpectraKit function.</p> <p>Wraps a SpectraKit processing function (e.g., <code>baseline_als</code>, <code>normalize_snv</code>, <code>smooth_savgol</code>) as a scikit-learn compatible transformer that can be used in <code>sklearn.pipeline.Pipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., ndarray]</code> <p>A SpectraKit processing function with signature <code>func(intensities, **kwargs) -&gt; np.ndarray</code>.</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to <code>func</code> on each <code>transform()</code> call.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.pipeline import Pipeline as SkPipeline\n&gt;&gt;&gt; from spectrakit import baseline_als, normalize_snv\n&gt;&gt;&gt; from spectrakit.sklearn import SpectralTransformer\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipe = SkPipeline([\n...     (\"baseline\", SpectralTransformer(baseline_als, lam=1e6)),\n...     (\"normalize\", SpectralTransformer(normalize_snv)),\n... ])\n&gt;&gt;&gt; X_processed = pipe.fit_transform(X_raw)\n</code></pre> Source code in <code>src/spectrakit/sklearn/transformers.py</code> <pre><code>class SpectralTransformer(BaseEstimator, TransformerMixin):  # type: ignore[misc]\n    \"\"\"scikit-learn transformer wrapping any SpectraKit function.\n\n    Wraps a SpectraKit processing function (e.g., ``baseline_als``,\n    ``normalize_snv``, ``smooth_savgol``) as a scikit-learn compatible\n    transformer that can be used in ``sklearn.pipeline.Pipeline``.\n\n    Args:\n        func: A SpectraKit processing function with signature\n            ``func(intensities, **kwargs) -&gt; np.ndarray``.\n        **kwargs: Keyword arguments passed to ``func`` on each\n            ``transform()`` call.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.pipeline import Pipeline as SkPipeline\n        &gt;&gt;&gt; from spectrakit import baseline_als, normalize_snv\n        &gt;&gt;&gt; from spectrakit.sklearn import SpectralTransformer\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; pipe = SkPipeline([\n        ...     (\"baseline\", SpectralTransformer(baseline_als, lam=1e6)),\n        ...     (\"normalize\", SpectralTransformer(normalize_snv)),\n        ... ])\n        &gt;&gt;&gt; X_processed = pipe.fit_transform(X_raw)\n    \"\"\"\n\n    def __init__(self, func: Callable[..., np.ndarray], **kwargs: Any) -&gt; None:\n        if not _HAS_SKLEARN:\n            raise DependencyError(\n                \"scikit-learn is required for SpectralTransformer. \"\n                \"Install with: pip install spectrakit[sklearn]\"\n            )\n        self.func = func\n        self.kwargs = kwargs\n\n    def fit(\n        self,\n        X: np.ndarray,  # noqa: N803 \u2014 sklearn convention\n        y: Any = None,\n    ) -&gt; SpectralTransformer:\n        \"\"\"No-op fit (stateless transformer).\n\n        Args:\n            X: Training data (ignored).\n            y: Training labels (ignored).\n\n        Returns:\n            Self.\n        \"\"\"\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:  # noqa: N803\n        \"\"\"Apply the wrapped SpectraKit function.\n\n        Args:\n            X: Input spectral data, shape ``(N, W)``.\n\n        Returns:\n            Processed spectral data, same shape.\n        \"\"\"\n        return self.func(X, **self.kwargs)\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n        \"\"\"Get transformer parameters (sklearn interface).\n\n        Args:\n            deep: If True, return nested params.\n\n        Returns:\n            Dict of parameters.\n        \"\"\"\n        params: dict[str, Any] = {\"func\": self.func}\n        params.update(self.kwargs)\n        return params\n\n    def set_params(self, **params: Any) -&gt; SpectralTransformer:\n        \"\"\"Set transformer parameters (sklearn interface).\n\n        Args:\n            **params: Parameters to set.\n\n        Returns:\n            Self.\n        \"\"\"\n        if \"func\" in params:\n            self.func = params.pop(\"func\")\n        self.kwargs.update(params)\n        return self\n\n    def __repr__(self) -&gt; str:\n        func_name = getattr(self.func, \"__name__\", str(self.func))\n        param_str = \", \".join(f\"{k}={v!r}\" for k, v in self.kwargs.items())\n        if param_str:\n            return f\"SpectralTransformer({func_name}, {param_str})\"\n        return f\"SpectralTransformer({func_name})\"\n</code></pre>"},{"location":"api/sklearn/#spectrakit.sklearn.SpectralTransformer.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: Any = None) -&gt; SpectralTransformer\n</code></pre> <p>No-op fit (stateless transformer).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data (ignored).</p> required <code>y</code> <code>Any</code> <p>Training labels (ignored).</p> <code>None</code> <p>Returns:</p> Type Description <code>SpectralTransformer</code> <p>Self.</p> Source code in <code>src/spectrakit/sklearn/transformers.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,  # noqa: N803 \u2014 sklearn convention\n    y: Any = None,\n) -&gt; SpectralTransformer:\n    \"\"\"No-op fit (stateless transformer).\n\n    Args:\n        X: Training data (ignored).\n        y: Training labels (ignored).\n\n    Returns:\n        Self.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/sklearn/#spectrakit.sklearn.SpectralTransformer.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Get transformer parameters (sklearn interface).</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, return nested params.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict of parameters.</p> Source code in <code>src/spectrakit/sklearn/transformers.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Get transformer parameters (sklearn interface).\n\n    Args:\n        deep: If True, return nested params.\n\n    Returns:\n        Dict of parameters.\n    \"\"\"\n    params: dict[str, Any] = {\"func\": self.func}\n    params.update(self.kwargs)\n    return params\n</code></pre>"},{"location":"api/sklearn/#spectrakit.sklearn.SpectralTransformer.set_params","title":"set_params","text":"<pre><code>set_params(**params: Any) -&gt; SpectralTransformer\n</code></pre> <p>Set transformer parameters (sklearn interface).</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameters to set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>SpectralTransformer</code> <p>Self.</p> Source code in <code>src/spectrakit/sklearn/transformers.py</code> <pre><code>def set_params(self, **params: Any) -&gt; SpectralTransformer:\n    \"\"\"Set transformer parameters (sklearn interface).\n\n    Args:\n        **params: Parameters to set.\n\n    Returns:\n        Self.\n    \"\"\"\n    if \"func\" in params:\n        self.func = params.pop(\"func\")\n    self.kwargs.update(params)\n    return self\n</code></pre>"},{"location":"api/sklearn/#spectrakit.sklearn.SpectralTransformer.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Apply the wrapped SpectraKit function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input spectral data, shape <code>(N, W)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Processed spectral data, same shape.</p> Source code in <code>src/spectrakit/sklearn/transformers.py</code> <pre><code>def transform(self, X: np.ndarray) -&gt; np.ndarray:  # noqa: N803\n    \"\"\"Apply the wrapped SpectraKit function.\n\n    Args:\n        X: Input spectral data, shape ``(N, W)``.\n\n    Returns:\n        Processed spectral data, same shape.\n    \"\"\"\n    return self.func(X, **self.kwargs)\n</code></pre>"},{"location":"api/smooth/","title":"Smoothing","text":""},{"location":"api/smooth/#spectrakit.smooth.smooth_savgol","title":"spectrakit.smooth.smooth_savgol","text":"<pre><code>smooth_savgol(\n    intensities: ndarray,\n    window_length: int = DEFAULT_WINDOW_LENGTH,\n    polyorder: int = DEFAULT_POLYORDER,\n) -&gt; np.ndarray\n</code></pre> <p>Apply Savitzky-Golay smoothing filter.</p> <p>Fits successive sub-sets of adjacent data points with a low-degree polynomial by the method of linear least squares.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>window_length</code> <code>int</code> <p>Length of the filter window (must be odd and greater than <code>polyorder</code>).</p> <code>DEFAULT_WINDOW_LENGTH</code> <code>polyorder</code> <code>int</code> <p>Order of the polynomial used to fit the samples.</p> <code>DEFAULT_POLYORDER</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Smoothed intensities, same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <code>ValueError</code> <p>If <code>window_length</code> or <code>polyorder</code> are invalid.</p> Source code in <code>src/spectrakit/smooth/savgol.py</code> <pre><code>def smooth_savgol(\n    intensities: np.ndarray,\n    window_length: int = DEFAULT_WINDOW_LENGTH,\n    polyorder: int = DEFAULT_POLYORDER,\n) -&gt; np.ndarray:\n    \"\"\"Apply Savitzky-Golay smoothing filter.\n\n    Fits successive sub-sets of adjacent data points with a low-degree\n    polynomial by the method of linear least squares.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)`` or ``(N, W)``.\n        window_length: Length of the filter window (must be odd and\n            greater than ``polyorder``).\n        polyorder: Order of the polynomial used to fit the samples.\n\n    Returns:\n        Smoothed intensities, same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n        ValueError: If ``window_length`` or ``polyorder`` are invalid.\n    \"\"\"\n    if window_length &lt; 1:\n        raise ValueError(f\"window_length must be &gt;= 1, got {window_length}\")\n    if window_length % 2 == 0:\n        raise ValueError(f\"window_length must be odd, got {window_length}\")\n    if polyorder &lt; 0:\n        raise ValueError(f\"polyorder must be &gt;= 0, got {polyorder}\")\n    if polyorder &gt;= window_length:\n        raise ValueError(\n            f\"polyorder ({polyorder}) must be less than window_length ({window_length})\"\n        )\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    return apply_along_spectra(\n        _smooth_savgol_1d,\n        intensities,\n        window_length=window_length,\n        polyorder=polyorder,\n    )\n</code></pre>"},{"location":"api/smooth/#spectrakit.smooth.smooth_whittaker","title":"spectrakit.smooth.smooth_whittaker","text":"<pre><code>smooth_whittaker(\n    intensities: ndarray,\n    lam: float = DEFAULT_LAMBDA,\n    differences: int = DEFAULT_DIFFERENCES,\n    wavenumbers: ndarray | None = None,\n) -&gt; np.ndarray\n</code></pre> <p>Apply Whittaker smoother (penalized least squares).</p> <p>Minimizes the sum of squared residuals plus a penalty on the roughness (measured by finite differences) of the fitted curve.</p> <p>When wavenumbers are provided, the penalty matrix accounts for non-uniform spacing between spectral points. This is important for spectra measured on instruments with variable point spacing, as the standard uniform-spacing penalty would over- or under-smooth regions with different point densities.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>Spectral intensities, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>lam</code> <code>float</code> <p>Smoothness parameter (lambda). Larger values produce smoother results. Typical range: 1e2 to 1e8.</p> <code>DEFAULT_LAMBDA</code> <code>differences</code> <code>int</code> <p>Order of the difference penalty. 2 penalizes curvature (default), 1 penalizes slope.</p> <code>DEFAULT_DIFFERENCES</code> <code>wavenumbers</code> <code>ndarray | None</code> <p>Wavenumber axis, shape <code>(W,)</code>. If provided, builds a non-uniform finite-difference penalty matrix that accounts for the actual point spacing.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Smoothed intensities, same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <code>ValueError</code> <p>If <code>lam</code> or <code>differences</code> are invalid, or if <code>wavenumbers</code> length does not match spectral width.</p> Source code in <code>src/spectrakit/smooth/whittaker.py</code> <pre><code>def smooth_whittaker(\n    intensities: np.ndarray,\n    lam: float = DEFAULT_LAMBDA,\n    differences: int = DEFAULT_DIFFERENCES,\n    wavenumbers: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Apply Whittaker smoother (penalized least squares).\n\n    Minimizes the sum of squared residuals plus a penalty on the\n    roughness (measured by finite differences) of the fitted curve.\n\n    When *wavenumbers* are provided, the penalty matrix accounts for\n    non-uniform spacing between spectral points. This is important for\n    spectra measured on instruments with variable point spacing, as the\n    standard uniform-spacing penalty would over- or under-smooth\n    regions with different point densities.\n\n    Args:\n        intensities: Spectral intensities, shape ``(W,)`` or ``(N, W)``.\n        lam: Smoothness parameter (lambda). Larger values produce\n            smoother results. Typical range: 1e2 to 1e8.\n        differences: Order of the difference penalty. 2 penalizes\n            curvature (default), 1 penalizes slope.\n        wavenumbers: Wavenumber axis, shape ``(W,)``. If provided,\n            builds a non-uniform finite-difference penalty matrix that\n            accounts for the actual point spacing.\n\n    Returns:\n        Smoothed intensities, same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n        ValueError: If ``lam`` or ``differences`` are invalid, or if\n            ``wavenumbers`` length does not match spectral width.\n    \"\"\"\n    if lam &lt;= 0:\n        raise ValueError(f\"lam (smoothness) must be positive, got {lam}\")\n    if differences &lt; 1:\n        raise ValueError(f\"differences must be &gt;= 1, got {differences}\")\n\n    intensities = ensure_float64(intensities)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    if wavenumbers is not None:\n        wavenumbers = ensure_float64(wavenumbers)\n        expected_w = intensities.shape[-1]\n        if wavenumbers.shape[0] != expected_w:\n            raise ValueError(\n                f\"wavenumbers length {wavenumbers.shape[0]} does not match \"\n                f\"intensities spectral width {expected_w}\"\n            )\n\n    # Pre-compute the penalty matrix once for the entire batch.\n    n = intensities.shape[-1]\n\n    if wavenumbers is not None and differences &lt;= 2:\n        if differences == 1:\n            D = _build_nonuniform_diff1(wavenumbers)\n        else:\n            D = _build_nonuniform_diff2(wavenumbers)\n    else:\n        D = sparse.eye(n, format=\"csc\")\n        for _ in range(differences):\n            D = D[1:] - D[:-1]\n\n    penalty_z = sparse.eye(n, format=\"csc\") + lam * D.T @ D\n\n    return apply_along_spectra(\n        _smooth_whittaker_1d,\n        intensities,\n        penalty_z=penalty_z,\n    )\n</code></pre>"},{"location":"api/spectrum/","title":"Spectrum","text":""},{"location":"api/spectrum/#spectrakit.spectrum.Spectrum","title":"spectrakit.spectrum.Spectrum  <code>dataclass</code>","text":"<p>Format-agnostic spectral data container.</p> <p>Wraps intensity values and optional wavenumber/wavelength axis with metadata. All processing functions accept and return numpy arrays, but this container provides a convenient way to keep data and metadata together.</p> <p>Attributes:</p> Name Type Description <code>intensities</code> <code>ndarray</code> <p>Spectral intensity values, shape (W,) for a single spectrum or (N, W) for a collection.</p> <code>wavenumbers</code> <code>ndarray | None</code> <p>X-axis values (cm^-1, nm, etc.), shape (W,). None if not available.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Arbitrary key-value metadata from the source file.</p> <code>source_format</code> <code>str</code> <p>Original file format (e.g., \"jcamp\", \"spc\", \"csv\").</p> <code>label</code> <code>str</code> <p>Human-readable label for display/logging.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spec = Spectrum(\n...     intensities=np.array([0.1, 0.5, 0.9, 0.5, 0.1]),\n...     wavenumbers=np.array([400, 800, 1200, 1600, 2000]),\n...     label=\"ethanol_ir\",\n... )\n&gt;&gt;&gt; spec.n_points\n5\n</code></pre> Source code in <code>src/spectrakit/spectrum.py</code> <pre><code>@dataclass\nclass Spectrum:\n    \"\"\"Format-agnostic spectral data container.\n\n    Wraps intensity values and optional wavenumber/wavelength axis with\n    metadata. All processing functions accept and return numpy arrays,\n    but this container provides a convenient way to keep data and\n    metadata together.\n\n    Attributes:\n        intensities: Spectral intensity values, shape (W,) for a single\n            spectrum or (N, W) for a collection.\n        wavenumbers: X-axis values (cm^-1, nm, etc.), shape (W,).\n            None if not available.\n        metadata: Arbitrary key-value metadata from the source file.\n        source_format: Original file format (e.g., \"jcamp\", \"spc\", \"csv\").\n        label: Human-readable label for display/logging.\n\n    Examples:\n        &gt;&gt;&gt; spec = Spectrum(\n        ...     intensities=np.array([0.1, 0.5, 0.9, 0.5, 0.1]),\n        ...     wavenumbers=np.array([400, 800, 1200, 1600, 2000]),\n        ...     label=\"ethanol_ir\",\n        ... )\n        &gt;&gt;&gt; spec.n_points\n        5\n    \"\"\"\n\n    intensities: np.ndarray\n    wavenumbers: np.ndarray | None = None\n    metadata: dict[str, Any] = field(default_factory=dict)\n    source_format: str = \"unknown\"\n    label: str = \"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate array shapes on construction.\"\"\"\n        if not isinstance(self.intensities, np.ndarray):\n            self.intensities = np.asarray(self.intensities, dtype=np.float64)\n        if self.wavenumbers is not None:\n            if not isinstance(self.wavenumbers, np.ndarray):\n                self.wavenumbers = np.asarray(self.wavenumbers, dtype=np.float64)\n            if self.intensities.ndim == 1:\n                if self.wavenumbers.shape[0] != self.intensities.shape[0]:\n                    raise ValueError(\n                        f\"wavenumbers length {self.wavenumbers.shape[0]} != \"\n                        f\"intensities length {self.intensities.shape[0]}\"\n                    )\n            elif self.intensities.ndim == 2:\n                if self.wavenumbers.shape[0] != self.intensities.shape[1]:\n                    raise ValueError(\n                        f\"wavenumbers length {self.wavenumbers.shape[0]} != \"\n                        f\"intensities width {self.intensities.shape[1]}\"\n                    )\n\n    @property\n    def n_points(self) -&gt; int:\n        \"\"\"Number of spectral data points (W).\"\"\"\n        if self.intensities.ndim == 1:\n            return int(self.intensities.shape[0])\n        return int(self.intensities.shape[1])\n\n    @property\n    def n_spectra(self) -&gt; int:\n        \"\"\"Number of spectra (N). Returns 1 for single spectrum.\"\"\"\n        if self.intensities.ndim == 1:\n            return 1\n        return int(self.intensities.shape[0])\n\n    @property\n    def shape(self) -&gt; tuple[int, ...]:\n        \"\"\"Shape of the intensities array.\"\"\"\n        return tuple(self.intensities.shape)\n\n    def copy(self) -&gt; Spectrum:\n        \"\"\"Return a deep copy.\"\"\"\n        return Spectrum(\n            intensities=self.intensities.copy(),\n            wavenumbers=self.wavenumbers.copy() if self.wavenumbers is not None else None,\n            metadata=copy.deepcopy(self.metadata),\n            source_format=self.source_format,\n            label=self.label,\n        )\n</code></pre>"},{"location":"api/spectrum/#spectrakit.spectrum.Spectrum.n_points","title":"n_points  <code>property</code>","text":"<pre><code>n_points: int\n</code></pre> <p>Number of spectral data points (W).</p>"},{"location":"api/spectrum/#spectrakit.spectrum.Spectrum.n_spectra","title":"n_spectra  <code>property</code>","text":"<pre><code>n_spectra: int\n</code></pre> <p>Number of spectra (N). Returns 1 for single spectrum.</p>"},{"location":"api/spectrum/#spectrakit.spectrum.Spectrum.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, ...]\n</code></pre> <p>Shape of the intensities array.</p>"},{"location":"api/spectrum/#spectrakit.spectrum.Spectrum.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate array shapes on construction.</p> Source code in <code>src/spectrakit/spectrum.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate array shapes on construction.\"\"\"\n    if not isinstance(self.intensities, np.ndarray):\n        self.intensities = np.asarray(self.intensities, dtype=np.float64)\n    if self.wavenumbers is not None:\n        if not isinstance(self.wavenumbers, np.ndarray):\n            self.wavenumbers = np.asarray(self.wavenumbers, dtype=np.float64)\n        if self.intensities.ndim == 1:\n            if self.wavenumbers.shape[0] != self.intensities.shape[0]:\n                raise ValueError(\n                    f\"wavenumbers length {self.wavenumbers.shape[0]} != \"\n                    f\"intensities length {self.intensities.shape[0]}\"\n                )\n        elif self.intensities.ndim == 2:\n            if self.wavenumbers.shape[0] != self.intensities.shape[1]:\n                raise ValueError(\n                    f\"wavenumbers length {self.wavenumbers.shape[0]} != \"\n                    f\"intensities width {self.intensities.shape[1]}\"\n                )\n</code></pre>"},{"location":"api/spectrum/#spectrakit.spectrum.Spectrum.copy","title":"copy","text":"<pre><code>copy() -&gt; Spectrum\n</code></pre> <p>Return a deep copy.</p> Source code in <code>src/spectrakit/spectrum.py</code> <pre><code>def copy(self) -&gt; Spectrum:\n    \"\"\"Return a deep copy.\"\"\"\n    return Spectrum(\n        intensities=self.intensities.copy(),\n        wavenumbers=self.wavenumbers.copy() if self.wavenumbers is not None else None,\n        metadata=copy.deepcopy(self.metadata),\n        source_format=self.source_format,\n        label=self.label,\n    )\n</code></pre>"},{"location":"api/transform/","title":"Spectral Transforms","text":""},{"location":"api/transform/#spectrakit.transform.transform_kubelka_munk","title":"spectrakit.transform.transform_kubelka_munk","text":"<pre><code>transform_kubelka_munk(reflectance: ndarray) -&gt; np.ndarray\n</code></pre> <p>Convert diffuse reflectance to Kubelka-Munk units.</p> <p>Applies the transformation: K/S = (1 - R)^2 / (2R)</p> <p>where R is the diffuse reflectance (0 to 1 scale). This linearizes the relationship between concentration and spectral response for diffuse reflectance measurements.</p> <p>Parameters:</p> Name Type Description Default <code>reflectance</code> <code>ndarray</code> <p>Diffuse reflectance values in [0, 1], shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Kubelka-Munk values (K/S), same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> Source code in <code>src/spectrakit/transform/kubelka_munk.py</code> <pre><code>def transform_kubelka_munk(reflectance: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert diffuse reflectance to Kubelka-Munk units.\n\n    Applies the transformation: K/S = (1 - R)^2 / (2R)\n\n    where R is the diffuse reflectance (0 to 1 scale). This\n    linearizes the relationship between concentration and spectral\n    response for diffuse reflectance measurements.\n\n    Args:\n        reflectance: Diffuse reflectance values in [0, 1], shape\n            ``(W,)`` or ``(N, W)``.\n\n    Returns:\n        Kubelka-Munk values (K/S), same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n    \"\"\"\n    reflectance = ensure_float64(reflectance)\n    validate_1d_or_2d(reflectance)\n    warn_if_not_finite(reflectance)\n\n    if np.any(reflectance &lt; 0) or np.any(reflectance &gt; 1):\n        warnings.warn(\n            \"Reflectance values outside [0, 1] detected. \"\n            \"Ensure you are passing reflectance, not absorbance.\",\n            stacklevel=2,\n        )\n\n    # Clamp reflectance to avoid division by zero\n    r = np.clip(reflectance, EPSILON, 1.0 - EPSILON)\n\n    return (1.0 - r) ** 2 / (2.0 * r)\n</code></pre>"},{"location":"api/transform/#spectrakit.transform.transform_atr_correction","title":"spectrakit.transform.transform_atr_correction","text":"<pre><code>transform_atr_correction(\n    intensities: ndarray,\n    wavenumbers: ndarray,\n    n_crystal: float = DEFAULT_N_CRYSTAL,\n    n_sample: float = DEFAULT_N_SAMPLE,\n    angle: float = DEFAULT_ANGLE,\n) -&gt; np.ndarray\n</code></pre> <p>Apply ATR path-length correction to infrared spectra.</p> <p>Corrects for the wavenumber-dependent penetration depth in ATR measurements. The effective path length in ATR varies with wavenumber, making peaks at lower wavenumbers appear stronger. This correction normalizes for that effect.</p> <p>Parameters:</p> Name Type Description Default <code>intensities</code> <code>ndarray</code> <p>ATR spectral intensities, shape <code>(W,)</code> or <code>(N, W)</code>.</p> required <code>wavenumbers</code> <code>ndarray</code> <p>Wavenumber axis in cm^-1, shape <code>(W,)</code>.</p> required <code>n_crystal</code> <code>float</code> <p>Refractive index of the ATR crystal. Common values: diamond = 2.4, ZnSe = 2.4, Ge = 4.0.</p> <code>DEFAULT_N_CRYSTAL</code> <code>n_sample</code> <code>float</code> <p>Refractive index of the sample. Typical organic samples: 1.4-1.6.</p> <code>DEFAULT_N_SAMPLE</code> <code>angle</code> <code>float</code> <p>Angle of incidence in degrees.</p> <code>DEFAULT_ANGLE</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>ATR-corrected intensities, same shape as input.</p> <p>Raises:</p> Type Description <code>SpectrumShapeError</code> <p>If input is not 1-D or 2-D.</p> <code>EmptySpectrumError</code> <p>If input has zero elements.</p> <code>ValueError</code> <p>If physics parameters are invalid (non-positive refractive indices, angle outside (0, 90), or angle below the critical angle for the given crystal/sample pair).</p> Source code in <code>src/spectrakit/transform/atr_correction.py</code> <pre><code>def transform_atr_correction(\n    intensities: np.ndarray,\n    wavenumbers: np.ndarray,\n    n_crystal: float = DEFAULT_N_CRYSTAL,\n    n_sample: float = DEFAULT_N_SAMPLE,\n    angle: float = DEFAULT_ANGLE,\n) -&gt; np.ndarray:\n    \"\"\"Apply ATR path-length correction to infrared spectra.\n\n    Corrects for the wavenumber-dependent penetration depth in ATR\n    measurements. The effective path length in ATR varies with\n    wavenumber, making peaks at lower wavenumbers appear stronger.\n    This correction normalizes for that effect.\n\n    Args:\n        intensities: ATR spectral intensities, shape ``(W,)`` or ``(N, W)``.\n        wavenumbers: Wavenumber axis in cm^-1, shape ``(W,)``.\n        n_crystal: Refractive index of the ATR crystal. Common values:\n            diamond = 2.4, ZnSe = 2.4, Ge = 4.0.\n        n_sample: Refractive index of the sample. Typical organic\n            samples: 1.4-1.6.\n        angle: Angle of incidence in degrees.\n\n    Returns:\n        ATR-corrected intensities, same shape as input.\n\n    Raises:\n        SpectrumShapeError: If input is not 1-D or 2-D.\n        EmptySpectrumError: If input has zero elements.\n        ValueError: If physics parameters are invalid (non-positive\n            refractive indices, angle outside (0, 90), or angle below\n            the critical angle for the given crystal/sample pair).\n    \"\"\"\n    intensities = ensure_float64(intensities)\n    wavenumbers = ensure_float64(wavenumbers)\n    validate_1d_or_2d(intensities)\n    warn_if_not_finite(intensities)\n\n    expected_w = intensities.shape[-1]\n    if wavenumbers.shape[0] != expected_w:\n        raise ValueError(\n            f\"wavenumbers length {wavenumbers.shape[0]} does not match \"\n            f\"intensities spectral width {expected_w}\"\n        )\n\n    # Validate physics parameters\n    if n_crystal &lt;= 0:\n        raise ValueError(f\"n_crystal must be positive, got {n_crystal}\")\n    if n_sample &lt;= 0:\n        raise ValueError(f\"n_sample must be positive, got {n_sample}\")\n    if not 0 &lt; angle &lt; 90:\n        raise ValueError(f\"angle must be in (0, 90) degrees, got {angle}\")\n\n    n_ratio = n_sample / n_crystal\n    if n_ratio &gt;= 1.0:\n        raise ValueError(\n            f\"n_sample ({n_sample}) must be less than n_crystal ({n_crystal}) \"\n            \"for total internal reflection\"\n        )\n\n    # Compute penetration depth factor: dp \u221d 1 / (\u03bd * sqrt(sin\u00b2\u03b8 - (n2/n1)\u00b2))\n    theta = np.radians(angle)\n    sin2_theta = np.sin(theta) ** 2\n\n    discriminant = sin2_theta - n_ratio**2\n    if discriminant &lt;= 0:\n        raise ValueError(\n            f\"Angle {angle}\u00b0 is below the critical angle for n_crystal={n_crystal}, \"\n            f\"n_sample={n_sample}. No total internal reflection occurs.\"\n        )\n\n    # Penetration depth: dp = \u03bb / (2\u03c0 * n1 * sqrt(sin\u00b2\u03b8 - (n2/n1)\u00b2))\n    # Since \u03bb = 1/\u03bd (in cm\u207b\u00b9), dp \u221d 1 / (\u03bd * sqrt(discriminant))\n    # Correction: multiply absorbance by \u03bd * sqrt(discriminant) to remove\n    # the path-length dependence, then normalize so the correction factor\n    # at the maximum wavenumber equals 1.\n    dp_inv = wavenumbers * np.sqrt(discriminant)\n    correction = dp_inv / np.max(dp_inv)\n\n    if intensities.ndim == 1:\n        return intensities * correction  # type: ignore[no-any-return]\n    return intensities * correction[np.newaxis, :]  # type: ignore[no-any-return]\n</code></pre>"},{"location":"examples/01_quickstart/","title":"SpectraKit Quick Start","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom spectrakit import (\n    baseline_als,\n    normalize_snv,\n    smooth_savgol,\n)\nfrom spectrakit.plot import plot_comparison, plot_spectrum\n</pre> import matplotlib.pyplot as plt import numpy as np  from spectrakit import (     baseline_als,     normalize_snv,     smooth_savgol, ) from spectrakit.plot import plot_comparison, plot_spectrum In\u00a0[\u00a0]: Copied! <pre>rng = np.random.default_rng(42)\nwavenumbers = np.linspace(400, 4000, 1000)\n\n\n# Create peaks\ndef gaussian(x, center, amp, sigma):\n    return amp * np.exp(-((x - center) ** 2) / (2 * sigma**2))\n\n\nsignal = (\n    gaussian(wavenumbers, 1000, 2.0, 30)\n    + gaussian(wavenumbers, 1650, 1.5, 40)\n    + gaussian(wavenumbers, 2900, 3.0, 50)\n    + gaussian(wavenumbers, 3400, 1.0, 60)\n)\n\n# Add baseline drift and noise\nbaseline = 0.5 + 0.3 * np.sin(wavenumbers / 800)\nnoise = rng.normal(0, 0.05, 1000)\nraw = signal + baseline + noise\n\nplot_spectrum(raw, wavenumbers, title=\"Raw Synthetic Spectrum\")\nplt.show()\n</pre> rng = np.random.default_rng(42) wavenumbers = np.linspace(400, 4000, 1000)   # Create peaks def gaussian(x, center, amp, sigma):     return amp * np.exp(-((x - center) ** 2) / (2 * sigma**2))   signal = (     gaussian(wavenumbers, 1000, 2.0, 30)     + gaussian(wavenumbers, 1650, 1.5, 40)     + gaussian(wavenumbers, 2900, 3.0, 50)     + gaussian(wavenumbers, 3400, 1.0, 60) )  # Add baseline drift and noise baseline = 0.5 + 0.3 * np.sin(wavenumbers / 800) noise = rng.normal(0, 0.05, 1000) raw = signal + baseline + noise  plot_spectrum(raw, wavenumbers, title=\"Raw Synthetic Spectrum\") plt.show() In\u00a0[\u00a0]: Copied! <pre>smoothed = smooth_savgol(raw, window_length=11, polyorder=3)\n\nplot_comparison(\n    raw,\n    smoothed,\n    wavenumbers,\n    labels=(\"Raw\", \"Smoothed\"),\n    title=\"Effect of Savitzky-Golay Smoothing\",\n)\nplt.show()\n</pre> smoothed = smooth_savgol(raw, window_length=11, polyorder=3)  plot_comparison(     raw,     smoothed,     wavenumbers,     labels=(\"Raw\", \"Smoothed\"),     title=\"Effect of Savitzky-Golay Smoothing\", ) plt.show() In\u00a0[\u00a0]: Copied! <pre>corrected = baseline_als(smoothed, lam=1e6, p=0.01)\n\nplot_comparison(\n    smoothed,\n    corrected,\n    wavenumbers,\n    labels=(\"Smoothed\", \"Baseline Corrected\"),\n    title=\"Baseline Correction with ALS\",\n)\nplt.show()\n</pre> corrected = baseline_als(smoothed, lam=1e6, p=0.01)  plot_comparison(     smoothed,     corrected,     wavenumbers,     labels=(\"Smoothed\", \"Baseline Corrected\"),     title=\"Baseline Correction with ALS\", ) plt.show() In\u00a0[\u00a0]: Copied! <pre>normalized = normalize_snv(corrected)\n\nplot_comparison(\n    corrected,\n    normalized,\n    wavenumbers,\n    labels=(\"Corrected\", \"SNV Normalized\"),\n    title=\"SNV Normalization\",\n)\nplt.show()\n</pre> normalized = normalize_snv(corrected)  plot_comparison(     corrected,     normalized,     wavenumbers,     labels=(\"Corrected\", \"SNV Normalized\"),     title=\"SNV Normalization\", ) plt.show() In\u00a0[\u00a0]: Copied! <pre>from spectrakit.pipeline import Pipeline\n\npipe = Pipeline()\npipe.add(\"smooth\", smooth_savgol, window_length=11, polyorder=3)\npipe.add(\"baseline\", baseline_als, lam=1e6, p=0.01)\npipe.add(\"normalize\", normalize_snv)\n\n# Apply to a batch of spectra\nbatch = np.vstack([raw + rng.normal(0, 0.02, 1000) for _ in range(10)])\nprocessed_batch = pipe.transform(batch)\n\nplot_spectrum(processed_batch, wavenumbers, title=\"Batch of 10 Processed Spectra\")\nplt.show()\n\nprint(f\"Input shape: {batch.shape}\")\nprint(f\"Output shape: {processed_batch.shape}\")\nprint(pipe)\n</pre> from spectrakit.pipeline import Pipeline  pipe = Pipeline() pipe.add(\"smooth\", smooth_savgol, window_length=11, polyorder=3) pipe.add(\"baseline\", baseline_als, lam=1e6, p=0.01) pipe.add(\"normalize\", normalize_snv)  # Apply to a batch of spectra batch = np.vstack([raw + rng.normal(0, 0.02, 1000) for _ in range(10)]) processed_batch = pipe.transform(batch)  plot_spectrum(processed_batch, wavenumbers, title=\"Batch of 10 Processed Spectra\") plt.show()  print(f\"Input shape: {batch.shape}\") print(f\"Output shape: {processed_batch.shape}\") print(pipe)"},{"location":"examples/01_quickstart/#spectrakit-quick-start","title":"SpectraKit Quick Start\u00b6","text":"<p>This notebook demonstrates the basic SpectraKit workflow:</p> <ol> <li>Generate synthetic spectral data</li> <li>Apply smoothing, baseline correction, and normalization</li> <li>Compare results</li> </ol>"},{"location":"examples/01_quickstart/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":"<p>Create a synthetic spectrum with peaks, baseline drift, and noise.</p>"},{"location":"examples/01_quickstart/#step-1-smoothing","title":"Step 1: Smoothing\u00b6","text":""},{"location":"examples/01_quickstart/#step-2-baseline-correction","title":"Step 2: Baseline Correction\u00b6","text":""},{"location":"examples/01_quickstart/#step-3-normalization","title":"Step 3: Normalization\u00b6","text":""},{"location":"examples/01_quickstart/#full-pipeline","title":"Full Pipeline\u00b6","text":"<p>Chain all steps together using the Pipeline class.</p>"},{"location":"examples/02_baseline_methods/","title":"Baseline Correction Methods","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom spectrakit import (\n    baseline_als,\n    baseline_polynomial,\n    baseline_rubberband,\n    baseline_snip,\n)\nfrom spectrakit.plot import plot_baseline\n</pre> import matplotlib.pyplot as plt import numpy as np  from spectrakit import (     baseline_als,     baseline_polynomial,     baseline_rubberband,     baseline_snip, ) from spectrakit.plot import plot_baseline In\u00a0[\u00a0]: Copied! <pre>wavenumbers = np.linspace(400, 4000, 1000)\n\n\ndef gaussian(x, c, a, s):\n    return a * np.exp(-((x - c) ** 2) / (2 * s**2))\n\n\npeaks = (\n    gaussian(wavenumbers, 1000, 2.0, 25)\n    + gaussian(wavenumbers, 1650, 1.5, 35)\n    + gaussian(wavenumbers, 2900, 2.5, 40)\n)\n\n# Curved baseline\ntrue_baseline = 1.0 + 0.5 * np.sin(wavenumbers / 600) + 0.0001 * (wavenumbers - 2000) ** 2 / 1e4\nspectrum = peaks + true_baseline\n\nplt.figure(figsize=(10, 4))\nplt.plot(wavenumbers, spectrum, label=\"Spectrum\")\nplt.plot(wavenumbers, true_baseline, \"--\", label=\"True Baseline\")\nplt.legend()\nplt.xlabel(\"Wavenumber\")\nplt.ylabel(\"Intensity\")\nplt.title(\"Synthetic Spectrum with Known Baseline\")\nplt.gca().invert_xaxis()\nplt.show()\n</pre> wavenumbers = np.linspace(400, 4000, 1000)   def gaussian(x, c, a, s):     return a * np.exp(-((x - c) ** 2) / (2 * s**2))   peaks = (     gaussian(wavenumbers, 1000, 2.0, 25)     + gaussian(wavenumbers, 1650, 1.5, 35)     + gaussian(wavenumbers, 2900, 2.5, 40) )  # Curved baseline true_baseline = 1.0 + 0.5 * np.sin(wavenumbers / 600) + 0.0001 * (wavenumbers - 2000) ** 2 / 1e4 spectrum = peaks + true_baseline  plt.figure(figsize=(10, 4)) plt.plot(wavenumbers, spectrum, label=\"Spectrum\") plt.plot(wavenumbers, true_baseline, \"--\", label=\"True Baseline\") plt.legend() plt.xlabel(\"Wavenumber\") plt.ylabel(\"Intensity\") plt.title(\"Synthetic Spectrum with Known Baseline\") plt.gca().invert_xaxis() plt.show() In\u00a0[\u00a0]: Copied! <pre>methods = {\n    \"ALS (lam=1e6)\": lambda y: y - baseline_als(y, lam=1e6, p=0.01),\n    \"SNIP (40 iters)\": lambda y: y - baseline_snip(y, num_iterations=40),\n    \"Polynomial (order=3)\": lambda y: y - baseline_polynomial(y, poly_order=3),\n    \"Rubberband\": lambda y: y - baseline_rubberband(y),\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8), sharex=True)\n\nfor ax, (name, method) in zip(axes.flat, methods.items(), strict=False):\n    corrected = method(spectrum)\n    ax.plot(wavenumbers, peaks, \"--\", alpha=0.5, label=\"True Signal\")\n    ax.plot(wavenumbers, corrected, label=\"Corrected\")\n    ax.set_title(name)\n    ax.legend(fontsize=8)\n    ax.invert_xaxis()\n\nplt.suptitle(\"Baseline Correction Comparison\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> methods = {     \"ALS (lam=1e6)\": lambda y: y - baseline_als(y, lam=1e6, p=0.01),     \"SNIP (40 iters)\": lambda y: y - baseline_snip(y, num_iterations=40),     \"Polynomial (order=3)\": lambda y: y - baseline_polynomial(y, poly_order=3),     \"Rubberband\": lambda y: y - baseline_rubberband(y), }  fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharex=True)  for ax, (name, method) in zip(axes.flat, methods.items(), strict=False):     corrected = method(spectrum)     ax.plot(wavenumbers, peaks, \"--\", alpha=0.5, label=\"True Signal\")     ax.plot(wavenumbers, corrected, label=\"Corrected\")     ax.set_title(name)     ax.legend(fontsize=8)     ax.invert_xaxis()  plt.suptitle(\"Baseline Correction Comparison\", fontsize=14) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(1, 3, figsize=(14, 4), sharex=True, sharey=True)\n\nfor ax, lam in zip(axes, [1e4, 1e6, 1e8], strict=True):\n    corrected = baseline_als(spectrum, lam=lam, p=0.01)\n    plot_baseline(\n        spectrum, spectrum - corrected + corrected * 0, wavenumbers, ax=ax, show_corrected=False\n    )\n    # Re-plot manually for clarity\n    ax.clear()\n    ax.plot(wavenumbers, spectrum, alpha=0.7, label=\"Original\")\n    bl = spectrum - corrected\n    ax.plot(wavenumbers, bl, \"--r\", label=\"Estimated BL\")\n    ax.plot(wavenumbers, true_baseline, \":g\", label=\"True BL\")\n    ax.set_title(f\"lam = {lam:.0e}\")\n    ax.legend(fontsize=8)\n    ax.invert_xaxis()\n\nplt.suptitle(\"ALS: Effect of Lambda Parameter\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(1, 3, figsize=(14, 4), sharex=True, sharey=True)  for ax, lam in zip(axes, [1e4, 1e6, 1e8], strict=True):     corrected = baseline_als(spectrum, lam=lam, p=0.01)     plot_baseline(         spectrum, spectrum - corrected + corrected * 0, wavenumbers, ax=ax, show_corrected=False     )     # Re-plot manually for clarity     ax.clear()     ax.plot(wavenumbers, spectrum, alpha=0.7, label=\"Original\")     bl = spectrum - corrected     ax.plot(wavenumbers, bl, \"--r\", label=\"Estimated BL\")     ax.plot(wavenumbers, true_baseline, \":g\", label=\"True BL\")     ax.set_title(f\"lam = {lam:.0e}\")     ax.legend(fontsize=8)     ax.invert_xaxis()  plt.suptitle(\"ALS: Effect of Lambda Parameter\", fontsize=14) plt.tight_layout() plt.show()"},{"location":"examples/02_baseline_methods/#baseline-correction-methods","title":"Baseline Correction Methods\u00b6","text":"<p>Compare the four built-in baseline correction methods on synthetic data.</p>"},{"location":"examples/02_baseline_methods/#create-synthetic-spectrum","title":"Create Synthetic Spectrum\u00b6","text":""},{"location":"examples/02_baseline_methods/#compare-methods","title":"Compare Methods\u00b6","text":""},{"location":"examples/02_baseline_methods/#als-parameter-sensitivity","title":"ALS Parameter Sensitivity\u00b6","text":""},{"location":"examples/03_derivatives_and_peaks/","title":"Derivatives and Peak Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom spectrakit import (\n    derivative_gap_segment,\n    derivative_savgol,\n    peaks_find,\n    peaks_integrate,\n    smooth_savgol,\n)\n</pre> import matplotlib.pyplot as plt import numpy as np  from spectrakit import (     derivative_gap_segment,     derivative_savgol,     peaks_find,     peaks_integrate,     smooth_savgol, ) In\u00a0[\u00a0]: Copied! <pre>wavenumbers = np.linspace(400, 4000, 1000)\n\n\ndef gaussian(x, c, a, s):\n    return a * np.exp(-((x - c) ** 2) / (2 * s**2))\n\n\n# Two overlapping peaks + one isolated peak\nspectrum = (\n    gaussian(wavenumbers, 1600, 2.0, 30)\n    + gaussian(wavenumbers, 1680, 1.5, 25)  # overlapping with first\n    + gaussian(wavenumbers, 2900, 3.0, 50)  # isolated\n)\n\n# Add some noise\nrng = np.random.default_rng(42)\nnoisy = spectrum + rng.normal(0, 0.03, len(spectrum))\n\nplt.figure(figsize=(10, 4))\nplt.plot(wavenumbers, noisy, alpha=0.7, label=\"Noisy\")\nplt.plot(wavenumbers, spectrum, \"--\", label=\"True\")\nplt.legend()\nplt.xlabel(\"Wavenumber\")\nplt.gca().invert_xaxis()\nplt.title(\"Test Spectrum with Overlapping Peaks\")\nplt.show()\n</pre> wavenumbers = np.linspace(400, 4000, 1000)   def gaussian(x, c, a, s):     return a * np.exp(-((x - c) ** 2) / (2 * s**2))   # Two overlapping peaks + one isolated peak spectrum = (     gaussian(wavenumbers, 1600, 2.0, 30)     + gaussian(wavenumbers, 1680, 1.5, 25)  # overlapping with first     + gaussian(wavenumbers, 2900, 3.0, 50)  # isolated )  # Add some noise rng = np.random.default_rng(42) noisy = spectrum + rng.normal(0, 0.03, len(spectrum))  plt.figure(figsize=(10, 4)) plt.plot(wavenumbers, noisy, alpha=0.7, label=\"Noisy\") plt.plot(wavenumbers, spectrum, \"--\", label=\"True\") plt.legend() plt.xlabel(\"Wavenumber\") plt.gca().invert_xaxis() plt.title(\"Test Spectrum with Overlapping Peaks\") plt.show() In\u00a0[\u00a0]: Copied! <pre># Smooth first to reduce noise amplification\nsmoothed = smooth_savgol(noisy, window_length=15, polyorder=3)\n\n# First and second derivatives\nd1 = derivative_savgol(smoothed, window_length=15, polyorder=3, deriv=1)\nd2 = derivative_savgol(smoothed, window_length=15, polyorder=3, deriv=2)\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n\naxes[0].plot(wavenumbers, smoothed)\naxes[0].set_ylabel(\"Intensity\")\naxes[0].set_title(\"Smoothed Spectrum\")\n\naxes[1].plot(wavenumbers, d1)\naxes[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\naxes[1].set_ylabel(\"1st Derivative\")\naxes[1].set_title(\"First Derivative (zero crossings = peak positions)\")\n\naxes[2].plot(wavenumbers, d2)\naxes[2].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\naxes[2].set_ylabel(\"2nd Derivative\")\naxes[2].set_title(\"Second Derivative (minima = peak positions)\")\naxes[2].set_xlabel(\"Wavenumber\")\n\nfor ax in axes:\n    ax.invert_xaxis()\n\nplt.tight_layout()\nplt.show()\n</pre> # Smooth first to reduce noise amplification smoothed = smooth_savgol(noisy, window_length=15, polyorder=3)  # First and second derivatives d1 = derivative_savgol(smoothed, window_length=15, polyorder=3, deriv=1) d2 = derivative_savgol(smoothed, window_length=15, polyorder=3, deriv=2)  fig, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)  axes[0].plot(wavenumbers, smoothed) axes[0].set_ylabel(\"Intensity\") axes[0].set_title(\"Smoothed Spectrum\")  axes[1].plot(wavenumbers, d1) axes[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5) axes[1].set_ylabel(\"1st Derivative\") axes[1].set_title(\"First Derivative (zero crossings = peak positions)\")  axes[2].plot(wavenumbers, d2) axes[2].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5) axes[2].set_ylabel(\"2nd Derivative\") axes[2].set_title(\"Second Derivative (minima = peak positions)\") axes[2].set_xlabel(\"Wavenumber\")  for ax in axes:     ax.invert_xaxis()  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>d1_sg = derivative_savgol(smoothed, window_length=15, polyorder=3, deriv=1)\nd1_gap = derivative_gap_segment(smoothed, gap=7, segment=7, deriv=1)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4), sharex=True)\n\naxes[0].plot(wavenumbers, d1_sg)\naxes[0].set_title(\"Savitzky-Golay 1st Derivative\")\naxes[0].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n\naxes[1].plot(wavenumbers, d1_gap)\naxes[1].set_title(\"Gap-Segment 1st Derivative\")\naxes[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n\nfor ax in axes:\n    ax.set_xlabel(\"Wavenumber\")\n    ax.invert_xaxis()\n\nplt.tight_layout()\nplt.show()\n</pre> d1_sg = derivative_savgol(smoothed, window_length=15, polyorder=3, deriv=1) d1_gap = derivative_gap_segment(smoothed, gap=7, segment=7, deriv=1)  fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharex=True)  axes[0].plot(wavenumbers, d1_sg) axes[0].set_title(\"Savitzky-Golay 1st Derivative\") axes[0].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)  axes[1].plot(wavenumbers, d1_gap) axes[1].set_title(\"Gap-Segment 1st Derivative\") axes[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)  for ax in axes:     ax.set_xlabel(\"Wavenumber\")     ax.invert_xaxis()  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>result = peaks_find(smoothed, wavenumbers, prominence=0.3, distance=20)\n\nplt.figure(figsize=(10, 4))\nplt.plot(wavenumbers, smoothed)\nplt.plot(result.wavenumbers, result.heights, \"rv\", markersize=10, label=\"Peaks\")\n\nfor wn, h in zip(result.wavenumbers, result.heights, strict=True):\n    plt.annotate(\n        f\"{wn:.0f}\", (wn, h), textcoords=\"offset points\", xytext=(0, 10), ha=\"center\", fontsize=9\n    )\n\nplt.xlabel(\"Wavenumber\")\nplt.ylabel(\"Intensity\")\nplt.title(f\"Found {len(result.indices)} Peaks\")\nplt.legend()\nplt.gca().invert_xaxis()\nplt.show()\n\nprint(f\"Peak positions: {result.wavenumbers}\")\nprint(f\"Peak heights: {result.heights}\")\n</pre> result = peaks_find(smoothed, wavenumbers, prominence=0.3, distance=20)  plt.figure(figsize=(10, 4)) plt.plot(wavenumbers, smoothed) plt.plot(result.wavenumbers, result.heights, \"rv\", markersize=10, label=\"Peaks\")  for wn, h in zip(result.wavenumbers, result.heights, strict=True):     plt.annotate(         f\"{wn:.0f}\", (wn, h), textcoords=\"offset points\", xytext=(0, 10), ha=\"center\", fontsize=9     )  plt.xlabel(\"Wavenumber\") plt.ylabel(\"Intensity\") plt.title(f\"Found {len(result.indices)} Peaks\") plt.legend() plt.gca().invert_xaxis() plt.show()  print(f\"Peak positions: {result.wavenumbers}\") print(f\"Peak heights: {result.heights}\") In\u00a0[\u00a0]: Copied! <pre># Define integration ranges around each peak\nranges = [(1500, 1750), (2800, 3050)]\nareas = peaks_integrate(smoothed, wavenumbers, ranges=ranges)\n\nplt.figure(figsize=(10, 4))\nplt.plot(wavenumbers, smoothed)\n\ncolors = [\"skyblue\", \"salmon\"]\nfor (lo, hi), area, color in zip(ranges, areas, colors, strict=True):\n    mask = (wavenumbers &gt;= lo) &amp; (wavenumbers &lt;= hi)\n    plt.fill_between(\n        wavenumbers[mask],\n        smoothed[mask],\n        alpha=0.3,\n        color=color,\n        label=f\"{lo}-{hi}: area={area:.2f}\",\n    )\n\nplt.xlabel(\"Wavenumber\")\nplt.ylabel(\"Intensity\")\nplt.title(\"Peak Integration\")\nplt.legend()\nplt.gca().invert_xaxis()\nplt.show()\n</pre> # Define integration ranges around each peak ranges = [(1500, 1750), (2800, 3050)] areas = peaks_integrate(smoothed, wavenumbers, ranges=ranges)  plt.figure(figsize=(10, 4)) plt.plot(wavenumbers, smoothed)  colors = [\"skyblue\", \"salmon\"] for (lo, hi), area, color in zip(ranges, areas, colors, strict=True):     mask = (wavenumbers &gt;= lo) &amp; (wavenumbers &lt;= hi)     plt.fill_between(         wavenumbers[mask],         smoothed[mask],         alpha=0.3,         color=color,         label=f\"{lo}-{hi}: area={area:.2f}\",     )  plt.xlabel(\"Wavenumber\") plt.ylabel(\"Intensity\") plt.title(\"Peak Integration\") plt.legend() plt.gca().invert_xaxis() plt.show()"},{"location":"examples/03_derivatives_and_peaks/#derivatives-and-peak-analysis","title":"Derivatives and Peak Analysis\u00b6","text":"<p>This notebook demonstrates:</p> <ol> <li>Computing spectral derivatives (Savitzky-Golay and gap-segment)</li> <li>Finding peaks in original and derivative spectra</li> <li>Integrating peak areas</li> </ol>"},{"location":"examples/03_derivatives_and_peaks/#create-test-spectrum-with-overlapping-peaks","title":"Create Test Spectrum with Overlapping Peaks\u00b6","text":""},{"location":"examples/03_derivatives_and_peaks/#savitzky-golay-derivatives","title":"Savitzky-Golay Derivatives\u00b6","text":""},{"location":"examples/03_derivatives_and_peaks/#gap-segment-derivative-comparison","title":"Gap-Segment Derivative Comparison\u00b6","text":""},{"location":"examples/03_derivatives_and_peaks/#peak-finding","title":"Peak Finding\u00b6","text":""},{"location":"examples/03_derivatives_and_peaks/#peak-integration","title":"Peak Integration\u00b6","text":""},{"location":"examples/04_scatter_correction/","title":"Scatter Correction: MSC and EMSC","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom spectrakit import normalize_snv, scatter_emsc, scatter_msc\nfrom spectrakit.plot import plot_spectrum\n</pre> import matplotlib.pyplot as plt import numpy as np  from spectrakit import normalize_snv, scatter_emsc, scatter_msc from spectrakit.plot import plot_spectrum In\u00a0[\u00a0]: Copied! <pre>rng = np.random.default_rng(42)\nwavenumbers = np.linspace(4000, 10000, 500)\n\n\n# True underlying spectrum\ndef gaussian(x, c, a, s):\n    return a * np.exp(-((x - c) ** 2) / (2 * s**2))\n\n\ntrue_spectrum = (\n    gaussian(wavenumbers, 5200, 0.8, 200)\n    + gaussian(wavenumbers, 6900, 0.6, 300)\n    + gaussian(wavenumbers, 8400, 0.4, 250)\n    + 0.1\n)\n\n# Simulate 20 spectra with multiplicative scatter\nn_spectra = 20\nspectra = np.zeros((n_spectra, len(wavenumbers)))\nfor i in range(n_spectra):\n    scale = rng.uniform(0.7, 1.3)\n    offset = rng.uniform(-0.2, 0.2)\n    noise = rng.normal(0, 0.01, len(wavenumbers))\n    spectra[i] = scale * true_spectrum + offset + noise\n\nplt.figure(figsize=(10, 4))\nplot_spectrum(spectra, wavenumbers, title=\"Raw Spectra with Scatter Effects\", invert_x=False)\nplt.show()\n</pre> rng = np.random.default_rng(42) wavenumbers = np.linspace(4000, 10000, 500)   # True underlying spectrum def gaussian(x, c, a, s):     return a * np.exp(-((x - c) ** 2) / (2 * s**2))   true_spectrum = (     gaussian(wavenumbers, 5200, 0.8, 200)     + gaussian(wavenumbers, 6900, 0.6, 300)     + gaussian(wavenumbers, 8400, 0.4, 250)     + 0.1 )  # Simulate 20 spectra with multiplicative scatter n_spectra = 20 spectra = np.zeros((n_spectra, len(wavenumbers))) for i in range(n_spectra):     scale = rng.uniform(0.7, 1.3)     offset = rng.uniform(-0.2, 0.2)     noise = rng.normal(0, 0.01, len(wavenumbers))     spectra[i] = scale * true_spectrum + offset + noise  plt.figure(figsize=(10, 4)) plot_spectrum(spectra, wavenumbers, title=\"Raw Spectra with Scatter Effects\", invert_x=False) plt.show() In\u00a0[\u00a0]: Copied! <pre>msc_corrected = scatter_msc(spectra)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\nplot_spectrum(spectra, wavenumbers, ax=axes[0], title=\"Before MSC\", invert_x=False)\nplot_spectrum(msc_corrected, wavenumbers, ax=axes[1], title=\"After MSC\", invert_x=False)\nplt.tight_layout()\nplt.show()\n</pre> msc_corrected = scatter_msc(spectra)  fig, axes = plt.subplots(1, 2, figsize=(14, 4)) plot_spectrum(spectra, wavenumbers, ax=axes[0], title=\"Before MSC\", invert_x=False) plot_spectrum(msc_corrected, wavenumbers, ax=axes[1], title=\"After MSC\", invert_x=False) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>emsc_corrected = scatter_emsc(spectra, poly_order=2)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\nplot_spectrum(spectra, wavenumbers, ax=axes[0], title=\"Before EMSC\", invert_x=False)\nplot_spectrum(\n    emsc_corrected, wavenumbers, ax=axes[1], title=\"After EMSC (poly_order=2)\", invert_x=False\n)\nplt.tight_layout()\nplt.show()\n</pre> emsc_corrected = scatter_emsc(spectra, poly_order=2)  fig, axes = plt.subplots(1, 2, figsize=(14, 4)) plot_spectrum(spectra, wavenumbers, ax=axes[0], title=\"Before EMSC\", invert_x=False) plot_spectrum(     emsc_corrected, wavenumbers, ax=axes[1], title=\"After EMSC (poly_order=2)\", invert_x=False ) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>snv_corrected = normalize_snv(spectra)\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\nplot_spectrum(msc_corrected, wavenumbers, ax=axes[0], title=\"MSC\", invert_x=False)\nplot_spectrum(emsc_corrected, wavenumbers, ax=axes[1], title=\"EMSC\", invert_x=False)\nplot_spectrum(snv_corrected, wavenumbers, ax=axes[2], title=\"SNV\", invert_x=False)\n\nplt.suptitle(\"Scatter Correction Comparison\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Variance across spectra (lower = better correction)\nprint(\"Variance across spectra (mean across wavelengths):\")\nprint(f\"  Raw:  {spectra.var(axis=0).mean():.6f}\")\nprint(f\"  MSC:  {msc_corrected.var(axis=0).mean():.6f}\")\nprint(f\"  EMSC: {emsc_corrected.var(axis=0).mean():.6f}\")\nprint(f\"  SNV:  {snv_corrected.var(axis=0).mean():.6f}\")\n</pre> snv_corrected = normalize_snv(spectra)  fig, axes = plt.subplots(1, 3, figsize=(16, 4))  plot_spectrum(msc_corrected, wavenumbers, ax=axes[0], title=\"MSC\", invert_x=False) plot_spectrum(emsc_corrected, wavenumbers, ax=axes[1], title=\"EMSC\", invert_x=False) plot_spectrum(snv_corrected, wavenumbers, ax=axes[2], title=\"SNV\", invert_x=False)  plt.suptitle(\"Scatter Correction Comparison\", fontsize=14) plt.tight_layout() plt.show()  # Variance across spectra (lower = better correction) print(\"Variance across spectra (mean across wavelengths):\") print(f\"  Raw:  {spectra.var(axis=0).mean():.6f}\") print(f\"  MSC:  {msc_corrected.var(axis=0).mean():.6f}\") print(f\"  EMSC: {emsc_corrected.var(axis=0).mean():.6f}\") print(f\"  SNV:  {snv_corrected.var(axis=0).mean():.6f}\")"},{"location":"examples/04_scatter_correction/#scatter-correction-msc-and-emsc","title":"Scatter Correction: MSC and EMSC\u00b6","text":"<p>This notebook demonstrates multiplicative scatter correction (MSC) and extended multiplicative scatter correction (EMSC) for NIR-type spectra.</p>"},{"location":"examples/04_scatter_correction/#generate-synthetic-nir-like-spectra","title":"Generate Synthetic NIR-like Spectra\u00b6","text":"<p>Simulate spectra with multiplicative scatter effects (common in diffuse reflectance).</p>"},{"location":"examples/04_scatter_correction/#msc-correction","title":"MSC Correction\u00b6","text":""},{"location":"examples/04_scatter_correction/#emsc-correction","title":"EMSC Correction\u00b6","text":"<p>EMSC adds polynomial baseline terms for wavelength-dependent scatter.</p>"},{"location":"examples/04_scatter_correction/#msc-vs-emsc-vs-snv","title":"MSC vs EMSC vs SNV\u00b6","text":"<p>Compare scatter correction approaches.</p>"},{"location":"examples/05_sklearn_pipeline/","title":"scikit-learn Pipeline Integration","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline as SkPipeline\nfrom sklearn.svm import SVC\n\nfrom spectrakit import baseline_als, normalize_snv, smooth_savgol\nfrom spectrakit.sklearn import SpectralTransformer\n</pre> import matplotlib.pyplot as plt import numpy as np from sklearn.decomposition import PCA from sklearn.model_selection import cross_val_score from sklearn.pipeline import Pipeline as SkPipeline from sklearn.svm import SVC  from spectrakit import baseline_als, normalize_snv, smooth_savgol from spectrakit.sklearn import SpectralTransformer In\u00a0[\u00a0]: Copied! <pre>rng = np.random.default_rng(42)\nn_samples = 100\nn_features = 500\nwavenumbers = np.linspace(400, 4000, n_features)\n\n\ndef gaussian(x, c, a, s):\n    return a * np.exp(-((x - c) ** 2) / (2 * s**2))\n\n\n# Class 0: peaks at 1000 and 2500\n# Class 1: peaks at 1000 and 3000\nX = np.zeros((n_samples, n_features))\ny = np.zeros(n_samples, dtype=int)\n\nfor i in range(n_samples):\n    # Common peak\n    base = gaussian(wavenumbers, 1000, 2.0 + rng.normal(0, 0.2), 30)\n    # Baseline drift\n    baseline = 0.3 * rng.random() + 0.2 * np.sin(wavenumbers / 800)\n    noise = rng.normal(0, 0.05, n_features)\n\n    if i &lt; n_samples // 2:\n        # Class 0\n        discriminant = gaussian(wavenumbers, 2500, 1.5 + rng.normal(0, 0.2), 40)\n        y[i] = 0\n    else:\n        # Class 1\n        discriminant = gaussian(wavenumbers, 3000, 1.5 + rng.normal(0, 0.2), 40)\n        y[i] = 1\n\n    X[i] = base + discriminant + baseline + noise\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\nfor i in range(5):\n    axes[0].plot(wavenumbers, X[i], alpha=0.7)\naxes[0].set_title(\"Class 0 (5 samples)\")\n\nfor i in range(50, 55):\n    axes[1].plot(wavenumbers, X[i], alpha=0.7)\naxes[1].set_title(\"Class 1 (5 samples)\")\n\nfor ax in axes:\n    ax.set_xlabel(\"Wavenumber\")\n    ax.set_ylabel(\"Intensity\")\n    ax.invert_xaxis()\n\nplt.tight_layout()\nplt.show()\n</pre> rng = np.random.default_rng(42) n_samples = 100 n_features = 500 wavenumbers = np.linspace(400, 4000, n_features)   def gaussian(x, c, a, s):     return a * np.exp(-((x - c) ** 2) / (2 * s**2))   # Class 0: peaks at 1000 and 2500 # Class 1: peaks at 1000 and 3000 X = np.zeros((n_samples, n_features)) y = np.zeros(n_samples, dtype=int)  for i in range(n_samples):     # Common peak     base = gaussian(wavenumbers, 1000, 2.0 + rng.normal(0, 0.2), 30)     # Baseline drift     baseline = 0.3 * rng.random() + 0.2 * np.sin(wavenumbers / 800)     noise = rng.normal(0, 0.05, n_features)      if i &lt; n_samples // 2:         # Class 0         discriminant = gaussian(wavenumbers, 2500, 1.5 + rng.normal(0, 0.2), 40)         y[i] = 0     else:         # Class 1         discriminant = gaussian(wavenumbers, 3000, 1.5 + rng.normal(0, 0.2), 40)         y[i] = 1      X[i] = base + discriminant + baseline + noise  # Visualize fig, axes = plt.subplots(1, 2, figsize=(14, 4)) for i in range(5):     axes[0].plot(wavenumbers, X[i], alpha=0.7) axes[0].set_title(\"Class 0 (5 samples)\")  for i in range(50, 55):     axes[1].plot(wavenumbers, X[i], alpha=0.7) axes[1].set_title(\"Class 1 (5 samples)\")  for ax in axes:     ax.set_xlabel(\"Wavenumber\")     ax.set_ylabel(\"Intensity\")     ax.invert_xaxis()  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>pipe = SkPipeline(\n    [\n        (\"smooth\", SpectralTransformer(smooth_savgol, window_length=11)),\n        (\"baseline\", SpectralTransformer(baseline_als, lam=1e6, p=0.01)),\n        (\"normalize\", SpectralTransformer(normalize_snv)),\n        (\"pca\", PCA(n_components=10)),\n        (\"svm\", SVC(kernel=\"rbf\", C=1.0)),\n    ]\n)\n\nprint(pipe)\n</pre> pipe = SkPipeline(     [         (\"smooth\", SpectralTransformer(smooth_savgol, window_length=11)),         (\"baseline\", SpectralTransformer(baseline_als, lam=1e6, p=0.01)),         (\"normalize\", SpectralTransformer(normalize_snv)),         (\"pca\", PCA(n_components=10)),         (\"svm\", SVC(kernel=\"rbf\", C=1.0)),     ] )  print(pipe) In\u00a0[\u00a0]: Copied! <pre>scores = cross_val_score(pipe, X, y, cv=5, scoring=\"accuracy\")\nprint(f\"Accuracy: {scores.mean():.3f} +/- {scores.std():.3f}\")\nprint(f\"Per-fold scores: {scores}\")\n</pre> scores = cross_val_score(pipe, X, y, cv=5, scoring=\"accuracy\") print(f\"Accuracy: {scores.mean():.3f} +/- {scores.std():.3f}\") print(f\"Per-fold scores: {scores}\") In\u00a0[\u00a0]: Copied! <pre># Apply just the preprocessing steps\npreprocess = SkPipeline(\n    [\n        (\"smooth\", SpectralTransformer(smooth_savgol, window_length=11)),\n        (\"baseline\", SpectralTransformer(baseline_als, lam=1e6, p=0.01)),\n        (\"normalize\", SpectralTransformer(normalize_snv)),\n    ]\n)\n\nX_processed = preprocess.fit_transform(X)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\nfor i in range(5):\n    axes[0].plot(wavenumbers, X[i], alpha=0.5)\n    axes[1].plot(wavenumbers, X_processed[i], alpha=0.5)\n\naxes[0].set_title(\"Raw\")\naxes[1].set_title(\"Preprocessed\")\n\nfor ax in axes:\n    ax.set_xlabel(\"Wavenumber\")\n    ax.invert_xaxis()\n\nplt.tight_layout()\nplt.show()\n</pre> # Apply just the preprocessing steps preprocess = SkPipeline(     [         (\"smooth\", SpectralTransformer(smooth_savgol, window_length=11)),         (\"baseline\", SpectralTransformer(baseline_als, lam=1e6, p=0.01)),         (\"normalize\", SpectralTransformer(normalize_snv)),     ] )  X_processed = preprocess.fit_transform(X)  fig, axes = plt.subplots(1, 2, figsize=(14, 4))  for i in range(5):     axes[0].plot(wavenumbers, X[i], alpha=0.5)     axes[1].plot(wavenumbers, X_processed[i], alpha=0.5)  axes[0].set_title(\"Raw\") axes[1].set_title(\"Preprocessed\")  for ax in axes:     ax.set_xlabel(\"Wavenumber\")     ax.invert_xaxis()  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Fit PCA on preprocessed data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_processed)\n\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(\n    X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"RdBu\", alpha=0.7, edgecolors=\"k\", linewidth=0.5\n)\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0] * 100:.1f}% var)\")\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1] * 100:.1f}% var)\")\nplt.title(\"PCA of Preprocessed Spectra\")\nplt.colorbar(scatter, label=\"Class\")\nplt.show()\n</pre> # Fit PCA on preprocessed data pca = PCA(n_components=2) X_pca = pca.fit_transform(X_processed)  plt.figure(figsize=(8, 6)) scatter = plt.scatter(     X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"RdBu\", alpha=0.7, edgecolors=\"k\", linewidth=0.5 ) plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0] * 100:.1f}% var)\") plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1] * 100:.1f}% var)\") plt.title(\"PCA of Preprocessed Spectra\") plt.colorbar(scatter, label=\"Class\") plt.show()"},{"location":"examples/05_sklearn_pipeline/#scikit-learn-pipeline-integration","title":"scikit-learn Pipeline Integration\u00b6","text":"<p>This notebook demonstrates using SpectraKit preprocessing steps inside a scikit-learn pipeline for a classification workflow.</p>"},{"location":"examples/05_sklearn_pipeline/#generate-synthetic-classification-data","title":"Generate Synthetic Classification Data\u00b6","text":"<p>Create two classes of spectra with different peak patterns.</p>"},{"location":"examples/05_sklearn_pipeline/#build-sklearn-pipeline","title":"Build sklearn Pipeline\u00b6","text":"<p>Combine SpectraKit preprocessing with PCA + SVM.</p>"},{"location":"examples/05_sklearn_pipeline/#cross-validation","title":"Cross-Validation\u00b6","text":""},{"location":"examples/05_sklearn_pipeline/#visualize-preprocessing-effect","title":"Visualize Preprocessing Effect\u00b6","text":""},{"location":"examples/05_sklearn_pipeline/#pca-visualization","title":"PCA Visualization\u00b6","text":""},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#basic-install","title":"Basic Install","text":"<p>SpectraKit requires Python 3.10+ and depends only on NumPy and SciPy:</p> <pre><code>pip install pyspectrakit\n</code></pre> <p>Note: The PyPI distribution name is <code>pyspectrakit</code> (due to a naming conflict). The import name is simply <code>import spectrakit</code>.</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>SpectraKit uses optional extras for features that require additional packages:</p> <pre><code># File format support (h5py for HDF5)\npip install pyspectrakit[io]\n\n# Command-line interface\npip install pyspectrakit[cli]\n\n# pybaselines backend (200+ baseline methods)\npip install pyspectrakit[baselines]\n\n# lmfit backend (peak fitting)\npip install pyspectrakit[fitting]\n\n# scikit-learn transformer bridge\npip install pyspectrakit[sklearn]\n\n# Plotting utilities\npip install pyspectrakit[plot]\n\n# Everything\npip install pyspectrakit[all]\n</code></pre>"},{"location":"getting-started/installation/#development-install","title":"Development Install","text":"<p>For contributing to SpectraKit:</p> <pre><code>git clone https://github.com/ktubhyam/spectrakit.git\ncd spectrakit\npip install -e \".[all,dev,docs]\"\n</code></pre> <p>This installs all optional dependencies plus development tools (pytest, ruff, mypy) and documentation tools (mkdocs, mkdocs-material).</p>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<pre><code>import spectrakit\nprint(spectrakit.__version__)\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks through a typical spectral preprocessing workflow.</p>"},{"location":"getting-started/quickstart/#loading-data","title":"Loading Data","text":"<pre><code>from spectrakit.io import read_csv, read_jcamp\n\n# From CSV\nspectrum = read_csv(\"data/sample.csv\")\n\n# From JCAMP-DX\nspectrum = read_jcamp(\"data/sample.jdx\")\n\n# Or work directly with NumPy arrays\nimport numpy as np\nintensities = np.loadtxt(\"data/raw.txt\")\n</code></pre>"},{"location":"getting-started/quickstart/#basic-processing","title":"Basic Processing","text":"<pre><code>from spectrakit import smooth_savgol, baseline_als, normalize_snv\n\n# Step 1: Smooth noisy data\nsmoothed = smooth_savgol(intensities, window_length=11, polyorder=3)\n\n# Step 2: Remove baseline\ncorrected = baseline_als(smoothed, lam=1e6, p=0.01)\n\n# Step 3: Normalize\nnormalized = normalize_snv(corrected)\n</code></pre> <p>All functions accept both 1D <code>(W,)</code> and 2D <code>(N, W)</code> arrays, where <code>N</code> is the number of spectra and <code>W</code> is the number of wavelength points.</p>"},{"location":"getting-started/quickstart/#using-the-pipeline","title":"Using the Pipeline","text":"<p>Chain steps together for reproducibility:</p> <pre><code>from spectrakit.pipeline import Pipeline\n\npipe = Pipeline()\npipe.add(smooth_savgol, window_length=11)\npipe.add(baseline_als, lam=1e6)\npipe.add(normalize_snv)\n\n# Apply to a batch of spectra\nprocessed = pipe.transform(spectra_batch)\n</code></pre>"},{"location":"getting-started/quickstart/#comparing-spectra","title":"Comparing Spectra","text":"<pre><code>from spectrakit import similarity_cosine, similarity_pearson\n\nscore = similarity_cosine(spectrum_a, spectrum_b)\nprint(f\"Cosine similarity: {score:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#finding-peaks","title":"Finding Peaks","text":"<pre><code>from spectrakit import peaks_find, peaks_integrate\n\nwavenumbers = np.linspace(400, 4000, 1000)\nresult = peaks_find(intensities, wavenumbers, prominence=0.1)\nprint(f\"Found {len(result.indices)} peaks at: {result.wavenumbers}\")\n\n# Integrate a peak region\narea = peaks_integrate(intensities, wavenumbers, ranges=[(1700, 1750)])\n</code></pre>"},{"location":"getting-started/quickstart/#visualization","title":"Visualization","text":"<pre><code>from spectrakit.plot import plot_spectrum, plot_comparison\n\n# Plot a single spectrum\nplot_spectrum(intensities, wavenumbers, title=\"My Spectrum\")\n\n# Before/after comparison\nplot_comparison(original, processed, wavenumbers)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Processing Workflow Guide \u2014 detailed walk-through</li> <li>Pipeline Guide \u2014 advanced pipeline usage</li> <li>scikit-learn Integration \u2014 using with sklearn</li> <li>API Reference \u2014 full function documentation</li> </ul>"},{"location":"guides/pipeline/","title":"Pipeline Guide","text":"<p>The <code>Pipeline</code> class chains spectral processing steps into a reusable, reproducible workflow. Steps are executed in order, and each step's output feeds into the next.</p>"},{"location":"guides/pipeline/#basic-usage","title":"Basic Usage","text":"<pre><code>from spectrakit import baseline_als, normalize_snv, smooth_savgol\nfrom spectrakit.pipeline import Pipeline\n\npipe = Pipeline()\npipe.add(\"smooth\", smooth_savgol, window_length=11, polyorder=3)\npipe.add(\"baseline\", baseline_als, lam=1e6, p=0.01)\npipe.add(\"normalize\", normalize_snv)\n\n# Apply to data\nprocessed = pipe.transform(raw_spectra)\n</code></pre> <p>Each <code>add()</code> call takes:</p> <ol> <li>name \u2014 A descriptive label for logging and display</li> <li>fn \u2014 Any function with signature <code>fn(intensities, **kwargs) -&gt; np.ndarray</code></li> <li>**kwargs \u2014 Arguments forwarded to the function</li> </ol>"},{"location":"guides/pipeline/#method-chaining","title":"Method Chaining","text":"<p><code>add()</code> returns <code>self</code>, so you can chain calls:</p> <pre><code>pipe = Pipeline()\npipe.add(\"smooth\", smooth_savgol, window_length=11).add(\n    \"baseline\", baseline_als, lam=1e6\n).add(\"normalize\", normalize_snv)\n</code></pre>"},{"location":"guides/pipeline/#working-with-spectrum-objects","title":"Working with Spectrum Objects","text":"<p>Use <code>transform_spectrum()</code> to process a <code>Spectrum</code> container directly. It returns a new <code>Spectrum</code> with processed intensities and updated metadata:</p> <pre><code>from spectrakit.spectrum import Spectrum\nfrom spectrakit.io import read_jcamp\n\nspectrum = read_jcamp(\"sample.jdx\")\nprocessed = pipe.transform_spectrum(spectrum)\n\n# Metadata records which pipeline steps were applied\nprint(processed.metadata[\"pipeline_steps\"])\n# ['smooth', 'baseline', 'normalize']\n</code></pre>"},{"location":"guides/pipeline/#custom-functions","title":"Custom Functions","text":"<p>Any callable matching the expected signature works as a pipeline step:</p> <pre><code>import numpy as np\n\ndef clip_negative(intensities: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Replace negative values with zero.\"\"\"\n    return np.clip(intensities, 0, None)\n\npipe = Pipeline()\npipe.add(\"clip\", clip_negative)\npipe.add(\"normalize\", normalize_snv)\n</code></pre>"},{"location":"guides/pipeline/#logging","title":"Logging","text":"<p>Pipeline logs each step at the <code>DEBUG</code> level. Enable logging to see step execution:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\npipe.transform(spectra)\n# DEBUG:spectrakit.pipeline:Pipeline step: smooth\n# DEBUG:spectrakit.pipeline:Pipeline step: baseline\n# DEBUG:spectrakit.pipeline:Pipeline step: normalize\n</code></pre>"},{"location":"guides/pipeline/#inspecting-the-pipeline","title":"Inspecting the Pipeline","text":"<pre><code>print(pipe)\n# Pipeline(steps=['smooth', 'baseline', 'normalize'])\n\n# Access individual steps\nfor name, fn, kwargs in pipe.steps:\n    print(f\"{name}: {fn.__name__}({kwargs})\")\n</code></pre>"},{"location":"guides/pipeline/#comparison-with-scikit-learn-pipeline","title":"Comparison with scikit-learn Pipeline","text":"<p>SpectraKit's <code>Pipeline</code> is lightweight and designed for spectral workflows. For ML integration, use the sklearn bridge instead:</p> Feature <code>spectrakit.Pipeline</code> <code>sklearn.Pipeline</code> Dependencies None (built-in) Requires scikit-learn Interface <code>add()</code> / <code>transform()</code> <code>fit()</code> / <code>transform()</code> Use case Spectral preprocessing ML model pipelines Stateful No (pure functions) Yes (fit stores state) <p>Both can be used together \u2014 preprocess with SpectraKit's Pipeline, then feed results into an sklearn Pipeline for modeling.</p>"},{"location":"guides/pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Processing Workflow \u2014 recommended step order</li> <li>scikit-learn Integration \u2014 ML pipeline bridge</li> <li>Pipeline API Reference \u2014 full documentation</li> </ul>"},{"location":"guides/sklearn/","title":"scikit-learn Integration","text":"<p>SpectraKit provides <code>SpectralTransformer</code>, a scikit-learn compatible wrapper that lets you use any SpectraKit function inside an <code>sklearn.pipeline.Pipeline</code>.</p>"},{"location":"guides/sklearn/#installation","title":"Installation","text":"<pre><code>pip install pyspectrakit[sklearn]\n</code></pre>"},{"location":"guides/sklearn/#basic-usage","title":"Basic Usage","text":"<pre><code>from spectrakit.sklearn import SpectralTransformer\nfrom spectrakit import smooth_savgol, normalize_snv\n\n# Wrap a SpectraKit function\nsmoother = SpectralTransformer(smooth_savgol, window_length=11)\n\n# Use the standard sklearn interface\nX_smooth = smoother.fit_transform(X_raw)\n</code></pre>"},{"location":"guides/sklearn/#building-an-sklearn-pipeline","title":"Building an sklearn Pipeline","text":"<p>Combine multiple SpectraKit steps with sklearn estimators:</p> <pre><code>from sklearn.pipeline import Pipeline as SkPipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\n\nfrom spectrakit.sklearn import SpectralTransformer\nfrom spectrakit import smooth_savgol, baseline_als, normalize_snv\n\npipe = SkPipeline([\n    # Spectral preprocessing\n    (\"smooth\", SpectralTransformer(smooth_savgol, window_length=11)),\n    (\"baseline\", SpectralTransformer(baseline_als, lam=1e6)),\n    (\"normalize\", SpectralTransformer(normalize_snv)),\n    # ML modeling\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=10)),\n    (\"svm\", SVC()),\n])\n\n# Fit and predict\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n</code></pre>"},{"location":"guides/sklearn/#cross-validation","title":"Cross-Validation","text":"<p>Since <code>SpectralTransformer</code> follows the sklearn API, it works with all sklearn utilities:</p> <pre><code>from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(pipe, X, y, cv=5)\nprint(f\"Accuracy: {scores.mean():.3f} +/- {scores.std():.3f}\")\n</code></pre>"},{"location":"guides/sklearn/#grid-search","title":"Grid Search","text":"<p>Tune SpectraKit parameters alongside ML hyperparameters:</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"smooth__window_length\": [7, 11, 15],\n    \"baseline__lam\": [1e5, 1e6, 1e7],\n    \"pca__n_components\": [5, 10, 20],\n    \"svm__C\": [0.1, 1, 10],\n}\n\ngrid = GridSearchCV(pipe, param_grid, cv=5, scoring=\"accuracy\")\ngrid.fit(X_train, y_train)\nprint(f\"Best params: {grid.best_params_}\")\n</code></pre> <p>Note</p> <p>GridSearchCV accesses parameters via <code>get_params()</code> / <code>set_params()</code>. The <code>SpectralTransformer</code> exposes both the wrapped function and its keyword arguments as parameters.</p>"},{"location":"guides/sklearn/#how-spectraltransformer-works","title":"How SpectralTransformer Works","text":"<p><code>SpectralTransformer</code> wraps any function with signature <code>func(intensities, **kwargs) -&gt; np.ndarray</code>:</p> <ul> <li><code>fit(X, y=None)</code> \u2014 No-op (returns self). All SpectraKit functions are stateless.</li> <li><code>transform(X)</code> \u2014 Calls <code>func(X, **kwargs)</code>.</li> <li><code>get_params()</code> \u2014 Returns <code>{\"func\": ..., **kwargs}</code>.</li> <li><code>set_params(**params)</code> \u2014 Updates function or kwargs.</li> </ul>"},{"location":"guides/sklearn/#parameter-access","title":"Parameter Access","text":"<pre><code>transformer = SpectralTransformer(smooth_savgol, window_length=11)\n\n# Inspect parameters\nprint(transformer.get_params())\n# {'func': &lt;function smooth_savgol&gt;, 'window_length': 11}\n\n# Update parameters\ntransformer.set_params(window_length=15)\n</code></pre>"},{"location":"guides/sklearn/#repr","title":"Repr","text":"<pre><code>&gt;&gt;&gt; SpectralTransformer(smooth_savgol, window_length=11)\nSpectralTransformer(smooth_savgol, window_length=11)\n</code></pre>"},{"location":"guides/sklearn/#next-steps","title":"Next Steps","text":"<ul> <li>Pipeline Guide \u2014 SpectraKit's native pipeline</li> <li>Processing Workflow \u2014 recommended step order</li> <li>SpectralTransformer API \u2014 full documentation</li> </ul>"},{"location":"guides/workflow/","title":"Spectral Processing Workflow","text":"<p>This guide walks through a complete spectral preprocessing workflow, from raw data to analysis-ready spectra. The order of operations matters \u2014 this guide presents the standard sequence used in chemometrics.</p>"},{"location":"guides/workflow/#typical-processing-order","title":"Typical Processing Order","text":"<pre><code>Raw Spectra\n  \u2502\n  \u251c\u2500 1. Smoothing (noise reduction)\n  \u251c\u2500 2. Scatter Correction (MSC/EMSC)\n  \u251c\u2500 3. Baseline Correction (ALS, SNIP, etc.)\n  \u251c\u2500 4. Normalization (SNV, min-max, area)\n  \u251c\u2500 5. Derivatives (optional, SG or gap-segment)\n  \u2514\u2500 6. Transform (optional, Kubelka-Munk, ATR)\n       \u2502\n       \u2514\u2500 Analysis-ready spectra\n</code></pre> <p>Tip</p> <p>Not every dataset requires all steps. Start simple and add steps as needed.</p>"},{"location":"guides/workflow/#step-1-smoothing","title":"Step 1: Smoothing","text":"<p>Smoothing reduces high-frequency noise without distorting spectral features.</p> <pre><code>from spectrakit import smooth_savgol, smooth_whittaker\n\n# Savitzky-Golay: good general-purpose smoother\nsmoothed = smooth_savgol(spectra, window_length=11, polyorder=3)\n\n# Whittaker: penalized least-squares, tunable via lambda\nsmoothed = smooth_whittaker(spectra, lam=1e4)\n</code></pre> <p>When to use:</p> <ul> <li><code>smooth_savgol</code> \u2014 Well-understood, preserves peak shapes. Start here.</li> <li><code>smooth_whittaker</code> \u2014 Better for heavy noise. Increase <code>lam</code> for more smoothing.</li> </ul> <p>Parameters to tune:</p> <ul> <li><code>window_length</code>: Larger = smoother but may broaden peaks. Must be odd.</li> <li><code>polyorder</code>: Higher = preserves more features. Usually 2 or 3.</li> <li><code>lam</code>: Whittaker smoothing penalty. Range: 1e2 (light) to 1e6 (heavy).</li> </ul>"},{"location":"guides/workflow/#step-2-scatter-correction","title":"Step 2: Scatter Correction","text":"<p>Multiplicative scatter effects are common in diffuse reflectance (NIR) spectra.</p> <pre><code>from spectrakit import scatter_msc, scatter_emsc\n\n# MSC: standard correction\ncorrected = scatter_msc(spectra)\n\n# EMSC: includes polynomial baseline terms\ncorrected = scatter_emsc(spectra, poly_order=2)\n</code></pre> <p>When to use:</p> <ul> <li><code>scatter_msc</code> \u2014 Standard for NIR diffuse reflectance. Requires 2D batch.</li> <li><code>scatter_emsc</code> \u2014 Better when scatter varies with wavelength (adds polynomial terms).</li> </ul> <p>Note</p> <p>MSC and EMSC require a reference spectrum. For batch data <code>(N, W)</code>, the mean spectrum is used by default. For single spectra, pass <code>reference</code> explicitly.</p>"},{"location":"guides/workflow/#step-3-baseline-correction","title":"Step 3: Baseline Correction","text":"<p>Remove broad baseline contributions from the spectrum.</p> <pre><code>from spectrakit import baseline_als, baseline_snip, baseline_polynomial\n\n# ALS: asymmetric least squares (most popular)\ncorrected = baseline_als(spectra, lam=1e6, p=0.01)\n\n# SNIP: peak clipping (good for spectra with many sharp peaks)\ncorrected = baseline_snip(spectra, num_iterations=40)\n\n# Polynomial: iterative polynomial fit\ncorrected = baseline_polynomial(spectra, poly_order=3)\n</code></pre> <p>When to use:</p> <ul> <li><code>baseline_als</code> \u2014 Most versatile. High <code>lam</code> = smoother baseline. Low <code>p</code> = asymmetric.</li> <li><code>baseline_snip</code> \u2014 Fast, good for Raman and XRF with sharp peaks.</li> <li><code>baseline_polynomial</code> \u2014 Simple, works well for gentle baselines.</li> <li><code>baseline_rubberband</code> \u2014 Convex hull approach, no parameters to tune.</li> </ul>"},{"location":"guides/workflow/#step-4-normalization","title":"Step 4: Normalization","text":"<p>Scale spectra to a common range or standard for meaningful comparison.</p> <pre><code>from spectrakit import normalize_snv, normalize_minmax, normalize_area\n\n# SNV: zero mean, unit variance per spectrum\nnormalized = normalize_snv(spectra)\n\n# Min-max: scale to [0, 1]\nnormalized = normalize_minmax(spectra)\n\n# Area: unit area under the curve\nnormalized = normalize_area(spectra)\n</code></pre> <p>When to use:</p> <ul> <li><code>normalize_snv</code> \u2014 Removes multiplicative and additive scatter. Standard for NIR.</li> <li><code>normalize_minmax</code> \u2014 Good for visualization and when absolute scale matters.</li> <li><code>normalize_area</code> \u2014 Preserves relative peak intensities.</li> <li><code>normalize_vector</code> \u2014 L2 normalization, useful before cosine similarity.</li> </ul>"},{"location":"guides/workflow/#step-5-derivatives-optional","title":"Step 5: Derivatives (Optional)","text":"<p>Derivatives resolve overlapping peaks and remove constant/linear baselines.</p> <pre><code>from spectrakit import derivative_savgol, derivative_gap_segment\n\n# SG first derivative\nd1 = derivative_savgol(spectra, window_length=11, polyorder=3, deriv=1)\n\n# SG second derivative (enhances peak resolution)\nd2 = derivative_savgol(spectra, window_length=11, polyorder=3, deriv=2)\n\n# Gap-segment derivative (Norris-Williams)\nd1_gap = derivative_gap_segment(spectra, gap=5, segment=5, deriv=1)\n</code></pre> <p>Warning</p> <p>Derivatives amplify noise. Always smooth first, or use a large enough <code>window_length</code> in <code>derivative_savgol</code>.</p>"},{"location":"guides/workflow/#step-6-transforms-optional","title":"Step 6: Transforms (Optional)","text":"<p>Apply physics-based spectral transforms.</p> <pre><code>import numpy as np\nfrom spectrakit import transform_kubelka_munk, transform_atr_correction\n\n# Kubelka-Munk: convert reflectance to absorption-like units\nkm = transform_kubelka_munk(reflectance_spectra)\n\n# ATR correction: compensate for depth of penetration\nwavenumbers = np.linspace(400, 4000, 1000)\ncorrected = transform_atr_correction(spectra, wavenumbers)\n</code></pre>"},{"location":"guides/workflow/#putting-it-all-together","title":"Putting It All Together","text":"<pre><code>from spectrakit import (\n    baseline_als,\n    normalize_snv,\n    smooth_savgol,\n)\nfrom spectrakit.pipeline import Pipeline\n\n# Define a reusable processing pipeline\npipe = Pipeline()\npipe.add(smooth_savgol, window_length=11, polyorder=3)\npipe.add(baseline_als, lam=1e6, p=0.01)\npipe.add(normalize_snv)\n\n# Apply to new data\nprocessed = pipe.transform(raw_spectra)\n</code></pre>"},{"location":"guides/workflow/#visualizing-each-step","title":"Visualizing Each Step","text":"<pre><code>from spectrakit.plot import plot_comparison\n\nimport numpy as np\n\nwavenumbers = np.linspace(400, 4000, 1000)\n\n# Compare raw vs. smoothed\nplot_comparison(raw, smoothed, wavenumbers, labels=(\"Raw\", \"Smoothed\"))\n\n# Compare original vs. baseline-corrected\nplot_comparison(smoothed, corrected, wavenumbers, labels=(\"Smoothed\", \"Corrected\"))\n</code></pre>"},{"location":"guides/workflow/#next-steps","title":"Next Steps","text":"<ul> <li>Pipeline Guide \u2014 advanced pipeline features</li> <li>scikit-learn Integration \u2014 use in ML workflows</li> <li>API Reference \u2014 full documentation</li> </ul>"}]}